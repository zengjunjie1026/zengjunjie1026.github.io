<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"zengjunjie1026.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.18.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="1.环境配置#在所有节点配置YUM：#清空原来自带配置文件： 123cd &#x2F;etc&#x2F;yum.repos.d&#x2F;mkdir &#x2F;tmp&#x2F;bakmv * &#x2F;tmp&#x2F;bak&#x2F; #配置系统源码，epel源： 12345curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;Centos-7.repoyum instal">
<meta property="og:type" content="article">
<meta property="og:title" content="ceph分布式安装">
<meta property="og:url" content="https://zengjunjie1026.github.io/2023/08/11/ceph%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85/index.html">
<meta property="og:site_name" content="分子美食家的博客">
<meta property="og:description" content="1.环境配置#在所有节点配置YUM：#清空原来自带配置文件： 123cd &#x2F;etc&#x2F;yum.repos.d&#x2F;mkdir &#x2F;tmp&#x2F;bakmv * &#x2F;tmp&#x2F;bak&#x2F; #配置系统源码，epel源： 12345curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;Centos-7.repoyum instal">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2023-08-11T09:59:52.000Z">
<meta property="article:modified_time" content="2023-08-16T13:32:28.823Z">
<meta property="article:author" content="andrew">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://zengjunjie1026.github.io/2023/08/11/ceph%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://zengjunjie1026.github.io/2023/08/11/ceph%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85/","path":"2023/08/11/ceph分布式安装/","title":"ceph分布式安装"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>ceph分布式安装 | 分子美食家的博客</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">分子美食家的博客</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">记录学习的技能和遇到的问题</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">andrew</p>
  <div class="site-description" itemprop="description">做自己爱做的事，爱自己在做的事！</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">46</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zengjunjie1026.github.io/2023/08/11/ceph%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="andrew">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="分子美食家的博客">
      <meta itemprop="description" content="做自己爱做的事，爱自己在做的事！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="ceph分布式安装 | 分子美食家的博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          ceph分布式安装
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-08-11 17:59:52" itemprop="dateCreated datePublished" datetime="2023-08-11T17:59:52+08:00">2023-08-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-08-16 21:32:28" itemprop="dateModified" datetime="2023-08-16T21:32:28+08:00">2023-08-16</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>1.环境配置<br>#在所有节点配置YUM：<br>#清空原来自带配置文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /etc/yum.repos.d/</span><br><span class="line"><span class="built_in">mkdir</span> /tmp/bak</span><br><span class="line"><span class="built_in">mv</span> * /tmp/bak/</span><br></pre></td></tr></table></figure>
<p>#配置系统源码，epel源：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo</span><br><span class="line">yum install wget -y</span><br><span class="line">wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo</span><br><span class="line"><span class="comment">#YUM优先级别：</span></span><br><span class="line">yum -y install yum-plugin-priorities.noarch</span><br></pre></td></tr></table></figure>
<p>#配置ceph源：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &lt;&lt; <span class="string">EOF | tee /etc/yum.repos.d/ceph.repo</span></span><br><span class="line"><span class="string">[Ceph]</span></span><br><span class="line"><span class="string">name=Ceph packages for $basearch</span></span><br><span class="line"><span class="string">baseurl=http://mirrors.163.com/ceph/rpm-nautilus/el7/\$basearch</span></span><br><span class="line"><span class="string">enabled=1</span></span><br><span class="line"><span class="string">gpgcheck=1</span></span><br><span class="line"><span class="string">type=rpm-md</span></span><br><span class="line"><span class="string">gpgkey=https://download.ceph.com/keys/release.asc</span></span><br><span class="line"><span class="string">priority=1</span></span><br><span class="line"><span class="string">[Ceph-noarch]</span></span><br><span class="line"><span class="string">name=Ceph noarch packages</span></span><br><span class="line"><span class="string">baseurl=http://mirrors.163.com/ceph/rpm-nautilus/el7/noarch</span></span><br><span class="line"><span class="string">enabled=1</span></span><br><span class="line"><span class="string">gpgcheck=1</span></span><br><span class="line"><span class="string">type=rpm-md</span></span><br><span class="line"><span class="string">gpgkey=https://download.ceph.com/keys/release.asc</span></span><br><span class="line"><span class="string">priority=1</span></span><br><span class="line"><span class="string">[ceph-source]</span></span><br><span class="line"><span class="string">name=Ceph source packages</span></span><br><span class="line"><span class="string">baseurl=http://mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS</span></span><br><span class="line"><span class="string">enabled=1</span></span><br><span class="line"><span class="string">gpgcheck=1</span></span><br><span class="line"><span class="string">type=rpm-md</span></span><br><span class="line"><span class="string">gpgkey=https://download.ceph.com/keys/release.asc</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure>
<p>#关闭防火墙：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld</span><br><span class="line">systemctl status firewalld</span><br></pre></td></tr></table></figure>
<p>#配置主机名称：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ceph1节点：</span><br><span class="line">hostnamectl --static set-hostname ceph1</span><br><span class="line">ceph2节点：</span><br><span class="line">hostnamectl --static set-hostname ceph2</span><br><span class="line">ceph3节点：</span><br><span class="line">hostnamectl --static set-hostname ceph3</span><br></pre></td></tr></table></figure>
<p>#所有节点配置hosts文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/etc/hosts</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">192.168.0.231    ceph1</span><br><span class="line">192.168.0.232    ceph2</span><br><span class="line">192.168.0.233    ceph3</span><br></pre></td></tr></table></figure>

<p>#所有节点NTP配置：<br>在所有集群和客户端节点安装NTP，修改配置。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">yum -y install ntp ntpdate</span><br><span class="line"><span class="comment"># 以ceph1为NTP服务端节点，在ceph1新建NTP文件。</span></span><br><span class="line">vi /etc/ntp.conf</span><br><span class="line"><span class="comment"># 并新增如下内容作为NTP服务端：</span></span><br><span class="line">restrict 127.0.0.1</span><br><span class="line">restrict ::1</span><br><span class="line">restrict 192.168.3.0 mask 255.255.255.0 //ceph1的网段与掩码</span><br><span class="line">server 127.127.1.0</span><br><span class="line">fudge 127.127.1.0 stratum 8</span><br></pre></td></tr></table></figure>
<p>在ceph2、ceph3及所有客户机节点新建NTP文件。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/ntp.conf</span><br><span class="line"><span class="comment"># 并新增如下内容作为客户端：</span></span><br><span class="line">server 192.168.3.166</span><br><span class="line"></span><br><span class="line">systemctl start ntpd</span><br><span class="line">systemctl <span class="built_in">enable</span> ntpd</span><br><span class="line">systemctl status ntpd</span><br></pre></td></tr></table></figure>
<p>#ssh配置，在ceph1节点生成公钥，并发放到各个主机&#x2F;客户机节点。：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa <span class="comment">#回车采取默认配置</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..3&#125;; <span class="keyword">do</span> ssh-copy-id ceph<span class="variable">$i</span>; <span class="keyword">done</span> <span class="comment">#根据提示输入yes及节点密码</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..3&#125;; <span class="keyword">do</span> ssh-copy-id client<span class="variable">$i</span>; <span class="keyword">done</span></span><br></pre></td></tr></table></figure>

<p>#在所有节点，关闭SELinux</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed -i <span class="string">&#x27;s/enforcing/disabled/&#x27;</span> /etc/selinux/config</span><br><span class="line">setenforce 0</span><br></pre></td></tr></table></figure>
<ol>
<li>安装Ceph软件<br>使用yum install安装ceph的时候会默认安装当前已有的最新版，如果不想安装最新版本，可以在&#x2F;etc&#x2F;yum.conf文件中加以限制。<br>2.1 在所有集群和客户端节点安装Ceph<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">yum -y install ceph</span><br><span class="line">ceph -v命令查看版本:</span><br><span class="line">[root@ceph1 ~]<span class="comment"># ceph -v</span></span><br><span class="line">ceph version 14.2.9 (581f22da52345dba46ee232b73b990f06029a2a0) nautilus (stable)</span><br><span class="line">[root@ceph2 ~]<span class="comment"># ceph -v</span></span><br><span class="line">ceph version 14.2.9 (581f22da52345dba46ee232b73b990f06029a2a0) nautilus (stable)</span><br><span class="line">[root@ceph3 ~]<span class="comment"># ceph -v</span></span><br><span class="line">ceph version 14.2.9 (581f22da52345dba46ee232b73b990f06029a2a0) nautilus (stable)</span><br></pre></td></tr></table></figure>
2.2 在ceph1节点额外安装ceph-deploy。<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install ceph-deploy</span><br></pre></td></tr></table></figure>
3.部署MON节点<br>3.1 创建目录生成配置文件<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">mkdir</span> <span class="keyword">cluster</span></span><br><span class="line"><span class="keyword">cd</span> <span class="keyword">cluster</span></span><br><span class="line">ceph-deploy new ceph1 ceph2 ceph3</span><br></pre></td></tr></table></figure></li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ~]<span class="comment"># cd cluster/</span></span><br><span class="line">[root@ceph1 cluster]<span class="comment"># ceph-deploy new ceph1 ceph2 ceph3</span></span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy new ceph1 ceph2 ceph3</span><br><span class="line">[ceph_deploy.cli][INFO  ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO  ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  func                          : &lt;<span class="keyword">function</span> new at 0x7ffb7dc07de8&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  overwrite_conf                : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  quiet                         : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7ffb7d58c6c8&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO  ]  ssh_copykey                   : True</span><br><span class="line">[ceph_deploy.cli][INFO  ]  mon                           : [<span class="string">&#x27;ceph1&#x27;</span>, <span class="string">&#x27;ceph2&#x27;</span>, <span class="string">&#x27;ceph3&#x27;</span>]</span><br><span class="line">[ceph_deploy.cli][INFO  ]  public_network                : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  ceph_conf                     : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cluster_network               : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  default_release               : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  fsid                          : None</span><br><span class="line">[ceph_deploy.new][DEBUG ] Creating new cluster named ceph</span><br><span class="line">[ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds</span><br><span class="line">[ceph1][DEBUG ] connected to host: ceph1 </span><br><span class="line">[ceph1][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph1][DEBUG ] detect machine <span class="built_in">type</span></span><br><span class="line">[ceph1][DEBUG ] find the location of an executable</span><br><span class="line">[ceph1][INFO  ] Running <span class="built_in">command</span>: /usr/sbin/ip <span class="built_in">link</span> show</span><br><span class="line">[ceph1][INFO  ] Running <span class="built_in">command</span>: /usr/sbin/ip addr show</span><br><span class="line">[ceph1][DEBUG ] IP addresses found: [u<span class="string">&#x27;192.168.0.231&#x27;</span>]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Resolving host ceph1</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor ceph1 at 192.168.0.231</span><br><span class="line">[ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds</span><br><span class="line">[ceph2][DEBUG ] connected to host: ceph1 </span><br><span class="line">[ceph2][INFO  ] Running <span class="built_in">command</span>: ssh -CT -o BatchMode=<span class="built_in">yes</span> ceph2</span><br><span class="line">[ceph2][DEBUG ] connected to host: ceph2 </span><br><span class="line">[ceph2][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph2][DEBUG ] detect machine <span class="built_in">type</span></span><br><span class="line">[ceph2][DEBUG ] find the location of an executable</span><br><span class="line">[ceph2][INFO  ] Running <span class="built_in">command</span>: /usr/sbin/ip <span class="built_in">link</span> show</span><br><span class="line">[ceph2][INFO  ] Running <span class="built_in">command</span>: /usr/sbin/ip addr show</span><br><span class="line">[ceph2][DEBUG ] IP addresses found: [u<span class="string">&#x27;192.168.0.232&#x27;</span>]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Resolving host ceph2</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor ceph2 at 192.168.0.232</span><br><span class="line">[ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds</span><br><span class="line">[ceph3][DEBUG ] connected to host: ceph1 </span><br><span class="line">[ceph3][INFO  ] Running <span class="built_in">command</span>: ssh -CT -o BatchMode=<span class="built_in">yes</span> ceph3</span><br><span class="line">[ceph3][DEBUG ] connected to host: ceph3 </span><br><span class="line">[ceph3][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph3][DEBUG ] detect machine <span class="built_in">type</span></span><br><span class="line">[ceph3][DEBUG ] find the location of an executable</span><br><span class="line">[ceph3][INFO  ] Running <span class="built_in">command</span>: /usr/sbin/ip <span class="built_in">link</span> show</span><br><span class="line">[ceph3][INFO  ] Running <span class="built_in">command</span>: /usr/sbin/ip addr show</span><br><span class="line">[ceph3][DEBUG ] IP addresses found: [u<span class="string">&#x27;192.168.0.233&#x27;</span>]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Resolving host ceph3</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor ceph3 at 192.168.0.233</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor initial members are [<span class="string">&#x27;ceph1&#x27;</span>, <span class="string">&#x27;ceph2&#x27;</span>, <span class="string">&#x27;ceph3&#x27;</span>]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor addrs are [<span class="string">&#x27;192.168.0.231&#x27;</span>, <span class="string">&#x27;192.168.0.232&#x27;</span>, <span class="string">&#x27;192.168.0.233&#x27;</span>]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Creating a random mon key...</span><br><span class="line">[ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...</span><br><span class="line">[ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...</span><br></pre></td></tr></table></figure>
<p>3.2 初始化密钥</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy mon create-initial</span><br></pre></td></tr></table></figure>
<p>3.3 将ceph.client.admin.keyring拷贝到各个节点上</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy --overwrite-conf admin ceph1 ceph2 ceph3</span><br></pre></td></tr></table></figure>
<p>3.4 查看是否配置成功。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 cluster]<span class="comment"># ceph -scluster:id:     ea192428-05d2-437a-8cce-9d187de82dd5health: HEALTH_OKservices:mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 5m)mgr: no daemons activeosd: 0 osds: 0 up, 0 indata:pools:   0 pools, 0 pgsobjects: 0 objects, 0 Busage:   0 B used, 0 B / 0 B availpgs:  </span></span><br></pre></td></tr></table></figure>
<p>4 部署MGR节点</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy mgr create ceph1 ceph2 ceph3</span><br></pre></td></tr></table></figure>
<p>查看MGR是否部署成功。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">ceph -s</span><br><span class="line">[root@ceph1 cluster]<span class="comment"># ceph -scluster:id:     ea192428-05d2-437a-8cce-9d187de82dd5health: HEALTH_WARNOSD count 0 &lt; osd_pool_default_size 3services:mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 8m)mgr: ceph1(active, since 22s), standbys: ceph2, ceph3osd: 0 osds: 0 up, 0 indata:pools:   0 pools, 0 pgsobjects: 0 objects, 0 Busage:   0 B used, 0 B / 0 B availpgs: </span></span><br><span class="line">```  </span><br><span class="line">5 部署OSD节点</span><br><span class="line">```bash</span><br><span class="line">ceph-deploy osd create --data /dev/sdb ceph1</span><br><span class="line">ceph-deploy osd create --data /dev/sdc ceph1</span><br><span class="line">ceph-deploy osd create --data /dev/sdd ceph1</span><br><span class="line">ceph-deploy osd create --data /dev/sdb ceph2</span><br><span class="line">ceph-deploy osd create --data /dev/sdc ceph2</span><br><span class="line">ceph-deploy osd create --data /dev/sdd ceph2</span><br><span class="line">ceph-deploy osd create --data /dev/sdb ceph3</span><br><span class="line">ceph-deploy osd create --data /dev/sdc ceph3</span><br><span class="line">ceph-deploy osd create --data /dev/sdd ceph3</span><br></pre></td></tr></table></figure>
<p>创建成功后，查看是否正常</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 cluster]<span class="comment"># ceph -scluster:id:     ea192428-05d2-437a-8cce-9d187de82dd5health: HEALTH_OKservices:mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 14m)mgr: ceph1(active, since 6m), standbys: ceph2, ceph3osd: 9 osds: 9 up (since 2m), 9 in (since 2m)data:pools:   0 pools, 0 pgsobjects: 0 objects, 0 Busage:   9.0 GiB used, 135 GiB / 144 GiB availpgs:  </span></span><br></pre></td></tr></table></figure>
<p>6 验证Ceph<br>创建存储池</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool create vdbench 10 10</span><br></pre></td></tr></table></figure>
<p>创建块设备</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rbd create image01 --size 200--pool vdbench --image-format 2 --image-feature layering</span><br><span class="line">rbd <span class="built_in">ls</span> --pool vdbench</span><br><span class="line">[root@ceph1 cluster]<span class="comment"># rbd create image01 --size 200 --pool  vdbench --image-format 2 --image-feature layering</span></span><br><span class="line">[root@ceph1 cluster]<span class="comment"># rbd ls --pool vdbench</span></span><br><span class="line">image01</span><br></pre></td></tr></table></figure>

<p>#PG 分 配 计 算<br>归置组(PG)的数量是由管理员在创建存储池的时候指定的，然后由 CRUSH 负责创建和使用，PG 的数量是 2 的 N 次方的倍数,每个 OSD 的 PG 不要超出 250 个 PG<br>Total PGs &#x3D; (Total_number_of_OSD * 100) &#x2F; max_replication_count<br>单个 pool 的 PG 计算如下：<br>有 100 个 osd，3 副本，5 个 pool<br>Total PGs &#x3D;100*100&#x2F;3&#x3D;3333<br>每个 pool 的 PG&#x3D;3333&#x2F;5&#x3D;512，那么创建 pool 的时候就指定 pg 为 512<br>客户端在读写对象时，需要提供的是对象标识和存储池名称<br>客户端需要在存储池中读写对象时，需要客户端将对象名称，对象名称的hash码，存储池中的PG数量和存储池名称作为输入信息提供给ceph，然后由CRUSH计算出PG的ID以及PG针对的主OSD即可读写OSD中的对象。<br>具体写操作如下：<br>1.APP向ceph客户端发送对某个对象的请求，此请求包含对象和存储池，然后ceph客户端对访问的对象做hash计算，并根据此hash值计算出对象所在的PG，完成对象从Pool至PG的映射。<br>APP 访问 pool ID 和 object ID （比如 pool &#x3D; pool1 and object-id &#x3D; “name1”）<br>ceph client 对 objectID 做哈希<br>ceph client 对该 hash 值取 PG 总数的模，得到 PG 编号(比如 32)<br>ceph client 对 pool ID 取 hash（比如 “pool1” &#x3D; 3）<br>ceph client 将 pool ID 和 PG ID 组合在一起(比如 3.23)得到 PG 的完整 ID。<br>2.然后客户端据 PG、CRUSH 运行图和归置组(placement rules)作为输入参数并再次进行计<br>算，并计算出对象所在的 PG 内的主 OSD ，从而完成对象从 PG 到 OSD 的映射。<br>3.客户端开始对主 OSD 进行读写请求(副本池 IO)，如果发生了写操作，会有 ceph 服务端完<br>成对象从主 OSD 到备份 OSD 的同步  </p>
<p>二.熟练 ceph 的用户管理及授权<br>客户端使用 session key 向 mon 请求所需要的服务，mon 向客户端提供一个 tiket，用于向实际处理数据的 OSD 等服务验证客户端身份，MON 和 OSD 共享同一个 secret.<br>ceph 用户需要拥有存储池访问权限，才能读取和写入数据<br>ceph 用户必须拥有执行权限才能使用 ceph 的管理命令<br>ceph 支持多种类型的用户，但可管理的用户都属于 client 类型<br>通过点号来分割用户类型和用户名，格式为 TYPE.ID，例如 client.admin。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">root@ceph-deploy:~<span class="comment"># cat /etc/ceph/ceph.client.admin.keyring </span></span><br><span class="line">[client.admin]</span><br><span class="line">        key = AQBnFaNj1iyBMBAAd+9hKWXaNw3GYxT9PEXvrQ==</span><br><span class="line">        caps mds = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">        caps mgr = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">        caps mon = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">        caps osd = <span class="string">&quot;allow *&quot;</span></span><br></pre></td></tr></table></figure>
<p>#列 出 指 定 用 户 信 息</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">root@ceph-deploy:~<span class="comment"># ceph auth get osd.10</span></span><br><span class="line">[osd.10]</span><br><span class="line">        key = AQB+I6Njk4KWNBAAL09FFayLKF44IgUQ1fjKYQ==</span><br><span class="line">        caps mgr = <span class="string">&quot;allow profile osd&quot;</span></span><br><span class="line">        caps mon = <span class="string">&quot;allow profile osd&quot;</span></span><br><span class="line">        caps osd = <span class="string">&quot;allow *&quot;</span></span><br><span class="line"></span><br><span class="line">exported keyring <span class="keyword">for</span> osd.10</span><br></pre></td></tr></table></figure>
<p>#: 列 出 用 户</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~$ ceph auth list</span><br><span class="line">mds.ceph-mgr1</span><br><span class="line">        key: AQAdRbFjOwBXIRAAUTdwElBzYPHW+4uFicFC7Q==</span><br><span class="line">        caps: [mds] allow</span><br><span class="line">        caps: [mon] allow profile mds</span><br><span class="line">        caps: [osd] allow rwx</span><br><span class="line">osd.0</span><br><span class="line">        key: AQC0IqNjbcgKIxAA+BCNpQeZiMujR+r+69Miig==</span><br><span class="line">        caps: [mgr] allow profile osd</span><br><span class="line">        caps: [mon] allow profile osd</span><br><span class="line">        caps: [osd] allow *</span><br></pre></td></tr></table></figure>
<p>#可以结合使用-o 文件名选项和 ceph auth list 将输出保存到某个文件。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth list -o 123.key</span><br></pre></td></tr></table></figure>

<p>#ceph auth add<br>此命令是添加用户的规范方法。它会创建用户、生成密钥，并添加所有指定的能力<br>添加认证 key：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth add client.tom mon <span class="string">&#x27;allow r&#x27;</span> osd <span class="string">&#x27;allow rwx pool=testpool2&#x27;</span></span><br><span class="line">added key <span class="keyword">for</span> client.tom</span><br></pre></td></tr></table></figure>

<p>##验证 key</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth get client.tom</span><br><span class="line">[client.tom]</span><br><span class="line">        key = AQD2vbJj8fIiDBAArtJBzQiuPy8nDWPSFVs0bw==</span><br><span class="line">        caps mon = <span class="string">&quot;allow r&quot;</span></span><br><span class="line">        caps osd = <span class="string">&quot;allow rwx pool=testpool2&quot;</span></span><br><span class="line">exported keyring <span class="keyword">for</span> client.tom</span><br></pre></td></tr></table></figure>
<p>##创建用户</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth get-or-create client.jack mon <span class="string">&#x27;allow r&#x27;</span> osd <span class="string">&#x27;allow rwx pool=testpool2&#x27;</span></span><br><span class="line">[client.jack]</span><br><span class="line">        key = AQC/vrJj5kenHhAAGeRJpY64feS4Dn6DD/R8VA==</span><br></pre></td></tr></table></figure>
<p>##再次创建用户</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth get-or-create client.jack mon <span class="string">&#x27;allow r&#x27;</span> osd <span class="string">&#x27;allow rwx pool=testpool2&#x27;</span></span><br><span class="line">[client.jack]</span><br><span class="line">        key = AQC/vrJj5kenHhAAGeRJpY64feS4Dn6DD/R8VA==</span><br></pre></td></tr></table></figure>
<p>#ceph auth get-or-create-key:<br>此命令是创建用户并返回用户密钥，对于只需要密钥的客户端(例如 libvrirt),此命令非常有用。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth get-or-create-key client.jack</span><br><span class="line">mon <span class="string">&#x27;allow r&#x27;</span> osd <span class="string">&#x27;allow rwx pool=mypool&#x27;</span></span><br><span class="line">AQAtr8dfi37XMhAADbHWEZ0shY1QZ5A8eBpeoQ==</span><br></pre></td></tr></table></figure>
<p>用户有 key 就显示没有就创建<br>#修 改 用 户 能 力</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth get client.jack</span><br><span class="line">[client.jack]</span><br><span class="line">        key = AQC/vrJj5kenHhAAGeRJpY64feS4Dn6DD/R8VA==</span><br><span class="line">        caps mon = <span class="string">&quot;allow r&quot;</span></span><br><span class="line">        caps osd = <span class="string">&quot;allow rwx pool=testpool2&quot;</span></span><br><span class="line">exported keyring <span class="keyword">for</span> client.jack</span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth caps client.jack mon <span class="string">&#x27;allow r&#x27;</span> osd <span class="string">&#x27;allow rw pool=testpool2&#x27;</span></span><br><span class="line">updated caps <span class="keyword">for</span> client.jack</span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth get client.jack</span><br><span class="line">[client.jack]</span><br><span class="line">        key = AQC/vrJj5kenHhAAGeRJpY64feS4Dn6DD/R8VA==</span><br><span class="line">        caps mon = <span class="string">&quot;allow r&quot;</span></span><br><span class="line">        caps osd = <span class="string">&quot;allow rw pool=testpool2&quot;</span></span><br><span class="line">exported keyring <span class="keyword">for</span> client.jack</span><br></pre></td></tr></table></figure>

<p>#删 除 用 户 </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth del client.tom</span><br><span class="line">updated</span><br></pre></td></tr></table></figure>
<p>#导出 keyring 至指定文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth get client.user1 -o</span><br><span class="line">ceph.client.user1.keyring</span><br><span class="line">exported keyring <span class="keyword">for</span> client.user1</span><br></pre></td></tr></table></figure>
<p>#验证指定用户的 keyring 文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ <span class="built_in">cat</span> ceph.client.user1.keyring</span><br><span class="line">[client.user1]</span><br><span class="line">key = AQAUUchfjpMqGRAARV6h0ofdDEneuaRnxuHjoQ==</span><br><span class="line">caps mon = <span class="string">&quot;allow r&quot;</span></span><br><span class="line">caps osd = <span class="string">&quot;allow * pool=mypool&quot;</span></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth del client.user1 <span class="comment">#演示误删除用户</span></span><br><span class="line">Updated</span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth get client.user1 <span class="comment">#确认用户被删除</span></span><br><span class="line">Error ENOENT: failed to find client.user1 <span class="keyword">in</span> keyring</span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth import -i</span><br><span class="line">ceph.client.user1.keyring <span class="comment">#导入用户</span></span><br><span class="line">imported keyring</span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth get client.user1 <span class="comment">#验证已恢复用户</span></span><br><span class="line">exported keyring <span class="keyword">for</span> client.user1</span><br></pre></td></tr></table></figure>
<p>#将多 用 户 导 出 至 秘 钥 环 ：<br>#创建 keyring 文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ceph-authtool --create-keyring ceph.client.user.keyring <span class="comment">#创建空的 keyring 文件</span></span><br><span class="line">creating ceph.client.user.keyring</span><br></pre></td></tr></table></figure>
<p>#把指定的 admin 用户的 keyring 文件内容导入到 user 用户的 keyring 文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$ceph</span>-authtool ./ceph.client.user.keyring</span><br><span class="line">--import-keyring ./ceph.client.admin.keyring</span><br><span class="line">importing contents of ./ceph.client.admin.keyring into ./ceph.client.user.keyring</span><br></pre></td></tr></table></figure>
<p>#验证 keyring 文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph-authtool -l ./ceph.client.user.keyring</span><br><span class="line">[client.admin]</span><br><span class="line">key = AQAGDKJfQk/dAxAA3Y+9xoE/p8in6QjoHeXmeg==</span><br><span class="line">caps mds = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">caps mgr = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">caps mon = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">caps osd = <span class="string">&quot;allow *&quot;</span></span><br></pre></td></tr></table></figure>
<p>#再导入一个其他用户的 keyring：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph-authtool ./ceph.client.user.keyring</span><br><span class="line">--import-keyring ./ceph.client.user1.keyring</span><br><span class="line">importing contents of ./ceph.client.user1.keyring into ./ceph.client.user.keyring</span><br></pre></td></tr></table></figure>
<p>#再次验证 keyring 文件是否包含多个用户的认证信息：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph-authtool -l ./ceph.client.user.keyring</span><br><span class="line">[client.admin]</span><br><span class="line">key = AQAGDKJfQk/dAxAA3Y+9xoE/p8in6QjoHeXmeg==</span><br><span class="line">caps mds = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">caps mgr = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">caps mon = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">caps osd = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">[client.user1]</span><br><span class="line">key = AQAUUchfjpMqGRAARV6h0ofdDEneuaRnxuHjoQ==</span><br><span class="line">caps mon = <span class="string">&quot;allow r&quot;</span></span><br><span class="line">caps osd = <span class="string">&quot;allow * pool=mypool&quot;</span></span><br></pre></td></tr></table></figure>
<p>三. 使用普通客户挂载块存储<br>#创建存储池：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br></pre></td><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$ceph</span> osd pool create rbd-data1 32 32</span><br><span class="line"></span><br><span class="line"><span class="comment">#存储池启用 rbd</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$ceph</span> osd pool application <span class="built_in">enable</span> rbd-data1 rbd</span><br><span class="line"><span class="comment">#初始化 rbd</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$rbd</span> pool init -p rbd-data1</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建两个镜像：</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$rbd</span> create data-img1 --size 3G --pool rbd-data1 --image-format 2 --image-feature layering</span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$rbd</span> create data-img2 --size 5G --pool rbd-data1 --image-format 2 --image-feature layering</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#列出镜像信息</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$rbd</span> <span class="built_in">ls</span> --pool rbd-data1</span><br><span class="line"><span class="comment">#以 json 格 式 显 示 镜 像 信 息</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$rbd</span> <span class="built_in">ls</span> --pool rbd-data1 -l --format json --pretty-format</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建普通账户</span></span><br><span class="line">ceph auth add client.shijie mon <span class="string">&#x27;allow r&#x27;</span> osd <span class="string">&#x27;allow rwx pool=rbd-data1&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#验证用户信息</span></span><br><span class="line">ceph auth get client.shijie</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建用 keyring 文件</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph-authtool --create-keyring ceph.client.shijie.keyring</span><br><span class="line"></span><br><span class="line"><span class="comment">#导出用户 keyring</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth get client.shijie -o ceph.client.shijie.keyring </span><br><span class="line"></span><br><span class="line"><span class="comment">#验证指定用户的 keyring 文件</span></span><br><span class="line"></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$cat</span> ceph.client.shijie.keyring </span><br><span class="line">  </span><br><span class="line"><span class="comment">#同 步 普 通 用 户 认 证 文 件</span></span><br><span class="line"></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ scp ceph.client.shijie.keyring root@172.31.6.109:/etc/ceph/</span><br><span class="line"></span><br><span class="line"><span class="comment">#管理端验证镜像状态</span></span><br><span class="line"></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$rbd</span> <span class="built_in">ls</span> -p rbd-data1 -l</span><br><span class="line"></span><br><span class="line"><span class="comment">#在client上操作</span></span><br><span class="line"><span class="comment">#映射 rbd</span></span><br><span class="line"></span><br><span class="line">root@ceph-node4:/<span class="comment"># rbd -p rbd-data1 map data-img1</span></span><br><span class="line">/dev/rbd0</span><br><span class="line">root@ceph-node4:/<span class="comment"># lsblk</span></span><br><span class="line">rbd0</span><br><span class="line">root@ceph-node4:/<span class="comment"># mkfs.xfs /dev/rbd0</span></span><br><span class="line">root@ceph-node4:/<span class="comment"># mount /dev/rbd0 /data</span></span><br><span class="line">root@ceph-node4:/<span class="comment"># docker run -it -d -p 3306:3306 -e MYSQL_ROOT_PASSWORD=&quot;12345678&quot; -v /data:/var/lib/mysql mysql:5.6.46</span></span><br><span class="line">48374db8541a7fa375c00611373051ef21690e89adfd4c156b3f6ffb0dbe95a2</span><br><span class="line">root@ceph-node4:/data<span class="comment"># ls</span></span><br><span class="line">ibdata1  ib_logfile0  ib_logfile1  mysql  performance_schema  <span class="built_in">test</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#在client上操作</span></span><br><span class="line"><span class="comment">#使用普通用户映射 rbd</span></span><br><span class="line"></span><br><span class="line">root@ceph-node4:/etc/ceph<span class="comment"># rbd --user shijie -p rbd-data1 map data-img2</span></span><br><span class="line">/dev/rbd1</span><br><span class="line"><span class="comment">#格式化</span></span><br><span class="line">root@ceph-node4:/etc/ceph<span class="comment">#mkfs.ext4 /dev/rbd0</span></span><br><span class="line">root@ceph-node4:/etc/ceph<span class="comment"># mkdir /data1</span></span><br><span class="line">root@ceph-node4:/etc/ceph<span class="comment"># mount /dev/rbd1 /data1</span></span><br><span class="line">root@ceph-node4:/etc/ceph<span class="comment"># cp /var/log/auth.log /data1</span></span><br><span class="line">root@ceph-node4:/etc/ceph<span class="comment"># cd /data1</span></span><br><span class="line">root@ceph-node4:/data1<span class="comment"># ls</span></span><br><span class="line">auth.log  lost+found</span><br><span class="line">四. 使用普通用户挂载 cephfs（可以通过 secret 或者 secretfile 的形式多主机同时挂载）</span><br><span class="line">Ceph FS 需要运行 Meta Data Services(MDS)服务，其守护进程为 ceph-mds，ceph-mds进程管理与 cephFS 上存储的文件相关的元数据，并协调对 ceph 存储集群的访问。</span><br><span class="line"></span><br><span class="line"><span class="comment">#部署MDS服务:</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ apt-cache madison ceph-mds</span><br><span class="line">  ceph-mds | 16.2.10-1bionic | https://mirrors.tuna.tsinghua.edu.cn/ceph/debian-pacific bionic/main amd64 Packages</span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ apt install ceph-mds=16.2.10-1bionic</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建CephFS meta data和data存储池</span></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph osd pool create cephfs-metadata 32 32</span><br><span class="line">pool <span class="string">&#x27;cephfs-metadata&#x27;</span> created <span class="comment">#保存 metadata 的 pool</span></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph osd pool create cephfs-data 64 64</span><br><span class="line">pool <span class="string">&#x27;cephfs-data&#x27;</span> created <span class="comment">#保存数据的 pool</span></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph fs new mycephfs cephfs-metadata</span><br><span class="line">cephfs-data</span><br><span class="line">new fs with metadata pool 7 and data pool 8</span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph fs <span class="built_in">ls</span></span><br><span class="line">name: mycephfs, metadata pool: cephfs-metadata, data pools: [cephfs-data ]</span><br><span class="line"><span class="comment">#查看指定 cephFS 状态</span></span><br><span class="line"></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph fs status mycephfs</span><br><span class="line">mycephfs - 0 clients</span><br><span class="line">RANK  STATE      MDS        ACTIVITY     DNS    INOS   DIRS   CAPS  </span><br><span class="line"> 0    active  ceph-mgr1  Reqs:    0 /s    10     13     12      0   </span><br><span class="line">      POOL         TYPE     USED  AVAIL  </span><br><span class="line">cephfs-metadata  metadata   146k  18.9T  </span><br><span class="line">  cephfs-data      data       0   18.9T  </span><br><span class="line">MDS version: ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)</span><br><span class="line"><span class="comment">#验证cephFS服务状态</span></span><br><span class="line"></span><br><span class="line">cephadmin@ceph-deploy:~$ ceph mds <span class="built_in">stat</span></span><br><span class="line">mycephfs:1 &#123;0=ceph-mgr1=up:active&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建客户端账户</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$ceph</span> auth add client.yanyan mon <span class="string">&#x27;allow r&#x27;</span> mds <span class="string">&#x27;allow rw&#x27;</span> osd <span class="string">&#x27;allow rwx pool=cephfs-data&#x27;</span></span><br><span class="line"><span class="comment">#验证账户</span></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth get client.yanyan</span><br><span class="line">exported keyring <span class="keyword">for</span> client.yanyan</span><br><span class="line">[client.yanyan]</span><br><span class="line">key = AQCxpdhfjQt1OxAAGe0mqTMveNu2ZMEem3tb0g==</span><br><span class="line">caps mds = <span class="string">&quot;allow rw&quot;</span></span><br><span class="line">caps mon = <span class="string">&quot;allow r&quot;</span></span><br><span class="line">caps osd = <span class="string">&quot;allow rwx pool=cephfs-data&quot;</span></span><br><span class="line"><span class="comment">#创建keyring 文件</span></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth get client.yanyan -o</span><br><span class="line">ceph.client.yanyan.keyring</span><br><span class="line">exported keyring <span class="keyword">for</span> client.yanyan</span><br><span class="line"><span class="comment">#创建 key 文件：</span></span><br><span class="line"></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth print-key client.yanyan &gt; yanyan.key</span><br><span class="line"><span class="comment">#验证用户的 keyring 文件</span></span><br><span class="line"></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ <span class="built_in">cat</span> ceph.client.yanyan.keyring</span><br><span class="line">[client.yanyan]</span><br><span class="line">key = AQCxpdhfjQt1OxAAGe0mqTMveNu2ZMEem3tb0g==</span><br><span class="line">caps mds = <span class="string">&quot;allow rw&quot;</span></span><br><span class="line">caps mon = <span class="string">&quot;allow r&quot;</span></span><br><span class="line">caps osd = <span class="string">&quot;allow rwx pool=cephfs-data&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#同步客户端认证文件 ：</span></span><br><span class="line"></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ scp ceph.conf ceph.client.yanyan.keyring</span><br><span class="line">yanyan.key root@172.31.6.109:/etc/ceph/</span><br><span class="line"></span><br><span class="line"><span class="comment">#客户端验证权限</span></span><br><span class="line"></span><br><span class="line">root@ceph-node4:~<span class="comment"># ceph --user yanyan -s</span></span><br><span class="line">  cluster:</span><br><span class="line">    <span class="built_in">id</span>:     7c088d6f-06b0-4584-b23f-c0f150af51d4</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph-mon1,ceph-mon2,ceph-mon3 (age 24m)</span><br><span class="line">    mgr: ceph-mgr1(active, since 65m)</span><br><span class="line">    mds: 1/1 daemons up</span><br><span class="line">    osd: 16 osds: 16 up (since 24m), 16 <span class="keyword">in</span> (since 13d)</span><br><span class="line">    rgw: 1 daemon active (1 hosts, 1 zones)</span><br><span class="line">  data:</span><br><span class="line">    volumes: 1/1 healthy</span><br><span class="line">    pools:   10 pools, 329 pgs</span><br><span class="line">    objects: 296 objects, 218 MiB</span><br><span class="line">    usage:   948 MiB used, 60 TiB / 60 TiB avail</span><br><span class="line">    pgs:     329 active+clean</span><br><span class="line"></span><br><span class="line"><span class="comment">#客户端通过 key 文件挂载:</span></span><br><span class="line"></span><br><span class="line"> root@ceph-node4:~<span class="comment">#mkdir /data</span></span><br><span class="line"> root@ceph-node4:/etc/ceph<span class="comment"># mount -t ceph 172.31.6.101:6789,172.31.6.102:6789,172.31.6.103:6789:/ /data -o name=yanyan,secretfile=/etc/ceph/yanyan.key</span></span><br><span class="line">root@ceph-node4:/etc/ceph<span class="comment"># df -h</span></span><br><span class="line">Filesystem                                               Size  Used Avail Use% Mounted on</span><br><span class="line">udev                                                     955M     0  955M   0% /dev</span><br><span class="line">172.31.6.101:6789,172.31.6.102:6789,172.31.6.103:6789:/   19T     0   19T   0% /data</span><br><span class="line"></span><br><span class="line"><span class="comment">#客户端通过key挂载</span></span><br><span class="line"></span><br><span class="line">root@ceph-node3:~<span class="comment"># mkdir /data</span></span><br><span class="line">root@ceph-node3:~<span class="comment"># mount -t ceph 172.31.6.101:6789,172.31.6.102:6789,172.31.6.103:6789:/ /data -o name=yanyan,secret=AQAfebVjaIPgABAAzkW4ChX2Qm2Sha/5twdxPA==</span></span><br><span class="line">root@ceph-node3:~<span class="comment"># df -h</span></span><br><span class="line">Filesystem                                               Size  Used Avail Use% Mounted on</span><br><span class="line">udev                                                     955M     0  955M   0% /dev</span><br><span class="line">172.31.6.101:6789,172.31.6.102:6789,172.31.6.103:6789:/   19T     0   19T   0% /data</span><br><span class="line">root@ceph-node3:~<span class="comment"># cp /var/log/auth.log /data</span></span><br><span class="line">root@ceph-node3:~<span class="comment"># cd /data</span></span><br><span class="line">root@ceph-node3:/data<span class="comment"># ls</span></span><br><span class="line">auth.log</span><br><span class="line">root@ceph-node3:/data<span class="comment"># vim auth.log </span></span><br><span class="line">root@ceph-node3:/data<span class="comment"># echo &quot;12345678&quot; &gt;&gt; auth.log </span></span><br><span class="line">root@ceph-node3:/data<span class="comment"># echo &quot;testlog&quot; &gt;&gt; auth.log</span></span><br><span class="line"><span class="comment">#在node4客户端上查看cephfs挂载点/data 目录下内容，已经同步</span></span><br><span class="line"></span><br><span class="line">root@ceph-node4:/<span class="comment"># tail -f /data/auth.log </span></span><br><span class="line">Jan  4 22:25:01 ceph-node3 CRON[4365]: pam_unix(cron:session): session closed <span class="keyword">for</span> user root</span><br><span class="line">12345678</span><br><span class="line">testlog</span><br><span class="line"><span class="comment">#客户端内核加载 ceph.ko 模块挂载 cephfs 文件系统</span></span><br><span class="line"></span><br><span class="line">root@ceph-node4:/<span class="comment"># lsmod|grep ceph</span></span><br><span class="line">ceph                  380928  1</span><br><span class="line">libceph               315392  1 ceph</span><br><span class="line">fscache                65536  1 ceph</span><br><span class="line">libcrc32c              16384  5 nf_conntrack,nf_nat,xfs,raid456,libceph</span><br><span class="line">root@ceph-node4:/<span class="comment"># modinfo ceph</span></span><br><span class="line">filename:       /lib/modules/4.15.0-130-generic/kernel/fs/ceph/ceph.ko</span><br><span class="line">license:        GPL</span><br><span class="line">description:    Ceph filesystem <span class="keyword">for</span> Linux</span><br><span class="line">author:         Patience Warnick &lt;patience@newdream.net&gt;</span><br><span class="line">author:         Yehuda Sadeh &lt;yehuda@hq.newdream.net&gt;</span><br><span class="line">author:         Sage Weil &lt;sage@newdream.net&gt;</span><br><span class="line"><span class="built_in">alias</span>:          fs-ceph</span><br><span class="line">srcversion:     CB79D9E4790452C6A392A1C</span><br><span class="line">depends:        libceph,fscache</span><br><span class="line">retpoline:      Y</span><br><span class="line">intree:         Y</span><br><span class="line">name:           ceph</span><br><span class="line">vermagic:       4.15.0-130-generic SMP mod_unload </span><br><span class="line">signat:         PKCS<span class="comment">#7</span></span><br><span class="line">signer:         </span><br><span class="line">sig_key:        </span><br><span class="line">sig_hashalgo:   md4</span><br></pre></td></tr></table></figure>

<p>五.实现 MDS 服务的多主一备高可用架构</p>
<p>#当前mds服务器状态</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~$ ceph mds <span class="built_in">stat</span></span><br><span class="line">mycephfs:1 &#123;0=ceph-mgr1=up:active&#125;</span><br><span class="line"><span class="comment">#添加MDS服务器</span></span><br><span class="line">将 ceph-mgr2 和 ceph-mon2 和 ceph-mon3 作为 mds 服务角色添加至 ceph 集群，最后实两主两备的 mds 高可用和高性能结构。</span><br><span class="line"><span class="comment">#mds 服务器安装 ceph-mds 服务</span></span><br><span class="line"></span><br><span class="line">[root@ceph-mgr2 ~]<span class="comment"># apt install ceph-mds -y</span></span><br><span class="line">[root@ceph-mon2 ~]<span class="comment"># apt install ceph-mds -y</span></span><br><span class="line">[root@ceph-mon3 ~]<span class="comment"># apt install ceph-mds -y</span></span><br><span class="line"><span class="comment">#添加 mds 服务器</span></span><br><span class="line"></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph-deploy mds create ceph-mgr2</span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph-deploy mds create ceph-mon2</span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph-deploy mds create ceph-mon3</span><br></pre></td></tr></table></figure>
<p>#验证 mds 服务器当前状态：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph mds <span class="built_in">stat</span></span><br><span class="line">mycephfs-1/1/1 up &#123;0=ceph-mgr1=up:active&#125;, 3 up:standby</span><br></pre></td></tr></table></figure>
<p>#验证 ceph集群当前状态<br>当前处于激活状态的 mds 服务器有一台，处于备份状态的 mds 服务器有三台。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph fs status</span><br><span class="line">mycephfs - 0 clients</span><br><span class="line">RANK  STATE      MDS        ACTIVITY     DNS    INOS   DIRS   CAPS  </span><br><span class="line"> 0    active  ceph-mgr1  Reqs:    0 /s    16     14     12      0   </span><br><span class="line">      POOL         TYPE     USED  AVAIL  </span><br><span class="line">cephfs-metadata  metadata   282k  18.9T  </span><br><span class="line">  cephfs-data      data     564k  18.9T  </span><br><span class="line">STANDBY MDS  </span><br></pre></td></tr></table></figure>
<p>#当前的文件系统状态:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph fs get mycephfs</span><br><span class="line">Filesystem <span class="string">&#x27;mycephfs&#x27;</span> (1)</span><br><span class="line">fs_name        mycephfs</span><br><span class="line">epoch        28</span><br><span class="line">flags        12</span><br><span class="line">created        2023-01-01T19:09:29.258956+0800</span><br><span class="line">modified        2023-01-05T14:58:00.369468+0800</span><br><span class="line">tableserver        0</span><br><span class="line">root        0</span><br><span class="line">session_timeout        60</span><br><span class="line">session_autoclose        300</span><br><span class="line">max_file_size        1099511627776</span><br><span class="line">required_client_features        &#123;&#125;</span><br><span class="line">last_failure        0</span><br><span class="line">last_failure_osd_epoch        406</span><br><span class="line">compat        compat=&#123;&#125;,rocompat=&#123;&#125;,incompat=&#123;1=base v0.20,2=client writeable ranges,3=default file layouts on <span class="built_in">dirs</span>,4=<span class="built_in">dir</span> inode <span class="keyword">in</span> separate object,5=mds uses versioned encoding,6=dirfrag is stored <span class="keyword">in</span> omap,7=mds uses inline data,8=no anchor table,9=file layout v2,10=snaprealm v2&#125;</span><br><span class="line">max_mds        1</span><br><span class="line"><span class="keyword">in</span>        0</span><br><span class="line">up        &#123;0=144106&#125;</span><br><span class="line">failed        </span><br><span class="line">damaged        </span><br><span class="line">stopped        </span><br><span class="line">data_pools        [8]</span><br><span class="line">metadata_pool        7</span><br><span class="line">inline_data        disabled</span><br><span class="line">balancer        </span><br><span class="line">standby_count_wanted        1</span><br><span class="line">[mds.ceph-mgr1&#123;0:144106&#125; state up:active <span class="built_in">seq</span> 27 addr [v2:172.31.6.104:6800/428364709,v1:172.31.6.104:6801/428364709] compat &#123;c=[1],r=[1],i=[7ff]&#125;]</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置处于激活状态mds的数量</span></span><br><span class="line">目前有四个 mds 服务器，但是有一个主三个备，可以优化一下部署架构，设置为为两主两备。</span><br><span class="line"></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph fs <span class="built_in">set</span> mycephfs max_mds 2</span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph fs status</span><br><span class="line">mycephfs - 0 clients</span><br><span class="line">RANK  STATE      MDS        ACTIVITY     DNS    INOS   DIRS   CAPS  </span><br><span class="line"> 0    active  ceph-mgr1  Reqs:    0 /s    16     14     12      0   </span><br><span class="line"> 1    active  ceph-mon2  Reqs:    0 /s    10     13     11      0   </span><br><span class="line">      POOL         TYPE     USED  AVAIL  </span><br><span class="line">cephfs-metadata  metadata   354k  18.9T  </span><br><span class="line">  cephfs-data      data     564k  18.9T  </span><br><span class="line">STANDBY MDS  </span><br><span class="line"> ceph-mgr2   </span><br><span class="line"> ceph-mon3   </span><br><span class="line">MDS version: ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)</span><br></pre></td></tr></table></figure>


<p>安装完成后ceph -s提示：“mon is allowing insecure global_id reclaim”。</p>
<p>解决方案：禁用不安全模式</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config <span class="built_in">set</span> mon auth_allow_insecure_global_id_reclaim <span class="literal">false</span></span><br></pre></td></tr></table></figure>
<p>输入完成后需要等待5-10秒。</p>
<p>PG数量计算公式：</p>
<p>（总OSD数量*100）&#x2F; 副本数（复制分数，默认为3）&#x3D; PG数量</p>
<p>一般情况下结果取2的N次方，尽量先设置小点，后期可以增大。如果先设置的比较大的话后期减小风险较高，所以尽量取小的2的N次方结果。<br>建一个存储池，要想使用ceph的存储功能，必须先创建存储池</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool create rbd 128 128 </span><br></pre></td></tr></table></figure>

<p>初始化存储池</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd pool init -p rbd</span><br></pre></td></tr></table></figure>
<p>1.设置存储池副本数</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@controller-1 ~]<span class="comment"># ceph osd pool get rbd size</span></span><br><span class="line">size: 2</span><br><span class="line">[root@controller-1 ~]<span class="comment"># ceph osd pool set rbd size 1</span></span><br><span class="line"><span class="built_in">set</span> pool 1 size to 1</span><br></pre></td></tr></table></figure>
<p>升级client的虚拟机内核到5版本</p>
<p>修改client下文件权限</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">chmod</span> +r /etc/ceph/ceph.client.admin.keyring</span><br><span class="line">client节点创建设备镜像，单位是M</span><br><span class="line">rbd create --size 4096 --pool rbd img</span><br><span class="line">client节点映射镜像到主机：</span><br><span class="line">rbd map img --name client.admin</span><br><span class="line">client节点格式化块设备</span><br><span class="line">mkfs.ext4 -m 0 /dev/rbd/rbd/foo </span><br><span class="line">client节点挂载mount块设备</span><br><span class="line"><span class="built_in">mkdir</span> /mnt/ceph-block-device</span><br><span class="line">mount /dev/rbd/rbd/foo /mnt/ceph-block-device -o discard</span><br><span class="line">创建文件系统时报不支持EC数据池问题</span><br><span class="line">当拥有一个ceph_metadata元数据副本类型池和一个ceph_data数据EC类型池时，建立文件系统：</span><br><span class="line">ceph fs new cephfs cephfs_metadata cephfs_data</span><br><span class="line">N版可能会报：</span><br><span class="line">Error EINVAL: pool <span class="string">&#x27;cephfs_data&#x27;</span> (<span class="built_in">id</span> <span class="string">&#x27;11&#x27;</span>) is an erasure-coded pool. Use of an EC pool <span class="keyword">for</span> the default data pool is discouraged; see the online CephFS documentation <span class="keyword">for</span> more information. Use --force to override.</span><br><span class="line">加上—force后：</span><br><span class="line">ceph fs new cephfs cephfs_metadata cephfs_data --force</span><br><span class="line"></span><br><span class="line">也可能会报：</span><br><span class="line"></span><br><span class="line">Error EINVAL: pool <span class="string">&#x27;cephfs_data&#x27;</span> (<span class="built_in">id</span> <span class="string">&#x27;11&#x27;</span>) is an erasure-coded pool, with no overwrite support</span><br><span class="line"></span><br><span class="line">这是需要手动设置ceph_data池 allow_ec_overwrites=<span class="literal">true</span></span><br><span class="line">ceph osd pool <span class="built_in">set</span> cephfs_data allow_ec_overwrites <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">再执行</span><br><span class="line">ceph fs new cephfs cephfs_metadata cephfs_data –forc</span><br><span class="line">就可以创建文件系统成功。</span><br></pre></td></tr></table></figure>
    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/08/18/gitlab-ci-runners/" rel="prev" title="gitlab_ci runners">
                  <i class="fa fa-angle-left"></i> gitlab_ci runners
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/08/16/spark-group-by-%E6%93%8D%E4%BD%9C/" rel="next" title="spark group by 操作">
                  spark group by 操作 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2023</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">andrew</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  






  





</body>
</html>
