<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"zengjunjie1026.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.18.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="1.环境配置#在所有节点配置YUM：#清空原来自带配置文件：cd &#x2F;etc&#x2F;yum.repos.d&#x2F;mkdir &#x2F;tmp&#x2F;bakmv * &#x2F;tmp&#x2F;bak&#x2F;#配置系统源码，epel源： curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo https:&#x2F;&#x2F;mi">
<meta property="og:type" content="website">
<meta property="og:title" content="ceph安装以及测试">
<meta property="og:url" content="https://zengjunjie1026.github.io/ceph%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E6%B5%8B%E8%AF%95/index.html">
<meta property="og:site_name" content="分子美食家的博客">
<meta property="og:description" content="1.环境配置#在所有节点配置YUM：#清空原来自带配置文件：cd &#x2F;etc&#x2F;yum.repos.d&#x2F;mkdir &#x2F;tmp&#x2F;bakmv * &#x2F;tmp&#x2F;bak&#x2F;#配置系统源码，epel源： curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo https:&#x2F;&#x2F;mi">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2023-08-16T03:40:47.000Z">
<meta property="article:modified_time" content="2023-08-18T02:16:11.349Z">
<meta property="article:author" content="andrew">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://zengjunjie1026.github.io/ceph%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E6%B5%8B%E8%AF%95/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":false,"lang":"zh-CN","comments":true,"permalink":"https://zengjunjie1026.github.io/ceph%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E6%B5%8B%E8%AF%95/index.html","path":"ceph安装以及测试/index.html","title":"ceph安装以及测试"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>ceph安装以及测试 | 分子美食家的博客
</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">分子美食家的博客</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">记录学习的技能和遇到的问题</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">andrew</p>
  <div class="site-description" itemprop="description">做自己爱做的事，爱自己在做的事！</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">46</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner page posts-expand">


    
    
    
    <div class="post-block" lang="zh-CN"><header class="post-header">

<h1 class="post-title" itemprop="name headline">ceph安装以及测试
</h1>

<div class="post-meta-container">
</div>

</header>

      
      
      <div class="post-body">
          <p>1.环境配置<br>#在所有节点配置YUM：<br>#清空原来自带配置文件：<br>cd &#x2F;etc&#x2F;yum.repos.d&#x2F;<br>mkdir &#x2F;tmp&#x2F;bak<br>mv * &#x2F;tmp&#x2F;bak&#x2F;<br>#配置系统源码，epel源：</p>
<p>curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo <a target="_blank" rel="noopener" href="https://mirrors.aliyun.com/repo/Centos-7.repo">https://mirrors.aliyun.com/repo/Centos-7.repo</a><br>yum install wget -y<br>wget -O &#x2F;etc&#x2F;yum.repos.d&#x2F;epel.repo <a target="_blank" rel="noopener" href="http://mirrors.aliyun.com/repo/epel-7.repo">http://mirrors.aliyun.com/repo/epel-7.repo</a><br>#YUM优先级别：<br>yum -y install yum-plugin-priorities.noarch</p>
<p>#配置ceph源：<br>cat &lt;&lt; EOF | tee &#x2F;etc&#x2F;yum.repos.d&#x2F;ceph.repo<br>[Ceph]<br>name&#x3D;Ceph packages for $basearch<br>baseurl&#x3D;<a target="_blank" rel="noopener" href="http://mirrors.163.com/ceph/rpm-nautilus/el7//$basearch">http://mirrors.163.com/ceph/rpm-nautilus/el7/\$basearch</a><br>enabled&#x3D;1<br>gpgcheck&#x3D;1<br>type&#x3D;rpm-md<br>gpgkey&#x3D;<a target="_blank" rel="noopener" href="https://download.ceph.com/keys/release.asc">https://download.ceph.com/keys/release.asc</a><br>priority&#x3D;1<br>[Ceph-noarch]<br>name&#x3D;Ceph noarch packages<br>baseurl&#x3D;<a target="_blank" rel="noopener" href="http://mirrors.163.com/ceph/rpm-nautilus/el7/noarch">http://mirrors.163.com/ceph/rpm-nautilus/el7/noarch</a><br>enabled&#x3D;1<br>gpgcheck&#x3D;1<br>type&#x3D;rpm-md<br>gpgkey&#x3D;<a target="_blank" rel="noopener" href="https://download.ceph.com/keys/release.asc">https://download.ceph.com/keys/release.asc</a><br>priority&#x3D;1<br>[ceph-source]<br>name&#x3D;Ceph source packages<br>baseurl&#x3D;<a target="_blank" rel="noopener" href="http://mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS">http://mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS</a><br>enabled&#x3D;1<br>gpgcheck&#x3D;1<br>type&#x3D;rpm-md<br>gpgkey&#x3D;<a target="_blank" rel="noopener" href="https://download.ceph.com/keys/release.asc">https://download.ceph.com/keys/release.asc</a><br>EOF</p>
<p>#关闭防火墙：<br>systemctl stop firewalld<br>systemctl disable firewalld<br>systemctl status firewalld</p>
<p>#配置主机名称：<br>ceph1节点：<br>hostnamectl –static set-hostname ceph1<br>ceph2节点：<br>hostnamectl –static set-hostname ceph2<br>ceph3节点：<br>hostnamectl –static set-hostname ceph3</p>
<p>#所有节点配置hosts文件：<br>&#x2F;etc&#x2F;hosts<br>127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4<br>::1         localhost localhost.localdomain localhost6 localhost6.localdomain6<br>192.168.0.231    ceph1<br>192.168.0.232    ceph2<br>192.168.0.233    ceph3</p>
<p>#所有节点NTP配置：<br>在所有集群和客户端节点安装NTP，修改配置。<br>yum -y install ntp ntpdate<br>以ceph1为NTP服务端节点，在ceph1新建NTP文件。<br>vi &#x2F;etc&#x2F;ntp.conf<br>并新增如下内容作为NTP服务端：<br>restrict 127.0.0.1<br>restrict ::1<br>restrict 192.168.3.0 mask 255.255.255.0 &#x2F;&#x2F;ceph1的网段与掩码<br>server 127.127.1.0<br>fudge 127.127.1.0 stratum 8</p>
<p>在ceph2、ceph3及所有客户机节点新建NTP文件。<br>vi &#x2F;etc&#x2F;ntp.conf<br>并新增如下内容作为客户端：<br>server 192.168.3.166</p>
<p>systemctl start ntpd<br>systemctl enable ntpd<br>systemctl status ntpd</p>
<p>#ssh配置，在ceph1节点生成公钥，并发放到各个主机&#x2F;客户机节点。：<br>ssh-keygen -t rsa #回车采取默认配置<br>for i in {1..3}; do ssh-copy-id ceph$i; done #根据提示输入yes及节点密码<br>for i in {1..3}; do ssh-copy-id client$i; done</p>
<p>#在所有节点，关闭SELinux<br>sed -i ‘s&#x2F;enforcing&#x2F;disabled&#x2F;‘ &#x2F;etc&#x2F;selinux&#x2F;config<br>setenforce 0</p>
<ol>
<li>安装Ceph软件<br>使用yum install安装ceph的时候会默认安装当前已有的最新版，如果不想安装最新版本，可以在&#x2F;etc&#x2F;yum.conf文件中加以限制。<br>2.1 在所有集群和客户端节点安装Ceph<br>yum -y install ceph<br>ceph -v命令查看版本:<br>[root@ceph1 ~]# ceph -v<br>ceph version 14.2.9 (581f22da52345dba46ee232b73b990f06029a2a0) nautilus (stable)<br>[root@ceph2 ~]# ceph -v<br>ceph version 14.2.9 (581f22da52345dba46ee232b73b990f06029a2a0) nautilus (stable)<br>[root@ceph3 ~]# ceph -v<br>ceph version 14.2.9 (581f22da52345dba46ee232b73b990f06029a2a0) nautilus (stable)<br>2.2 在ceph1节点额外安装ceph-deploy。<br>yum -y install ceph-deploy<br>3.部署MON节点<br>3.1 创建目录生成配置文件<br>mkdir cluster<br>cd cluster<br>ceph-deploy new ceph1 ceph2 ceph3<br>[root@ceph1 ~]# cd cluster&#x2F;<br>[root@ceph1 cluster]# ceph-deploy new ceph1 ceph2 ceph3<br>[ceph_deploy.conf][DEBUG ] found configuration file at: &#x2F;root&#x2F;.cephdeploy.conf<br>[ceph_deploy.cli][INFO  ] Invoked (2.0.1): &#x2F;usr&#x2F;bin&#x2F;ceph-deploy new ceph1 ceph2 ceph3<br>[ceph_deploy.cli][INFO  ] ceph-deploy options:<br>[ceph_deploy.cli][INFO  ]  username                      : None<br>[ceph_deploy.cli][INFO  ]  func                          : &lt;function new at 0x7ffb7dc07de8&gt;<br>[ceph_deploy.cli][INFO  ]  verbose                       : False<br>[ceph_deploy.cli][INFO  ]  overwrite_conf                : False<br>[ceph_deploy.cli][INFO  ]  quiet                         : False<br>[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7ffb7d58c6c8&gt;<br>[ceph_deploy.cli][INFO  ]  cluster                       : ceph<br>[ceph_deploy.cli][INFO  ]  ssh_copykey                   : True<br>[ceph_deploy.cli][INFO  ]  mon                           : [‘ceph1’, ‘ceph2’, ‘ceph3’]<br>[ceph_deploy.cli][INFO  ]  public_network                : None<br>[ceph_deploy.cli][INFO  ]  ceph_conf                     : None<br>[ceph_deploy.cli][INFO  ]  cluster_network               : None<br>[ceph_deploy.cli][INFO  ]  default_release               : False<br>[ceph_deploy.cli][INFO  ]  fsid                          : None<br>[ceph_deploy.new][DEBUG ] Creating new cluster named ceph<br>[ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds<br>[ceph1][DEBUG ] connected to host: ceph1<br>[ceph1][DEBUG ] detect platform information from remote host<br>[ceph1][DEBUG ] detect machine type<br>[ceph1][DEBUG ] find the location of an executable<br>[ceph1][INFO  ] Running command: &#x2F;usr&#x2F;sbin&#x2F;ip link show<br>[ceph1][INFO  ] Running command: &#x2F;usr&#x2F;sbin&#x2F;ip addr show<br>[ceph1][DEBUG ] IP addresses found: [u’192.168.0.231’]<br>[ceph_deploy.new][DEBUG ] Resolving host ceph1<br>[ceph_deploy.new][DEBUG ] Monitor ceph1 at 192.168.0.231<br>[ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds<br>[ceph2][DEBUG ] connected to host: ceph1<br>[ceph2][INFO  ] Running command: ssh -CT -o BatchMode&#x3D;yes ceph2<br>[ceph2][DEBUG ] connected to host: ceph2<br>[ceph2][DEBUG ] detect platform information from remote host<br>[ceph2][DEBUG ] detect machine type<br>[ceph2][DEBUG ] find the location of an executable<br>[ceph2][INFO  ] Running command: &#x2F;usr&#x2F;sbin&#x2F;ip link show<br>[ceph2][INFO  ] Running command: &#x2F;usr&#x2F;sbin&#x2F;ip addr show<br>[ceph2][DEBUG ] IP addresses found: [u’192.168.0.232’]<br>[ceph_deploy.new][DEBUG ] Resolving host ceph2<br>[ceph_deploy.new][DEBUG ] Monitor ceph2 at 192.168.0.232<br>[ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds<br>[ceph3][DEBUG ] connected to host: ceph1<br>[ceph3][INFO  ] Running command: ssh -CT -o BatchMode&#x3D;yes ceph3<br>[ceph3][DEBUG ] connected to host: ceph3<br>[ceph3][DEBUG ] detect platform information from remote host<br>[ceph3][DEBUG ] detect machine type<br>[ceph3][DEBUG ] find the location of an executable<br>[ceph3][INFO  ] Running command: &#x2F;usr&#x2F;sbin&#x2F;ip link show<br>[ceph3][INFO  ] Running command: &#x2F;usr&#x2F;sbin&#x2F;ip addr show<br>[ceph3][DEBUG ] IP addresses found: [u’192.168.0.233’]<br>[ceph_deploy.new][DEBUG ] Resolving host ceph3<br>[ceph_deploy.new][DEBUG ] Monitor ceph3 at 192.168.0.233<br>[ceph_deploy.new][DEBUG ] Monitor initial members are [‘ceph1’, ‘ceph2’, ‘ceph3’]<br>[ceph_deploy.new][DEBUG ] Monitor addrs are [‘192.168.0.231’, ‘192.168.0.232’, ‘192.168.0.233’]<br>[ceph_deploy.new][DEBUG ] Creating a random mon key…<br>[ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring…<br>[ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf…<br>3.2 初始化密钥<br>ceph-deploy mon create-initial<br>3.3 将ceph.client.admin.keyring拷贝到各个节点上<br>ceph-deploy –overwrite-conf admin ceph1 ceph2 ceph3<br>3.4 查看是否配置成功。<br>[root@ceph1 cluster]# ceph -scluster:id:     ea192428-05d2-437a-8cce-9d187de82dd5health: HEALTH_OKservices:mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 5m)mgr: no daemons activeosd: 0 osds: 0 up, 0 indata:pools:   0 pools, 0 pgsobjects: 0 objects, 0 Busage:   0 B used, 0 B &#x2F; 0 B availpgs:<br>4 部署MGR节点<br>ceph-deploy mgr create ceph1 ceph2 ceph3<br>查看MGR是否部署成功。<br>ceph -s<br>[root@ceph1 cluster]# ceph -scluster:id:     ea192428-05d2-437a-8cce-9d187de82dd5health: HEALTH_WARNOSD count 0 &lt; osd_pool_default_size 3services:mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 8m)mgr: ceph1(active, since 22s), standbys: ceph2, ceph3osd: 0 osds: 0 up, 0 indata:pools:   0 pools, 0 pgsobjects: 0 objects, 0 Busage:   0 B used, 0 B &#x2F; 0 B availpgs:<br>5 部署OSD节点<br>ceph-deploy osd create –data &#x2F;dev&#x2F;sdb ceph1<br>ceph-deploy osd create –data &#x2F;dev&#x2F;sdc ceph1<br>ceph-deploy osd create –data &#x2F;dev&#x2F;sdd ceph1<br>ceph-deploy osd create –data &#x2F;dev&#x2F;sdb ceph2<br>ceph-deploy osd create –data &#x2F;dev&#x2F;sdc ceph2<br>ceph-deploy osd create –data &#x2F;dev&#x2F;sdd ceph2<br>ceph-deploy osd create –data &#x2F;dev&#x2F;sdb ceph3<br>ceph-deploy osd create –data &#x2F;dev&#x2F;sdc ceph3<br>ceph-deploy osd create –data &#x2F;dev&#x2F;sdd ceph3<br>创建成功后，查看是否正常<br>[root@ceph1 cluster]# ceph -scluster:id:     ea192428-05d2-437a-8cce-9d187de82dd5health: HEALTH_OKservices:mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 14m)mgr: ceph1(active, since 6m), standbys: ceph2, ceph3osd: 9 osds: 9 up (since 2m), 9 in (since 2m)data:pools:   0 pools, 0 pgsobjects: 0 objects, 0 Busage:   9.0 GiB used, 135 GiB &#x2F; 144 GiB availpgs:<br>6 验证Ceph<br>创建存储池<br>ceph osd pool create vdbench 10 10<br>创建块设备<br>rbd create image01 –size 200–pool vdbench –image-format 2 –image-feature layering<br>rbd ls –pool vdbench<br>[root@ceph1 cluster]# rbd create image01 –size 200 –pool  vdbench –image-format 2 –image-feature layering<br>[root@ceph1 cluster]# rbd ls –pool vdbench<br>image01</li>
</ol>
<p>#PG 分 配 计 算<br>归置组(PG)的数量是由管理员在创建存储池的时候指定的，然后由 CRUSH 负责创建和使用，PG 的数量是 2 的 N 次方的倍数,每个 OSD 的 PG 不要超出 250 个 PG<br>Total PGs &#x3D; (Total_number_of_OSD * 100) &#x2F; max_replication_count<br>单个 pool 的 PG 计算如下：<br>有 100 个 osd，3 副本，5 个 pool<br>Total PGs &#x3D;100*100&#x2F;3&#x3D;3333<br>每个 pool 的 PG&#x3D;3333&#x2F;5&#x3D;512，那么创建 pool 的时候就指定 pg 为 512<br>客户端在读写对象时，需要提供的是对象标识和存储池名称<br>客户端需要在存储池中读写对象时，需要客户端将对象名称，对象名称的hash码，存储池中的PG数量和存储池名称作为输入信息提供给ceph，然后由CRUSH计算出PG的ID以及PG针对的主OSD即可读写OSD中的对象。<br>具体写操作如下：<br>1.APP向ceph客户端发送对某个对象的请求，此请求包含对象和存储池，然后ceph客户端对访问的对象做hash计算，并根据此hash值计算出对象所在的PG，完成对象从Pool至PG的映射。<br>APP 访问 pool ID 和 object ID （比如 pool &#x3D; pool1 and object-id &#x3D; “name1”）<br>ceph client 对 objectID 做哈希<br>ceph client 对该 hash 值取 PG 总数的模，得到 PG 编号(比如 32)<br>ceph client 对 pool ID 取 hash（比如 “pool1” &#x3D; 3）<br>ceph client 将 pool ID 和 PG ID 组合在一起(比如 3.23)得到 PG 的完整 ID。<br>2.然后客户端据 PG、CRUSH 运行图和归置组(placement rules)作为输入参数并再次进行计<br>算，并计算出对象所在的 PG 内的主 OSD ，从而完成对象从 PG 到 OSD 的映射。<br>3.客户端开始对主 OSD 进行读写请求(副本池 IO)，如果发生了写操作，会有 ceph 服务端完<br>成对象从主 OSD 到备份 OSD 的同步</p>
<p>二.熟练 ceph 的用户管理及授权<br>客户端使用 session key 向 mon 请求所需要的服务，mon 向客户端提供一个 tiket，用于向实际处理数据的 OSD 等服务验证客户端身份，MON 和 OSD 共享同一个 secret.<br>ceph 用户需要拥有存储池访问权限，才能读取和写入数据<br>ceph 用户必须拥有执行权限才能使用 ceph 的管理命令<br>ceph 支持多种类型的用户，但可管理的用户都属于 client 类型<br>通过点号来分割用户类型和用户名，格式为 TYPE.ID，例如 client.admin。<br>root@ceph-deploy:~# cat &#x2F;etc&#x2F;ceph&#x2F;ceph.client.admin.keyring<br>[client.admin]<br>        key &#x3D; AQBnFaNj1iyBMBAAd+9hKWXaNw3GYxT9PEXvrQ&#x3D;&#x3D;<br>        caps mds &#x3D; “allow *”<br>        caps mgr &#x3D; “allow *”<br>        caps mon &#x3D; “allow *”<br>        caps osd &#x3D; “allow *”</p>
<p>#列 出 指 定 用 户 信 息<br>root@ceph-deploy:~# ceph auth get osd.10<br>[osd.10]<br>        key &#x3D; AQB+I6Njk4KWNBAAL09FFayLKF44IgUQ1fjKYQ&#x3D;&#x3D;<br>        caps mgr &#x3D; “allow profile osd”<br>        caps mon &#x3D; “allow profile osd”<br>        caps osd &#x3D; “allow *”</p>
<p>exported keyring for osd.10</p>
<p>#: 列 出 用 户<br>cephadmin@ceph-deploy:~$ ceph auth list<br>mds.ceph-mgr1<br>        key: AQAdRbFjOwBXIRAAUTdwElBzYPHW+4uFicFC7Q&#x3D;&#x3D;<br>        caps: [mds] allow<br>        caps: [mon] allow profile mds<br>        caps: [osd] allow rwx<br>osd.0<br>        key: AQC0IqNjbcgKIxAA+BCNpQeZiMujR+r+69Miig&#x3D;&#x3D;<br>        caps: [mgr] allow profile osd<br>        caps: [mon] allow profile osd<br>        caps: [osd] allow *</p>
<p>#可以结合使用-o 文件名选项和 ceph auth list 将输出保存到某个文件。<br>[cephadmin@ceph-deploy ceph-cluster]$ ceph auth list -o 123.key</p>
<p>#ceph auth add<br>此命令是添加用户的规范方法。它会创建用户、生成密钥，并添加所有指定的能力<br>添加认证 key：<br>cephadmin@ceph-deploy:~&#x2F;ceph-cluster$ ceph auth add client.tom mon ‘allow r’ osd ‘allow rwx pool&#x3D;testpool2’<br>added key for client.tom</p>
<p>##验证 key<br>cephadmin@ceph-deploy:~&#x2F;ceph-cluster$ ceph auth get client.tom<br>[client.tom]<br>        key &#x3D; AQD2vbJj8fIiDBAArtJBzQiuPy8nDWPSFVs0bw&#x3D;&#x3D;<br>        caps mon &#x3D; “allow r”<br>        caps osd &#x3D; “allow rwx pool&#x3D;testpool2”<br>exported keyring for client.tom</p>
<p>##创建用户<br>cephadmin@ceph-deploy:~&#x2F;ceph-cluster$ ceph auth get-or-create client.jack mon ‘allow r’ osd ‘allow rwx pool&#x3D;testpool2’<br>[client.jack]<br>        key &#x3D; AQC&#x2F;vrJj5kenHhAAGeRJpY64feS4Dn6DD&#x2F;R8VA&#x3D;&#x3D;</p>
<p>##再次创建用户<br>cephadmin@ceph-deploy:~&#x2F;ceph-cluster$ ceph auth get-or-create client.jack mon ‘allow r’ osd ‘allow rwx pool&#x3D;testpool2’<br>[client.jack]<br>        key &#x3D; AQC&#x2F;vrJj5kenHhAAGeRJpY64feS4Dn6DD&#x2F;R8VA&#x3D;&#x3D;</p>
<p>#ceph auth get-or-create-key:<br>此命令是创建用户并返回用户密钥，对于只需要密钥的客户端(例如 libvrirt),此命令非常有用。<br>[cephadmin@ceph-deploy ceph-cluster]$ ceph auth get-or-create-key client.jack<br>mon ‘allow r’ osd ‘allow rwx pool&#x3D;mypool’<br>AQAtr8dfi37XMhAADbHWEZ0shY1QZ5A8eBpeoQ&#x3D;&#x3D;</p>
<p>用户有 key 就显示没有就创建<br>#修 改 用 户 能 力<br>cephadmin@ceph-deploy:<del>&#x2F;ceph-cluster$ ceph auth get client.jack<br>[client.jack]<br>        key &#x3D; AQC&#x2F;vrJj5kenHhAAGeRJpY64feS4Dn6DD&#x2F;R8VA&#x3D;&#x3D;<br>        caps mon &#x3D; “allow r”<br>        caps osd &#x3D; “allow rwx pool&#x3D;testpool2”<br>exported keyring for client.jack<br>cephadmin@ceph-deploy:</del>&#x2F;ceph-cluster$ ceph auth caps client.jack mon ‘allow r’ osd ‘allow rw pool&#x3D;testpool2’<br>updated caps for client.jack<br>cephadmin@ceph-deploy:~&#x2F;ceph-cluster$ ceph auth get client.jack<br>[client.jack]<br>        key &#x3D; AQC&#x2F;vrJj5kenHhAAGeRJpY64feS4Dn6DD&#x2F;R8VA&#x3D;&#x3D;<br>        caps mon &#x3D; “allow r”<br>        caps osd &#x3D; “allow rw pool&#x3D;testpool2”<br>exported keyring for client.jack</p>
<p>#删 除 用 户<br>cephadmin@ceph-deploy:~&#x2F;ceph-cluster$ ceph auth del client.tom<br>updated<br>#导出 keyring 至指定文件<br>[cephadmin@ceph-deploy ceph-cluster]$ ceph auth get client.user1 -o<br>ceph.client.user1.keyring<br>exported keyring for client.user1<br>#验证指定用户的 keyring 文件：<br>[cephadmin@ceph-deploy ceph-cluster]$ cat ceph.client.user1.keyring<br>[client.user1]<br>key &#x3D; AQAUUchfjpMqGRAARV6h0ofdDEneuaRnxuHjoQ&#x3D;&#x3D;<br>caps mon &#x3D; “allow r”<br>caps osd &#x3D; “allow * pool&#x3D;mypool”<br>[cephadmin@ceph-deploy ceph-cluster]$ ceph auth del client.user1 #演示误删除用户<br>Updated<br>[cephadmin@ceph-deploy ceph-cluster]$ ceph auth get client.user1 #确认用户被删除<br>Error ENOENT: failed to find client.user1 in keyring<br>[cephadmin@ceph-deploy ceph-cluster]$ ceph auth import -i<br>ceph.client.user1.keyring #导入用户<br>imported keyring<br>[cephadmin@ceph-deploy ceph-cluster]$ ceph auth get client.user1 #验证已恢复用户<br>exported keyring for client.user1<br>#将多 用 户 导 出 至 秘 钥 环 ：<br>#创建 keyring 文件：<br>$ ceph-authtool –create-keyring ceph.client.user.keyring #创建空的 keyring 文件<br>creating ceph.client.user.keyring<br>#把指定的 admin 用户的 keyring 文件内容导入到 user 用户的 keyring 文件：<br>$ceph-authtool .&#x2F;ceph.client.user.keyring<br>–import-keyring .&#x2F;ceph.client.admin.keyring<br>importing contents of .&#x2F;ceph.client.admin.keyring into .&#x2F;ceph.client.user.keyring<br>#验证 keyring 文件：<br>[cephadmin@ceph-deploy ceph-cluster]$ ceph-authtool -l .&#x2F;ceph.client.user.keyring<br>[client.admin]<br>key &#x3D; AQAGDKJfQk&#x2F;dAxAA3Y+9xoE&#x2F;p8in6QjoHeXmeg&#x3D;&#x3D;<br>caps mds &#x3D; “allow *”<br>caps mgr &#x3D; “allow *”<br>caps mon &#x3D; “allow *”<br>caps osd &#x3D; “allow *”<br>#再导入一个其他用户的 keyring：<br>[cephadmin@ceph-deploy ceph-cluster]$ ceph-authtool .&#x2F;ceph.client.user.keyring<br>–import-keyring .&#x2F;ceph.client.user1.keyring<br>importing contents of .&#x2F;ceph.client.user1.keyring into .&#x2F;ceph.client.user.keyring<br>#再次验证 keyring 文件是否包含多个用户的认证信息：<br>[cephadmin@ceph-deploy ceph-cluster]$ ceph-authtool -l .&#x2F;ceph.client.user.keyring<br>[client.admin]<br>key &#x3D; AQAGDKJfQk&#x2F;dAxAA3Y+9xoE&#x2F;p8in6QjoHeXmeg&#x3D;&#x3D;<br>caps mds &#x3D; “allow *”<br>caps mgr &#x3D; “allow *”<br>caps mon &#x3D; “allow *”<br>caps osd &#x3D; “allow *”<br>[client.user1]<br>key &#x3D; AQAUUchfjpMqGRAARV6h0ofdDEneuaRnxuHjoQ&#x3D;&#x3D;<br>caps mon &#x3D; “allow r”<br>caps osd &#x3D; “allow * pool&#x3D;mypool”</p>
<p>三. 使用普通客户挂载块存储<br>#创建存储池：<br>cephadmin@ceph-deploy:~&#x2F;ceph-cluster$ceph osd pool create rbd-data1 32 32</p>
<p>#存储池启用 rbd<br>cephadmin@ceph-deploy:<del>&#x2F;ceph-cluster$ceph osd pool application enable rbd-data1 rbd<br>#初始化 rbd<br>cephadmin@ceph-deploy:</del>&#x2F;ceph-cluster$rbd pool init -p rbd-data1</p>
<p>#创建两个镜像：<br>cephadmin@ceph-deploy:<del>&#x2F;ceph-cluster$rbd create data-img1 –size 3G –pool rbd-data1 –image-format 2 –image-feature layering<br>cephadmin@ceph-deploy:</del>&#x2F;ceph-cluster$rbd create data-img2 –size 5G –pool rbd-data1 –image-format 2 –image-feature layering</p>
<p>#列出镜像信息<br>cephadmin@ceph-deploy:<del>&#x2F;ceph-cluster$rbd ls –pool rbd-data1<br>#以 json 格 式 显 示 镜 像 信 息<br>cephadmin@ceph-deploy:</del>&#x2F;ceph-cluster$rbd ls –pool rbd-data1 -l –format json –pretty-format</p>
<p>#创建普通账户<br>ceph auth add client.shijie mon ‘allow r’ osd ‘allow rwx pool&#x3D;rbd-data1’</p>
<p>#验证用户信息<br>ceph auth get client.shijie</p>
<p>#创建用 keyring 文件<br>cephadmin@ceph-deploy:~&#x2F;ceph-cluster$ ceph-authtool –create-keyring ceph.client.shijie.keyring</p>
<p>#导出用户 keyring<br>cephadmin@ceph-deploy:~&#x2F;ceph-cluster$ ceph auth get client.shijie -o ceph.client.shijie.keyring </p>
<p>#验证指定用户的 keyring 文件</p>
<p>cephadmin@ceph-deploy:~&#x2F;ceph-cluster$cat ceph.client.shijie.keyring </p>
<p>#同 步 普 通 用 户 认 证 文 件</p>
<p>cephadmin@ceph-deploy:~&#x2F;ceph-cluster$ scp ceph.client.shijie.keyring <a href="mailto:&#114;&#x6f;&#x6f;&#116;&#64;&#x31;&#55;&#x32;&#x2e;&#x33;&#49;&#x2e;&#54;&#x2e;&#49;&#x30;&#x39;">&#114;&#x6f;&#x6f;&#116;&#64;&#x31;&#55;&#x32;&#x2e;&#x33;&#49;&#x2e;&#54;&#x2e;&#49;&#x30;&#x39;</a>:&#x2F;etc&#x2F;ceph&#x2F;</p>
<p>#管理端验证镜像状态</p>
<p>cephadmin@ceph-deploy:~&#x2F;ceph-cluster$rbd ls -p rbd-data1 -l</p>
<p>#在client上操作<br>#映射 rbd</p>
<p>root@ceph-node4:&#x2F;# rbd -p rbd-data1 map data-img1<br>&#x2F;dev&#x2F;rbd0<br>root@ceph-node4:&#x2F;# lsblk<br>rbd0<br>root@ceph-node4:&#x2F;# mkfs.xfs &#x2F;dev&#x2F;rbd0<br>root@ceph-node4:&#x2F;# mount &#x2F;dev&#x2F;rbd0 &#x2F;data<br>root@ceph-node4:&#x2F;# docker run -it -d -p 3306:3306 -e MYSQL_ROOT_PASSWORD&#x3D;”12345678” -v &#x2F;data:&#x2F;var&#x2F;lib&#x2F;mysql mysql:5.6.46<br>48374db8541a7fa375c00611373051ef21690e89adfd4c156b3f6ffb0dbe95a2<br>root@ceph-node4:&#x2F;data# ls<br>ibdata1  ib_logfile0  ib_logfile1  mysql  performance_schema  test</p>
<p>#在client上操作<br>#使用普通用户映射 rbd</p>
<p>root@ceph-node4:&#x2F;etc&#x2F;ceph# rbd –user shijie -p rbd-data1 map data-img2<br>&#x2F;dev&#x2F;rbd1<br>#格式化<br>root@ceph-node4:&#x2F;etc&#x2F;ceph#mkfs.ext4 &#x2F;dev&#x2F;rbd0<br>root@ceph-node4:&#x2F;etc&#x2F;ceph# mkdir &#x2F;data1<br>root@ceph-node4:&#x2F;etc&#x2F;ceph# mount &#x2F;dev&#x2F;rbd1 &#x2F;data1<br>root@ceph-node4:&#x2F;etc&#x2F;ceph# cp &#x2F;var&#x2F;log&#x2F;auth.log &#x2F;data1<br>root@ceph-node4:&#x2F;etc&#x2F;ceph# cd &#x2F;data1<br>root@ceph-node4:&#x2F;data1# ls<br>auth.log  lost+found<br>四. 使用普通用户挂载 cephfs（可以通过 secret 或者 secretfile 的形式多主机同时挂载）<br>Ceph FS 需要运行 Meta Data Services(MDS)服务，其守护进程为 ceph-mds，ceph-mds进程管理与 cephFS 上存储的文件相关的元数据，并协调对 ceph 存储集群的访问。</p>
<p>#部署MDS服务:<br>cephadmin@ceph-deploy:<del>&#x2F;ceph-cluster$ apt-cache madison ceph-mds<br>  ceph-mds | 16.2.10-1bionic | <a target="_blank" rel="noopener" href="https://mirrors.tuna.tsinghua.edu.cn/ceph/debian-pacific">https://mirrors.tuna.tsinghua.edu.cn/ceph/debian-pacific</a> bionic&#x2F;main amd64 Packages<br>cephadmin@ceph-deploy:</del>&#x2F;ceph-cluster$ apt install ceph-mds&#x3D;16.2.10-1bionic</p>
<p>#创建CephFS meta data和data存储池<br>[cephadmin@ceph-deploy ceph-cluster]$ ceph osd pool create cephfs-metadata 32 32<br>pool ‘cephfs-metadata’ created #保存 metadata 的 pool<br>[cephadmin@ceph-deploy ceph-cluster]$ ceph osd pool create cephfs-data 64 64<br>pool ‘cephfs-data’ created #保存数据的 pool<br>[cephadmin@ceph-deploy ceph-cluster]$ ceph fs new mycephfs cephfs-metadata<br>cephfs-data<br>new fs with metadata pool 7 and data pool 8<br>cephadmin@ceph-deploy:~&#x2F;ceph-cluster$ ceph fs ls<br>name: mycephfs, metadata pool: cephfs-metadata, data pools: [cephfs-data ]<br>#查看指定 cephFS 状态</p>
<p>cephadmin@ceph-deploy:~&#x2F;ceph-cluster$ ceph fs status mycephfs<br>mycephfs - 0 clients<br>RANK  STATE      MDS        ACTIVITY     DNS    INOS   DIRS   CAPS<br> 0    active  ceph-mgr1  Reqs:    0 &#x2F;s    10     13     12      0<br>      POOL         TYPE     USED  AVAIL<br>cephfs-metadata  metadata   146k  18.9T<br>  cephfs-data      data       0   18.9T<br>MDS version: ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)<br>#验证cephFS服务状态</p>
<p>cephadmin@ceph-deploy:~$ ceph mds stat<br>mycephfs:1 {0&#x3D;ceph-mgr1&#x3D;up:active}</p>
<p>#创建客户端账户<br>cephadmin@ceph-deploy:~&#x2F;ceph-cluster$ceph auth add client.yanyan mon ‘allow r’ mds ‘allow rw’ osd ‘allow rwx pool&#x3D;cephfs-data’<br>#验证账户<br>[cephadmin@ceph-deploy ceph-cluster]$ ceph auth get client.yanyan<br>exported keyring for client.yanyan<br>[client.yanyan]<br>key &#x3D; AQCxpdhfjQt1OxAAGe0mqTMveNu2ZMEem3tb0g&#x3D;&#x3D;<br>caps mds &#x3D; “allow rw”<br>caps mon &#x3D; “allow r”<br>caps osd &#x3D; “allow rwx pool&#x3D;cephfs-data”<br>#创建keyring 文件<br>[cephadmin@ceph-deploy ceph-cluster]$ ceph auth get client.yanyan -o<br>ceph.client.yanyan.keyring<br>exported keyring for client.yanyan<br>#创建 key 文件：</p>
<p>[cephadmin@ceph-deploy ceph-cluster]$ ceph auth print-key client.yanyan &gt; yanyan.key<br>#验证用户的 keyring 文件</p>
<p>[cephadmin@ceph-deploy ceph-cluster]$ cat ceph.client.yanyan.keyring<br>[client.yanyan]<br>key &#x3D; AQCxpdhfjQt1OxAAGe0mqTMveNu2ZMEem3tb0g&#x3D;&#x3D;<br>caps mds &#x3D; “allow rw”<br>caps mon &#x3D; “allow r”<br>caps osd &#x3D; “allow rwx pool&#x3D;cephfs-data”</p>
<p>#同步客户端认证文件 ：</p>
<p>[cephadmin@ceph-deploy ceph-cluster]$ scp ceph.conf ceph.client.yanyan.keyring<br>yanyan.key <a href="mailto:&#x72;&#111;&#x6f;&#x74;&#x40;&#x31;&#x37;&#50;&#x2e;&#51;&#x31;&#46;&#x36;&#x2e;&#x31;&#48;&#57;">&#x72;&#111;&#x6f;&#x74;&#x40;&#x31;&#x37;&#50;&#x2e;&#51;&#x31;&#46;&#x36;&#x2e;&#x31;&#48;&#57;</a>:&#x2F;etc&#x2F;ceph&#x2F;</p>
<p>#客户端验证权限</p>
<p>root@ceph-node4:~# ceph –user yanyan -s<br>  cluster:<br>    id:     7c088d6f-06b0-4584-b23f-c0f150af51d4<br>    health: HEALTH_OK<br>  services:<br>    mon: 3 daemons, quorum ceph-mon1,ceph-mon2,ceph-mon3 (age 24m)<br>    mgr: ceph-mgr1(active, since 65m)<br>    mds: 1&#x2F;1 daemons up<br>    osd: 16 osds: 16 up (since 24m), 16 in (since 13d)<br>    rgw: 1 daemon active (1 hosts, 1 zones)<br>  data:<br>    volumes: 1&#x2F;1 healthy<br>    pools:   10 pools, 329 pgs<br>    objects: 296 objects, 218 MiB<br>    usage:   948 MiB used, 60 TiB &#x2F; 60 TiB avail<br>    pgs:     329 active+clean</p>
<p>#客户端通过 key 文件挂载:</p>
<p> root@ceph-node4:~#mkdir &#x2F;data<br> root@ceph-node4:&#x2F;etc&#x2F;ceph# mount -t ceph 172.31.6.101:6789,172.31.6.102:6789,172.31.6.103:6789:&#x2F; &#x2F;data -o name&#x3D;yanyan,secretfile&#x3D;&#x2F;etc&#x2F;ceph&#x2F;yanyan.key<br>root@ceph-node4:&#x2F;etc&#x2F;ceph# df -h<br>Filesystem                                               Size  Used Avail Use% Mounted on<br>udev                                                     955M     0  955M   0% &#x2F;dev<br>172.31.6.101:6789,172.31.6.102:6789,172.31.6.103:6789:&#x2F;   19T     0   19T   0% &#x2F;data</p>
<p>#客户端通过key挂载</p>
<p>root@ceph-node3:<del># mkdir &#x2F;data<br>root@ceph-node3:</del># mount -t ceph 172.31.6.101:6789,172.31.6.102:6789,172.31.6.103:6789:&#x2F; &#x2F;data -o name&#x3D;yanyan,secret&#x3D;AQAfebVjaIPgABAAzkW4ChX2Qm2Sha&#x2F;5twdxPA&#x3D;&#x3D;<br>root@ceph-node3:<del># df -h<br>Filesystem                                               Size  Used Avail Use% Mounted on<br>udev                                                     955M     0  955M   0% &#x2F;dev<br>172.31.6.101:6789,172.31.6.102:6789,172.31.6.103:6789:&#x2F;   19T     0   19T   0% &#x2F;data<br>root@ceph-node3:</del># cp &#x2F;var&#x2F;log&#x2F;auth.log &#x2F;data<br>root@ceph-node3:~# cd &#x2F;data<br>root@ceph-node3:&#x2F;data# ls<br>auth.log<br>root@ceph-node3:&#x2F;data# vim auth.log<br>root@ceph-node3:&#x2F;data# echo “12345678” &gt;&gt; auth.log<br>root@ceph-node3:&#x2F;data# echo “testlog” &gt;&gt; auth.log<br>#在node4客户端上查看cephfs挂载点&#x2F;data 目录下内容，已经同步</p>
<p>root@ceph-node4:&#x2F;# tail -f &#x2F;data&#x2F;auth.log<br>Jan  4 22:25:01 ceph-node3 CRON[4365]: pam_unix(cron:session): session closed for user root<br>12345678<br>testlog<br>#客户端内核加载 ceph.ko 模块挂载 cephfs 文件系统</p>
<p>root@ceph-node4:&#x2F;# lsmod|grep ceph<br>ceph                  380928  1<br>libceph               315392  1 ceph<br>fscache                65536  1 ceph<br>libcrc32c              16384  5 nf_conntrack,nf_nat,xfs,raid456,libceph<br>root@ceph-node4:&#x2F;# modinfo ceph<br>filename:       &#x2F;lib&#x2F;modules&#x2F;4.15.0-130-generic&#x2F;kernel&#x2F;fs&#x2F;ceph&#x2F;ceph.ko<br>license:        GPL<br>description:    Ceph filesystem for Linux<br>author:         Patience Warnick <a href="mailto:&#112;&#97;&#116;&#105;&#x65;&#110;&#99;&#101;&#64;&#110;&#x65;&#x77;&#100;&#114;&#x65;&#97;&#x6d;&#46;&#110;&#x65;&#116;">&#112;&#97;&#116;&#105;&#x65;&#110;&#99;&#101;&#64;&#110;&#x65;&#x77;&#100;&#114;&#x65;&#97;&#x6d;&#46;&#110;&#x65;&#116;</a><br>author:         Yehuda Sadeh <a href="mailto:&#x79;&#101;&#x68;&#117;&#100;&#x61;&#64;&#x68;&#x71;&#x2e;&#110;&#x65;&#x77;&#100;&#114;&#x65;&#x61;&#109;&#46;&#110;&#101;&#116;">&#x79;&#101;&#x68;&#117;&#100;&#x61;&#64;&#x68;&#x71;&#x2e;&#110;&#x65;&#x77;&#100;&#114;&#x65;&#x61;&#109;&#46;&#110;&#101;&#116;</a><br>author:         Sage Weil <a href="mailto:&#x73;&#x61;&#103;&#101;&#x40;&#110;&#101;&#119;&#x64;&#114;&#x65;&#x61;&#109;&#x2e;&#110;&#x65;&#x74;">&#x73;&#x61;&#103;&#101;&#x40;&#110;&#101;&#119;&#x64;&#114;&#x65;&#x61;&#109;&#x2e;&#110;&#x65;&#x74;</a><br>alias:          fs-ceph<br>srcversion:     CB79D9E4790452C6A392A1C<br>depends:        libceph,fscache<br>retpoline:      Y<br>intree:         Y<br>name:           ceph<br>vermagic:       4.15.0-130-generic SMP mod_unload<br>signat:         PKCS#7<br>signer:<br>sig_key:<br>sig_hashalgo:   md4</p>

      </div>
      
      
      
    </div>

    
    


</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2023</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">andrew</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  






  





</body>
</html>
