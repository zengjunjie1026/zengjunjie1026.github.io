<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>spark null 值处理</title>
    <url>/2023/08/16/spark-null-%E5%80%BC%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<p> scala spark na null处理 | 分子美食家的博客                  </p>
<h1 id="分子美食家的博客"><a href="#分子美食家的博客" class="headerlink" title="分子美食家的博客"></a><a href="/">分子美食家的博客</a></h1><h2 id="野生工程师的专栏"><a href="#野生工程师的专栏" class="headerlink" title="野生工程师的专栏"></a><a href="/">野生工程师的专栏</a></h2><p><a href="/">Home</a> <a href="/archives">Archives</a></p>
<p><a href="/atom.xml" title="RSS Feed"></a></p>
<p></p>
<p><a href="/2022/05/14/scala-spark-na-null%E5%A4%84%E7%90%86/">2022-05-14</a></p>
<h1 id="scala-spark-na-null处理"><a href="#scala-spark-na-null处理" class="headerlink" title="scala spark na null处理"></a>scala spark na null处理</h1><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parseCheck</span></span>(s: <span class="type">String</span>,min:<span class="type">Double</span>,max:<span class="type">Double</span>): <span class="type">Option</span>[<span class="type">Double</span>] = {</span><br><span class="line">      <span class="keyword">try</span> {</span><br><span class="line">        <span class="keyword">val</span> va = s.toDouble</span><br><span class="line">        <span class="keyword">if</span> (va &lt; min || va &gt; max) {</span><br><span class="line">          <span class="type">Some</span>(va)</span><br><span class="line">        }<span class="keyword">else</span> {</span><br><span class="line">          <span class="type">Some</span>(<span class="type">Double</span>.<span class="type">NaN</span>)</span><br><span class="line">        }</span><br><span class="line">      }</span><br><span class="line">      <span class="keyword">catch</span> { <span class="keyword">case</span> _ =&gt; <span class="type">Some</span>(<span class="type">Double</span>.<span class="type">NaN</span>)</span><br><span class="line">      }</span><br><span class="line">    }</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parseDouble</span></span>(s: <span class="type">String</span>): <span class="type">Option</span>[<span class="type">Double</span>] = <span class="keyword">try</span> { <span class="type">Some</span>(s.toDouble) } <span class="keyword">catch</span> { <span class="keyword">case</span> _ =&gt; <span class="type">Some</span>(<span class="type">Double</span>.<span class="type">NaN</span>) }</span><br></pre></td></tr></tbody></table>

<p>Share</p>
<p>[<strong>Older</strong></p>
<p>scala-spark-dataframe—groupby基本操作</p>
<p>](&#x2F;2022&#x2F;05&#x2F;07&#x2F;scala-spark-dataframe%E2%80%94groupby%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C&#x2F;)</p>
<h3 id="Archives"><a href="#Archives" class="headerlink" title="Archives"></a>Archives</h3><ul>
<li><a href="/archives/2022/05/">May 2022</a></li>
</ul>
<h3 id="Recent-Posts"><a href="#Recent-Posts" class="headerlink" title="Recent Posts"></a>Recent Posts</h3><ul>
<li><a href="/2022/05/14/scala-spark-na-null%E5%A4%84%E7%90%86/">scala spark na null处理</a></li>
<li><a href="/2022/05/07/scala-spark-dataframe%E2%80%94groupby%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/">scala-spark-dataframe—groupby基本操作</a></li>
<li><a href="/2022/05/04/fedora-k8s/">fedora-k8s</a></li>
<li><a href="/2022/05/04/openwrt%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/">openwrt安装配置</a></li>
<li><a href="/2022/05/04/ubuntu-supervisor%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/">ubuntu supervisor进程管理</a></li>
</ul>
<p>© 2022 andrew<br>Powered by <a href="https://hexo.io/">Hexo</a></p>
<p><a href="/">Home</a> <a href="/archives">Archives</a></p>
<p>%</p>
]]></content>
  </entry>
  <entry>
    <title>spark group by 操作</title>
    <url>/2023/08/16/spark-group-by-%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<p>scala-spark-dataframe—groupby基本操作<br>[andrew@hadoop102 bin]$ .&#x2F;spark-shell<br>2022-05-07 20:35:13,392 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>Setting default log level to “WARN”.<br>To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).<br>Spark context Web UI available at <a href="http://hadoop102:4040/">http://hadoop102:4040</a><br>Spark context available as ‘sc’ (master &#x3D; local[*], app id &#x3D; local-1651926921962).<br>Spark session available as ‘spark’.<br>Welcome to<br>      ____              __<br>     &#x2F; <strong>&#x2F;</strong>  ___ _<em><em><strong>&#x2F; &#x2F;</strong><br>    <em>\ / _ / _ &#96;&#x2F; _<em>&#x2F;  ‘</em>&#x2F;<br>   &#x2F;_</em></em>&#x2F; .__&#x2F;_,</em>&#x2F;<em>&#x2F; &#x2F;</em>&#x2F;_\   version 3.0.0<br>      &#x2F;_&#x2F;</p>
<p>Using Scala version 2.12.10 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_212)<br>Type in expressions to have them evaluated.<br>Type :help for more information.</p>
<p>scala&gt; val df &#x3D; spark.createDataset(Seq(<br>     |   (“aaa”,1,2),(“bbb”,3,4),(“ccc”,3,5),(“bbb”,4, 6))   ).toDF(“key1”,”key2”,”key3”)<br>df: org.apache.spark.sql.DataFrame &#x3D; [key1: string, key2: int … 1 more field]</p>
<p>scala&gt; df.show()<br>+—-+—-+—-+<br>|key1|key2|key3|<br>+—-+—-+—-+<br>| aaa|   1|   2|<br>| bbb|   3|   4|<br>| ccc|   3|   5|<br>| bbb|   4|   6|<br>+—-+—-+—-+</p>
<p>scala&gt; df.printSchema()<br>root<br> |– key1: string (nullable &#x3D; true)<br> |– key2: integer (nullable &#x3D; false)<br> |– key3: integer (nullable &#x3D; false)</p>
<p>scala&gt; df.groupBy(“key1”).count.show<br>+—-+—–+<br>|key1|count|<br>+—-+—–+<br>| ccc|    1|<br>| aaa|    1|<br>| bbb|    2|<br>+—-+—–+</p>
<p>scala&gt; df.select(“key1”).distinct.show<br>+—-+<br>|key1|<br>+—-+<br>| ccc|<br>| aaa|<br>| bbb|<br>+—-+</p>
<p>scala&gt; df.select(“key1”).distinct.count<br>res4: Long &#x3D; 3</p>
<p>scala&gt; f.groupBy(“key1”).count.sort(“key1”).show<br><console>:24: error: not found: value f<br>       f.groupBy(“key1”).count.sort(“key1”).show<br>       ^</p>
<p>scala&gt; df.groupBy(“key1”).count.sort(“key1”).show<br>+—-+—–+<br>|key1|count|<br>+—-+—–+<br>| aaa|    1|<br>| bbb|    2|<br>| ccc|    1|<br>+—-+—–+</p>
<p>scala&gt; df.groupBy(“key1”).count.sort($”count”.desc).show<br>+—-+—–+<br>|key1|count|<br>+—-+—–+<br>| bbb|    2|<br>| ccc|    1|<br>| aaa|    1|<br>+—-+—–+</p>
<p>scala&gt; df.groupBy(“key1”).count.withColumnRenamed(“count”, “cnt”).sort($”cnt”.desc).show<br>+—-+—+<br>|key1|cnt|<br>+—-+—+<br>| bbb|  2|<br>| aaa|  1|<br>| ccc|  1|<br>+—-+—+</p>
<p>scala&gt; df.groupBy(“key1”).agg(count(“key1”).as(“cnt”)).show<br>+—-+—+<br>|key1|cnt|<br>+—-+—+<br>| ccc|  1|<br>| aaa|  1|<br>| bbb|  2|<br>+—-+—+</p>
<p>scala&gt;  df.groupBy(“key1”).agg(count(“key1”), max(“key2”), avg(“key3”)).show<br>+—-+———–+———+———+<br>|key1|count(key1)|max(key2)|avg(key3)|<br>+—-+———–+———+———+<br>| ccc|          1|        3|      5.0|<br>| aaa|          1|        1|      2.0|<br>| bbb|          2|        4|      5.0|<br>+—-+———–+———+———+</p>
<p>scala&gt; f.groupBy(“key1”)<br><console>:24: error: not found: value f<br>       f.groupBy(“key1”)<br>       ^</p>
<p>scala&gt;        df.groupBy(“key1”).agg(“key1”-&gt;”count”, “key2”-&gt;”max”, “key3”-&gt;”avg”).show<br>+—-+———–+———+———+<br>|key1|count(key1)|max(key2)|avg(key3)|<br>+—-+———–+———+———+<br>| ccc|          1|        3|      5.0|<br>| aaa|          1|        1|      2.0|<br>| bbb|          2|        4|      5.0|<br>+—-+———–+———+———+</p>
<p>scala&gt; df.groupBy(“key1”).agg(Map((“key1”,”count”), (“key2”,”max”), (“key3”,”avg”))).show<br>+—-+———–+———+———+<br>|key1|count(key1)|max(key2)|avg(key3)|<br>+—-+———–+———+———+<br>| ccc|          1|        3|      5.0|<br>| aaa|          1|        1|      2.0|<br>| bbb|          2|        4|      5.0|<br>+—-+———–+———+———+</p>
<p>scala&gt; df.groupBy(“key1”).agg(count(“key1”).as(“cnt”), max(“key2”).as(“max_key2”), avg(“key3”).as(“avg_key3”)).sort($”cnt”,$”max_key2”.desc).show<br>+—-+—+——–+——–+<br>|key1|cnt|max_key2|avg_key3|<br>+—-+—+——–+——–+<br>| ccc|  1|       3|     5.0|<br>| aaa|  1|       1|     2.0|<br>| bbb|  2|       4|     5.0|<br>+—-+—+——–+——–+</p>
<p>package groupby</p>
<p>import org.apache.spark.SparkConf<br>import org.apache.spark.sql.SparkSession</p>
<p>object demos {</p>
<p>  def main(args: Array[String]): Unit &#x3D; {<br>    val conf &#x3D; new SparkConf().setAppName(“LzSparkDatasetExamples”).setMaster(“local[*]”)<br>    val sparkSession &#x3D; SparkSession.builder().enableHiveSupport().config(conf).getOrCreate()</p>
<p>&#x2F;&#x2F;    &#x2F;&#x2F;LOGGER.info(“——– this is info ——–”)<br>    import sparkSession.implicits._<br>    val df &#x3D; sparkSession.createDataset(Seq(<br>      (“aaa”, 1, 2),<br>      (“bbb”, 3, 4),<br>      (“ccc”, 3, 5),<br>      (“bbb”, 4, 6)<br>    )).toDF(“key1”, “key2”, “key3”)</p>
<pre><code>import org.apache.spark.sql.functions._
//LOGGER.info(&quot;--------df.groupBy(\&quot;key1\&quot;).count().show()-----------&quot;)
df.groupBy(&quot;key1&quot;).count().show()
//LOGGER.info(&quot;--------df.select(\&quot;key1\&quot;).distinct().show()-----------&quot;)
df.select(&quot;key1&quot;).distinct().show()
val key1Count = df.select(&quot;key1&quot;).distinct().count()
//LOGGER.info(&quot;--------df.select(\&quot;key1\&quot;).distinct().count()-----------&quot; +key1Count)
//LOGGER.info(&quot;--------df.groupBy(\&quot;key1\&quot;).count().sort(\&quot;key1\&quot;).show()-----------&quot;)
df.groupBy(&quot;key1&quot;).count().sort(&quot;key1&quot;).show()
//LOGGER.info(&quot;--------df.groupBy(\&quot;key1\&quot;).count().sort($\&quot;key1\&quot;.desc).show()-----------&quot;)
df.groupBy(&quot;key1&quot;).count().sort($&quot;key1&quot;.desc).show()
//LOGGER.info(&quot;--------df.groupBy(\&quot;key1\&quot;).count.withColumnRenamed(\&quot;count\&quot;, \&quot;cnt\&quot;).sort($\&quot;cnt\&quot;.desc).show-----------&quot;)
df.groupBy(&quot;key1&quot;).count
  .withColumnRenamed(&quot;count&quot;, &quot;cnt&quot;).sort($&quot;cnt&quot;.desc).show()
//LOGGER.info(&quot;--------df.groupBy(\&quot;key1\&quot;).agg(count(\&quot;key1\&quot;).as(\&quot;cnt\&quot;)).show-----------&quot;)
df.groupBy(&quot;key1&quot;).agg(count(&quot;key1&quot;).as(&quot;cnt&quot;)).show()

// 使用agg聚合函数
df.groupBy(&quot;key1&quot;).agg(count(&quot;key1&quot;), max(&quot;key2&quot;), avg(&quot;key3&quot;)).show
df.groupBy(&quot;key1&quot;).agg(&quot;key1&quot;-&gt;&quot;count&quot;, &quot;key2&quot;-&gt;&quot;max&quot;, &quot;key3&quot;-&gt;&quot;avg&quot;).show()
df.groupBy(&quot;key1&quot;).agg(Map((&quot;key1&quot;,&quot;count&quot;), (&quot;key2&quot;,&quot;max&quot;), (&quot;key3&quot;,&quot;avg&quot;))).show()
df.groupBy(&quot;key1&quot;).agg((&quot;key1&quot;,&quot;count&quot;), (&quot;key2&quot;,&quot;max&quot;), (&quot;key3&quot;,&quot;avg&quot;)).show
df.groupBy(&quot;key1&quot;)
  .agg(count(&quot;key1&quot;).as(&quot;cnt&quot;), max(&quot;key2&quot;).as(&quot;max_key2&quot;), avg(&quot;key3&quot;).as(&quot;avg_key3&quot;))
  .sort($&quot;cnt&quot;,$&quot;max_key2&quot;.desc).show
</code></pre>
<p>  }</p>
<p>}</p>
]]></content>
  </entry>
  <entry>
    <title>ceph分布式安装</title>
    <url>/2023/08/16/ceph%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>docker基本操作</title>
    <url>/2023/08/16/docker%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>elasticsearch分布式安装</title>
    <url>/2023/08/16/elasticsearch%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>fedora k8s 安装</title>
    <url>/2023/08/16/fedora-k8s-%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>gitlab_ci runners</title>
    <url>/2023/08/16/gitlab-ci-runners/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>hadoop安装</title>
    <url>/2023/08/16/hadoop%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<p>Hadoop 下载地址:<a href="https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/">https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/</a></p>
<p>cd &#x2F;opt&#x2F;software&#x2F;</p>
<p>解压安装文件到&#x2F;opt&#x2F;module 下面<br>[andrew@hadoop101 software]$ tar -zxvf hadoop-3.1.3.tar.gz -C &#x2F;opt&#x2F;module&#x2F;</p>
<p>将 Hadoop 添加到环境变量<br>vim ~&#x2F;.bashrc</p>
<p>#HADOOP_HOME<br>export HADOOP_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3<br>export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;bin<br>export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;sbin   </p>
<p>source ~&#x2F;.bashrc</p>
<p>修改以下文件<br>vim mapred-site.xml</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	 <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>localhost:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	 <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>localhost:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>vim yarn-site.xml</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--MR shuffle --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>4096<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>4096<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 关闭yarn对物理内存和虚拟内存的限制检查 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmen-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmen-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p>vim hdfs-site.xml</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--nn web访问 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>localhost:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">     <span class="comment">&lt;!--2nn web访问 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>localhost:9868<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">     <span class="comment">&lt;!-- 指定hdfs副本数量 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///opt/module/hadoop-3.1.3/data/dfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>vim workers<br>localhost</p>
<p>vim core-site.xml</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=<span class="string">&quot;text/xsl&quot;</span> href=<span class="string">&quot;configuration.xsl&quot;</span>?&gt;</span></span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span></span><br><span class="line"><span class="comment">  you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment">  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment">  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment">  See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment">  limitations under the License. See accompanying LICENSE file.</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Put site-specific property overrides in this file. --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!--指定NameNode的地址 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- hadoop数据存放目录 --&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!--HDFS网页登入使用的的静态用户为andrew --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>andrew<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- 配置andrew允通过代理访问主机节点 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.andrew.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- 配置andrew允通过代理访问主机节点 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.andrew.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codecs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line">			org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">			org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">			org.apache.hadoop.io.compress.BZip2Codec,</span><br><span class="line">			org.apache.hadoop.io.compress.SnappyCodec,</span><br><span class="line">			com.hadoop.compression.lzo.LzoCodec,</span><br><span class="line">			com.hadoop.compression.lzo.LzopCodec</span><br><span class="line">		<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codec.lzo.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>com.hadoop.compression.lzo.LzoCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>vim hadoop-env.sh</p>
<figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="code"><pre><span class="line">export HDFS_NAMENODE_USER=andrew</span><br><span class="line">export HDFS_DATANODE_USER=andrew</span><br><span class="line">export HDFS_SECONDARYNAMENODE_USER=andrew</span><br><span class="line">export YARN_RESOURCEMANAGER_USER=andrew</span><br><span class="line">export YARN_NODEMANAGER_USER=andrew</span><br><span class="line">export HDFS_JOURNALNODE_USER=andrew</span><br><span class="line">export HDFS_ZKFC_USER=andrew</span><br><span class="line">export HADOOP_SHELL_EXECNAME=andrew</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>k8s安装</title>
    <url>/2023/08/16/k8s%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>kubeflow基础搭建</title>
    <url>/2023/08/16/kubeflow%E5%9F%BA%E7%A1%80%E6%90%AD%E5%BB%BA/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>linux硬盘格式化与挂载</title>
    <url>/2023/08/16/linux%E7%A1%AC%E7%9B%98%E6%A0%BC%E5%BC%8F%E5%8C%96%E4%B8%8E%E6%8C%82%E8%BD%BD/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>loguru elasticsearch kibana 日志处理</title>
    <url>/2023/08/16/loguru-elasticsearch-kibana-%E6%97%A5%E5%BF%97%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>minio分布式安装</title>
    <url>/2023/08/16/minio%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>mongodb 分布式安装</title>
    <url>/2023/08/16/mongodb-%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<p>参考搭建文档<a href="https://blog.csdn.net/msh6453/article/details/131161845">https://blog.csdn.net/msh6453/article/details/131161845</a></p>
<p>前言<br>官方文档：<a href="https://www.mongodb.com/docs/%EF%BC%88%E5%8F%AF%E4%BB%A5%E5%8F%82%E8%80%83%EF%BC%89">https://www.mongodb.com/docs/（可以参考）</a></p>
<p>一，安装说明<br>1.1环境说明<br>1、首先确定部署的环境，确定下服务器的端口，一般默认是22的端口；<br>2、操作系统Centos7.9；<br>3、 数据库mongodb-linux-x86_64-rhel70-4.4.22。<br>mongodb版本4.4.22</p>
<p><img src="/image-1.png" alt="Alt text"></p>
<p>mongos，数据库集群请求的入口，所有的请求都通过mongos进行协调，不需要在应用程序添加一个路由选择器，mongos自己就是一个请求分发中心，它负责把对应的数据请求请求转发到对应的shard服务器上。在生产环境通常有多mongos作为请求的入口，防止其中一个挂掉所有的mongodb请求都没有办法操作。</p>
<p>config server，顾名思义为配置服务器，存储所有数据库元信息（路由、分片）的配置。mongos本身没有物理存储分片服务器和数据路由信息，只是缓存在内存里，配置服务器则实际存储这些数据。mongos第一次启动或者关掉重启就会从 config server 加载配置信息，以后如果配置服务器信息变化会通知到所有的 mongos 更新自己的状态，这样 mongos 就能继续准确路由。在生产环境通常有多个 config server 配置服务器，因为它存储了分片路由的元数据，防止数据丢失！</p>
<p>shard，分片（sharding）是指将数据库拆分，将其分散在不同的机器上的过程。将数据分散到不同的机器上，不需要功能强大的服务器就可以存储更多的数据和处理更大的负载。基本思想就是将集合切成小块，这些块分散到若干片里，每个片只负责总数据的一部分，最后通过一个均衡器来对各个分片进行均衡（数据迁移）。</p>
<p>replica set，中文翻译副本集，其实就是shard的备份，防止shard挂掉之后数据丢失。复制提供了数据的冗余备份，并在多个服务器上存储数据副本，提高了数据的可用性， 并可以保证数据的安全性。</p>
<p>仲裁者（Arbiter），是复制集中的一个MongoDB实例，它并不保存数据。仲裁节点使用最小的资源并且不要求硬件设备，不能将Arbiter部署在同一个数据集节点中，可以部署在其他应用服务器或者监视服务器中，也可部署在单独的虚拟机中。为了确保复制集中有奇数的投票成员（包括primary），需要添加仲裁节点做为投票，否则primary不能运行时不会自动切换primary。</p>
<p>简单了解之后，我们可以这样总结一下，应用请求mongos来操作mongodb的增删改查，配置服务器存储数据库元信息，并且和mongos做同步，数据最终存入在shard（分片）上，为了防止数据丢失同步在副本集中存储了一份，仲裁在数据存储到分片的时候决定存储到哪个节点。</p>
<table>
<thead>
<tr>
<th>服务器</th>
<th>ceph-node-1</th>
<th>ceph-node-2</th>
<th>ceph-node-3</th>
</tr>
</thead>
<tbody><tr>
<td>ip</td>
<td>10.30.0.48</td>
<td>10.30.0.49</td>
<td>10.30.0.50</td>
</tr>
<tr>
<td>server-route</td>
<td>mongos</td>
<td>mongos</td>
<td>mongos</td>
</tr>
<tr>
<td>server-config</td>
<td>config server</td>
<td>config server</td>
<td>config server</td>
</tr>
<tr>
<td>server-config</td>
<td>shard server1 主节点</td>
<td>shard server1 副节点</td>
<td>shard server1 仲裁</td>
</tr>
<tr>
<td>server-config</td>
<td>shard server2 仲裁</td>
<td>shard server2 主节点</td>
<td>shard server2 副节点</td>
</tr>
<tr>
<td>server-config</td>
<td>shard server3 副节点</td>
<td>shard server3 仲裁</td>
<td>shard server3 主节点</td>
</tr>
</tbody></table>
<h2 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -p /opt/mongo/MongoDB/conf</span><br><span class="line"><span class="built_in">mkdir</span> -p /opt/mongo/MongoDB/server</span><br><span class="line"><span class="built_in">mkdir</span> -p /opt/mongo/MongoDB/mongos/log</span><br><span class="line"><span class="built_in">mkdir</span> -p /opt/mongo/MongoDB/config/data</span><br><span class="line"><span class="built_in">mkdir</span> -p /opt/mongo/MongoDB/config/log</span><br><span class="line"><span class="built_in">mkdir</span> -p /opt/mongo/MongoDB/shard1/data</span><br><span class="line"><span class="built_in">mkdir</span> -p /opt/mongo/MongoDB/shard1/log</span><br><span class="line"><span class="built_in">mkdir</span> -p /opt/mongo/MongoDB/shard2/data</span><br><span class="line"><span class="built_in">mkdir</span> -p /opt/mongo/MongoDB/shard2/log</span><br><span class="line"><span class="built_in">mkdir</span> -p /opt/mongo/MongoDB/shard3/data</span><br><span class="line"><span class="built_in">mkdir</span> -p /opt/mongo/MongoDB/shard3/log</span><br><span class="line"></span><br><span class="line">wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-4.4.22.tgz</span><br><span class="line">tar -xvzf mongodb-linux-x86_64-rhel70-4.4.22.tgz -C /opt/mongo/MongoDB/server/</span><br><span class="line"><span class="built_in">mv</span> /opt/mongo/MongoDB/server/mongodb-linux-x86_64-rhel70-4.4.22 /opt/mongo/MongoDB/server/mongodb</span><br><span class="line"></span><br><span class="line">vim /etc/profile</span><br><span class="line"><span class="built_in">export</span> MONGODB_HOME=/opt/mongo/MongoDB/server/mongodb</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$MONGODB_HOME</span>/bin:<span class="variable">$PATH</span></span><br><span class="line">:wq!</span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br><span class="line">vim /opt/mongo/MongoDB/conf/config.conf </span><br><span class="line">vim /opt/mongo/MongoDB/conf/shard1.conf</span><br><span class="line">vim /opt/mongo/MongoDB/conf/shard2.conf</span><br><span class="line">vim /opt/mongo/MongoDB/conf/shard3.conf</span><br><span class="line">vim /opt/mongo/MongoDB/conf/mongos.conf</span><br></pre></td></tr></table></figure>

<p>配置文件如mongo&#x2F;conf 下面<br>注意安装的时候不要设置最后的安全密钥，待完毕后添加</p>
<p>启动服务顺序</p>
<p>mongod -f &#x2F;opt&#x2F;mongo&#x2F;MongoDB&#x2F;conf&#x2F;config.conf<br>mongod -f &#x2F;opt&#x2F;mongo&#x2F;MongoDB&#x2F;conf&#x2F;shard1.conf<br>mongod -f &#x2F;opt&#x2F;mongo&#x2F;MongoDB&#x2F;conf&#x2F;shard2.conf<br>mongod -f &#x2F;opt&#x2F;mongo&#x2F;MongoDB&#x2F;conf&#x2F;shard3.conf<br>mongos -f &#x2F;opt&#x2F;mongo&#x2F;MongoDB&#x2F;conf&#x2F;mongos.conf</p>
<p>随便登入一台</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mongo 10.30.0.48:21000</span><br><span class="line">use admin</span><br><span class="line">config = &#123;_id : &quot;config&quot;,members : [&#123;_id : 0, host : &quot;10.30.0.48:21000&quot; &#125;,</span><br><span class="line">&#123;_id : 1, host : &quot;10.30.0.49:21000&quot; &#125;,&#123;_id : 2, host : &quot;10.30.0.50:21000&quot; &#125;]&#125;</span><br><span class="line">rs.initiate(config)</span><br></pre></td></tr></table></figure>


<p>同理 server sharded1</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mongo 10.30.0.48:27001</span><br><span class="line">use admin</span><br><span class="line">config = &#123; _id : &quot;shard1&quot;,members : [&#123;_id : 0, host : &quot;10.30.0.48:27001&quot; ,priority: 2 &#125;,&#123;_id : 1, host : &quot;10.30.0.49:27001&quot; ,priority: 1 &#125;,&#123;_id : 2, host : &quot;10.30.0.50:27001&quot;,arbiterOnly: true&#125;]&#125;</span><br><span class="line">//(“priority”优先级，数字越大，优先等级越高；“arbiterOnly”冲裁节点；冲裁节点根据优先等级判断哪个节点作为主节点)</span><br><span class="line">rs.initiate(config)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mongo 10.30.0.48:27002</span><br><span class="line">server shard2</span><br><span class="line">use admin</span><br><span class="line">config = &#123; _id : <span class="string">&quot;shard2&quot;</span>,members : [&#123;_id : 0, host : <span class="string">&quot;10.30.0.48:27002&quot;</span> ,arbiterOnly: <span class="literal">true</span> &#125;,</span><br><span class="line">&#123;_id : 1, host : <span class="string">&quot;10.30.0.49:27002&quot;</span> ,priority: 2 &#125;,&#123;_id : 2, host : <span class="string">&quot;10.30.0.50:27002&quot;</span>,priority: 1&#125;]&#125;</span><br><span class="line">rs.initiate(config)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>server shard3</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mongo 10.30.0.48:27003</span><br><span class="line">use admin</span><br><span class="line">config = &#123; _id : &quot;shard3&quot;,members : [&#123;_id : 0, host : &quot;10.30.0.48:27003&quot; ,priority: 1 &#125;,</span><br><span class="line"></span><br><span class="line">&#123;_id : 1, host : &quot;10.30.0.49:27003&quot; ,arbiterOnly: true &#125;,&#123;_id : 2, host : &quot;10.30.0.50:27003&quot;,priority: 2&#125;]&#125;</span><br><span class="line"></span><br><span class="line">rs.initiate(config)</span><br></pre></td></tr></table></figure>



<p>添加分片服务器</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">use admin</span><br><span class="line">sh.addShard(<span class="string">&quot;shard1/10.30.0.48:27001,10.30.0.49:27001,10.30.0.50:27001&quot;</span>)</span><br><span class="line"></span><br><span class="line">sh.addShard(<span class="string">&quot;shard2/10.30.0.48:27002,10.30.0.49:27002,10.30.0.50:27002&quot;</span>)</span><br><span class="line"></span><br><span class="line">sh.addShard(<span class="string">&quot;shard3/10.30.0.48:27003,10.30.0.49:27003,10.30.0.50:27003&quot;</span>)</span><br><span class="line"></span><br><span class="line">sh.status()</span><br></pre></td></tr></table></figure>


<p>创建用户</p>
<p>执行命令： mongo -port 20000<br>执行命令： use admin&#x2F;&#x2F;这个条件是必须的<br>执行命令：db.createUser(</p>
<pre><code>&#123;

    user:&quot;ml_grp&quot;,

    pwd:&quot;ml&amp;dl#mongodb&quot;,

    roles:[&#123;role:&quot;root&quot;,db:&quot;admin&quot;&#125;]

&#125;
</code></pre>
<p>)</p>
<p>use admin<br>执行命令：db.auth(‘ml_grp’,’passwd’)<br>执行命令：db.runCommand( { enablesharding :”zjxndc”});&#x2F;&#x2F;为zjxndc开启分片功能<br>执行命令：db.runCommand( { shardcollection : “zjxndc.measureHisvalues”,key : {_id: 1} } )&#x2F;</p>
<p>use config<br>db.settings.save({“_id”:”chunksize”,”value”:1})<br>use zjxndc<br>d、执行命令：for (var i &#x3D; 1; i &lt;&#x3D; 100000; i++){</p>
<p>db.measureHisvalues.insert({“_id”:i,”test1”:”testval1”+i});</p>
<p>}</p>
<p><img src="/1408af41c022dc62726ecf884c95ff0a.png" alt="enable partition"><br><img src="/image.png" alt="查看是否分区成功"></p>
<h3 id="配置安全"><a href="#配置安全" class="headerlink" title="配置安全"></a>配置安全</h3><p>4.1 安全验证设置用户<br>1、副本集和共享集群的各个节点成员之间使用内部身份验证，可以使用密钥文件或x.509证书。密钥文件比较简单，官方推荐如果是测试环境可以使用密钥文件，但是正是环境，官方推荐x.509证书。原理就是，集群中每一个实例彼此连接的时候都检验彼此使用的证书的内容是否相同。只有证书相同的实例彼此才可以访问。使用客户端连接到mongodb集群时，开启访问授权。对于集群外部的访问。如通过可视化客户端，或者通过代码连接的时候，需要开启授权。<br>a、生成密钥文件，在keyfile身份验证中，副本集中的每个mongod实例都使用keyfile的内容作为共享密码，只有具有正确密钥文件的mongod或者mongos实例可以连接到副本集。密钥文件的内容必须在6到1024个字符之间，并且在unix&#x2F;linux系统中文件所有者必须有对文件至少有读的权限。可以用任何方式生成密钥文件例如：(任意一台机器即可)</p>
<p>执行命令：openssl rand -base64 756 &gt; &#x2F;opt&#x2F;mongo&#x2F;MongoDB&#x2F;testKeyFile.file &#x2F;&#x2F;生成密钥</p>
<p>执行命令：chmod 400 &#x2F;opt&#x2F;mongo&#x2F;MongoDB&#x2F;testKeyFile.file &#x2F;&#x2F;赋予权限</p>
<p>执行命令：scp -P22 &#x2F;opt&#x2F;mongo&#x2F;MongoDB&#x2F;testKeyFile.file <a href="mailto:&#114;&#x6f;&#111;&#116;&#64;&#x31;&#x30;&#46;&#51;&#x30;&#x2e;&#48;&#46;&#52;&#57;">&#114;&#x6f;&#111;&#116;&#64;&#x31;&#x30;&#46;&#51;&#x30;&#x2e;&#48;&#46;&#52;&#57;</a>:&#x2F;opt&#x2F;mongo&#x2F;MongoDB &#x2F;&#x2F;拷贝至其他0.55服务器上（“-P22”是端口，根据实际情况来）</p>
<p>执行命令：scp -P22 &#x2F;opt&#x2F;mongo&#x2F;MongoDB&#x2F;testKeyFile.file root@:10.30.0.49 &#x2F;opt&#x2F;mongo&#x2F;MongoDB &#x2F;&#x2F;拷贝至其他0.56服务器上</p>
<p>创建用户成功后，关闭所有的节点（三台机都需要操作）</p>
<p>执行命令：按照先后顺序来处理关闭，mongos&gt;config&gt;shadr3&gt;shadr2&gt;shadr1</p>
<p>&#x2F;&#x2F;注意的是每一个服务的关闭都需要在三台机上关闭，在关闭其他服务。例如关闭shadr3服务，先关闭0.54服务器上的shadr3服务，其次0.55服务器上的shadr3服务，再是0.56服务器上的shadr3服务；然后在关闭shadr2服务，也是按照这个顺序处理。（这个地方主要新手操作避免出错）</p>
<p>执行命令：</p>
<p>mongod -f &#x2F;opt&#x2F;mongo&#x2F;MongoDB&#x2F;conf&#x2F;shard1.conf –shutdown</p>
<p>mongod -f &#x2F;opt&#x2F;mongo&#x2F;MongoDB&#x2F;conf&#x2F;shard2.conf –shutdown</p>
<p>mongod -f &#x2F;opt&#x2F;mongo&#x2F;MongoDB&#x2F;conf&#x2F;shard3.conf –shutdown</p>
<p>mongod -f &#x2F;opt&#x2F;mongo&#x2F;MongoDB&#x2F;conf&#x2F;config.conf –shutdown</p>
<p>mongo 10.30.0.48:20000&#x2F;&#x2F;mongos需要这样关闭，用上面的命令有问题。</p>
<p>use admin<br>db.auth(‘ml_grp’,’passwd’)<br>db.shutdownServer()</p>
<p>3、配置testKeyFile.file，依次在每台机器上的mongos.conf、config.conf、shard1.conf、shard2.conf、shard3.conf的配置和开启授权验证。<br>a、先是config.conf、shard1.conf、shard2.conf、shard3.conf的配置和开启授权验证。（三台机器的这些文件都需要添加）<br>在conf这几个文件的的最后添加：<br>security:</p>
<p>  keyFile: &#x2F;opt&#x2F;mongo&#x2F;MongoDB&#x2F;testKeyFile.file</p>
<p>  authorization: enabled</p>
<p>b、然后在三台机器的mongos.conf配置文件中最后添加：<br>security:</p>
<p>  keyFile: &#x2F;opt&#x2F;mongo&#x2F;MongoDB&#x2F;testKeyFile.file</p>
<p>&#x2F;&#x2F;这里就说明了testKeyFile.file最好在每台机器放在一个位置，为了后面复制粘贴处理</p>
<p>&#x2F;&#x2F;解释说明： mongos比mongod少了authorization：enabled的配置。原因是，副本集加分片的安全认证需要配置两方面的，副本集各个节点之间使用内部身份验证，用于内部各个mongo实例的通信，只有相同keyfile才能相互访问。所以都要开启keyFile: &#x2F;data&#x2F;mongodb&#x2F;testKeyFile.file</p>
<pre><code>然而对于所有的mongod，才是真正的保存数据的分片。mongos只做路由，不保存数据。所以所有的mongod开启访问数据的授权authorization:enabled。这样用户只有账号密码正确才能访问到数据
</code></pre>
]]></content>
  </entry>
  <entry>
    <title>redis分布式安装</title>
    <url>/2023/08/16/redis%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<p>1.环境配置<br>#在所有节点配置YUM：<br>#清空原来自带配置文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /etc/yum.repos.d/</span><br><span class="line"><span class="built_in">mkdir</span> /tmp/bak</span><br><span class="line"><span class="built_in">mv</span> * /tmp/bak/</span><br></pre></td></tr></table></figure>
<p>#配置系统源码，epel源：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo</span><br><span class="line">yum install wget -y</span><br><span class="line">wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo</span><br><span class="line"><span class="comment">#YUM优先级别：</span></span><br><span class="line">yum -y install yum-plugin-priorities.noarch</span><br></pre></td></tr></table></figure>
<p>#配置ceph源：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cat</span> &lt;&lt; <span class="string">EOF | tee /etc/yum.repos.d/ceph.repo</span></span><br><span class="line"><span class="string">[Ceph]</span></span><br><span class="line"><span class="string">name=Ceph packages for $basearch</span></span><br><span class="line"><span class="string">baseurl=http://mirrors.163.com/ceph/rpm-nautilus/el7/\$basearch</span></span><br><span class="line"><span class="string">enabled=1</span></span><br><span class="line"><span class="string">gpgcheck=1</span></span><br><span class="line"><span class="string">type=rpm-md</span></span><br><span class="line"><span class="string">gpgkey=https://download.ceph.com/keys/release.asc</span></span><br><span class="line"><span class="string">priority=1</span></span><br><span class="line"><span class="string">[Ceph-noarch]</span></span><br><span class="line"><span class="string">name=Ceph noarch packages</span></span><br><span class="line"><span class="string">baseurl=http://mirrors.163.com/ceph/rpm-nautilus/el7/noarch</span></span><br><span class="line"><span class="string">enabled=1</span></span><br><span class="line"><span class="string">gpgcheck=1</span></span><br><span class="line"><span class="string">type=rpm-md</span></span><br><span class="line"><span class="string">gpgkey=https://download.ceph.com/keys/release.asc</span></span><br><span class="line"><span class="string">priority=1</span></span><br><span class="line"><span class="string">[ceph-source]</span></span><br><span class="line"><span class="string">name=Ceph source packages</span></span><br><span class="line"><span class="string">baseurl=http://mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS</span></span><br><span class="line"><span class="string">enabled=1</span></span><br><span class="line"><span class="string">gpgcheck=1</span></span><br><span class="line"><span class="string">type=rpm-md</span></span><br><span class="line"><span class="string">gpgkey=https://download.ceph.com/keys/release.asc</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure>
<p>#关闭防火墙：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld</span><br><span class="line">systemctl status firewalld</span><br></pre></td></tr></table></figure>
<p>#配置主机名称：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph1节点：</span><br><span class="line">hostnamectl --static set-hostname ceph1</span><br><span class="line">ceph2节点：</span><br><span class="line">hostnamectl --static set-hostname ceph2</span><br><span class="line">ceph3节点：</span><br><span class="line">hostnamectl --static set-hostname ceph3</span><br></pre></td></tr></table></figure>
<p>#所有节点配置hosts文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/etc/hosts</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">192.168.0.231    ceph1</span><br><span class="line">192.168.0.232    ceph2</span><br><span class="line">192.168.0.233    ceph3</span><br></pre></td></tr></table></figure>

<p>#所有节点NTP配置：<br>在所有集群和客户端节点安装NTP，修改配置。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum -y install ntp ntpdate</span><br><span class="line"><span class="comment"># 以ceph1为NTP服务端节点，在ceph1新建NTP文件。</span></span><br><span class="line">vi /etc/ntp.conf</span><br><span class="line"><span class="comment"># 并新增如下内容作为NTP服务端：</span></span><br><span class="line">restrict 127.0.0.1</span><br><span class="line">restrict ::1</span><br><span class="line">restrict 192.168.3.0 mask 255.255.255.0 //ceph1的网段与掩码</span><br><span class="line">server 127.127.1.0</span><br><span class="line">fudge 127.127.1.0 stratum 8</span><br></pre></td></tr></table></figure>
<p>在ceph2、ceph3及所有客户机节点新建NTP文件。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vi /etc/ntp.conf</span><br><span class="line"><span class="comment"># 并新增如下内容作为客户端：</span></span><br><span class="line">server 192.168.3.166</span><br><span class="line"></span><br><span class="line">systemctl start ntpd</span><br><span class="line">systemctl <span class="built_in">enable</span> ntpd</span><br><span class="line">systemctl status ntpd</span><br></pre></td></tr></table></figure>
<p>#ssh配置，在ceph1节点生成公钥，并发放到各个主机&#x2F;客户机节点。：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa <span class="comment">#回车采取默认配置</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..3&#125;; <span class="keyword">do</span> ssh-copy-id ceph<span class="variable">$i</span>; <span class="keyword">done</span> <span class="comment">#根据提示输入yes及节点密码</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..3&#125;; <span class="keyword">do</span> ssh-copy-id client<span class="variable">$i</span>; <span class="keyword">done</span></span><br></pre></td></tr></table></figure>

<p>#在所有节点，关闭SELinux</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sed -i <span class="string">&#x27;s/enforcing/disabled/&#x27;</span> /etc/selinux/config</span><br><span class="line">setenforce 0</span><br></pre></td></tr></table></figure>
<ol>
<li>安装Ceph软件<br>使用yum install安装ceph的时候会默认安装当前已有的最新版，如果不想安装最新版本，可以在&#x2F;etc&#x2F;yum.conf文件中加以限制。<br>2.1 在所有集群和客户端节点安装Ceph<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum -y install ceph</span><br><span class="line">ceph -v命令查看版本:</span><br><span class="line">[root@ceph1 ~]<span class="comment"># ceph -v</span></span><br><span class="line">ceph version 14.2.9 (581f22da52345dba46ee232b73b990f06029a2a0) nautilus (stable)</span><br><span class="line">[root@ceph2 ~]<span class="comment"># ceph -v</span></span><br><span class="line">ceph version 14.2.9 (581f22da52345dba46ee232b73b990f06029a2a0) nautilus (stable)</span><br><span class="line">[root@ceph3 ~]<span class="comment"># ceph -v</span></span><br><span class="line">ceph version 14.2.9 (581f22da52345dba46ee232b73b990f06029a2a0) nautilus (stable)</span><br></pre></td></tr></table></figure>
2.2 在ceph1节点额外安装ceph-deploy。<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum -y install ceph-deploy</span><br></pre></td></tr></table></figure>
3.部署MON节点<br>3.1 创建目录生成配置文件<figure class="highlight stata"><table><tr><td class="code"><pre><span class="line"><span class="keyword">mkdir</span> <span class="keyword">cluster</span></span><br><span class="line"><span class="keyword">cd</span> <span class="keyword">cluster</span></span><br><span class="line">ceph-deploy new ceph1 ceph2 ceph3</span><br></pre></td></tr></table></figure></li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@ceph1 ~]<span class="comment"># cd cluster/</span></span><br><span class="line">[root@ceph1 cluster]<span class="comment"># ceph-deploy new ceph1 ceph2 ceph3</span></span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy new ceph1 ceph2 ceph3</span><br><span class="line">[ceph_deploy.cli][INFO  ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO  ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  func                          : &lt;<span class="keyword">function</span> new at 0x7ffb7dc07de8&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  overwrite_conf                : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  quiet                         : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7ffb7d58c6c8&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO  ]  ssh_copykey                   : True</span><br><span class="line">[ceph_deploy.cli][INFO  ]  mon                           : [<span class="string">&#x27;ceph1&#x27;</span>, <span class="string">&#x27;ceph2&#x27;</span>, <span class="string">&#x27;ceph3&#x27;</span>]</span><br><span class="line">[ceph_deploy.cli][INFO  ]  public_network                : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  ceph_conf                     : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cluster_network               : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  default_release               : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  fsid                          : None</span><br><span class="line">[ceph_deploy.new][DEBUG ] Creating new cluster named ceph</span><br><span class="line">[ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds</span><br><span class="line">[ceph1][DEBUG ] connected to host: ceph1 </span><br><span class="line">[ceph1][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph1][DEBUG ] detect machine <span class="built_in">type</span></span><br><span class="line">[ceph1][DEBUG ] find the location of an executable</span><br><span class="line">[ceph1][INFO  ] Running <span class="built_in">command</span>: /usr/sbin/ip <span class="built_in">link</span> show</span><br><span class="line">[ceph1][INFO  ] Running <span class="built_in">command</span>: /usr/sbin/ip addr show</span><br><span class="line">[ceph1][DEBUG ] IP addresses found: [u<span class="string">&#x27;192.168.0.231&#x27;</span>]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Resolving host ceph1</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor ceph1 at 192.168.0.231</span><br><span class="line">[ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds</span><br><span class="line">[ceph2][DEBUG ] connected to host: ceph1 </span><br><span class="line">[ceph2][INFO  ] Running <span class="built_in">command</span>: ssh -CT -o BatchMode=<span class="built_in">yes</span> ceph2</span><br><span class="line">[ceph2][DEBUG ] connected to host: ceph2 </span><br><span class="line">[ceph2][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph2][DEBUG ] detect machine <span class="built_in">type</span></span><br><span class="line">[ceph2][DEBUG ] find the location of an executable</span><br><span class="line">[ceph2][INFO  ] Running <span class="built_in">command</span>: /usr/sbin/ip <span class="built_in">link</span> show</span><br><span class="line">[ceph2][INFO  ] Running <span class="built_in">command</span>: /usr/sbin/ip addr show</span><br><span class="line">[ceph2][DEBUG ] IP addresses found: [u<span class="string">&#x27;192.168.0.232&#x27;</span>]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Resolving host ceph2</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor ceph2 at 192.168.0.232</span><br><span class="line">[ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds</span><br><span class="line">[ceph3][DEBUG ] connected to host: ceph1 </span><br><span class="line">[ceph3][INFO  ] Running <span class="built_in">command</span>: ssh -CT -o BatchMode=<span class="built_in">yes</span> ceph3</span><br><span class="line">[ceph3][DEBUG ] connected to host: ceph3 </span><br><span class="line">[ceph3][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph3][DEBUG ] detect machine <span class="built_in">type</span></span><br><span class="line">[ceph3][DEBUG ] find the location of an executable</span><br><span class="line">[ceph3][INFO  ] Running <span class="built_in">command</span>: /usr/sbin/ip <span class="built_in">link</span> show</span><br><span class="line">[ceph3][INFO  ] Running <span class="built_in">command</span>: /usr/sbin/ip addr show</span><br><span class="line">[ceph3][DEBUG ] IP addresses found: [u<span class="string">&#x27;192.168.0.233&#x27;</span>]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Resolving host ceph3</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor ceph3 at 192.168.0.233</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor initial members are [<span class="string">&#x27;ceph1&#x27;</span>, <span class="string">&#x27;ceph2&#x27;</span>, <span class="string">&#x27;ceph3&#x27;</span>]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor addrs are [<span class="string">&#x27;192.168.0.231&#x27;</span>, <span class="string">&#x27;192.168.0.232&#x27;</span>, <span class="string">&#x27;192.168.0.233&#x27;</span>]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Creating a random mon key...</span><br><span class="line">[ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...</span><br><span class="line">[ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...</span><br></pre></td></tr></table></figure>
<p>3.2 初始化密钥</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-deploy mon create-initial</span><br></pre></td></tr></table></figure>
<p>3.3 将ceph.client.admin.keyring拷贝到各个节点上</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-deploy --overwrite-conf admin ceph1 ceph2 ceph3</span><br></pre></td></tr></table></figure>
<p>3.4 查看是否配置成功。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@ceph1 cluster]<span class="comment"># ceph -scluster:id:     ea192428-05d2-437a-8cce-9d187de82dd5health: HEALTH_OKservices:mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 5m)mgr: no daemons activeosd: 0 osds: 0 up, 0 indata:pools:   0 pools, 0 pgsobjects: 0 objects, 0 Busage:   0 B used, 0 B / 0 B availpgs:  </span></span><br></pre></td></tr></table></figure>
<p>4 部署MGR节点</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-deploy mgr create ceph1 ceph2 ceph3</span><br></pre></td></tr></table></figure>
<p>查看MGR是否部署成功。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph -s</span><br><span class="line">[root@ceph1 cluster]<span class="comment"># ceph -scluster:id:     ea192428-05d2-437a-8cce-9d187de82dd5health: HEALTH_WARNOSD count 0 &lt; osd_pool_default_size 3services:mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 8m)mgr: ceph1(active, since 22s), standbys: ceph2, ceph3osd: 0 osds: 0 up, 0 indata:pools:   0 pools, 0 pgsobjects: 0 objects, 0 Busage:   0 B used, 0 B / 0 B availpgs: </span></span><br><span class="line">```  </span><br><span class="line">5 部署OSD节点</span><br><span class="line">```bash</span><br><span class="line">ceph-deploy osd create --data /dev/sdb ceph1</span><br><span class="line">ceph-deploy osd create --data /dev/sdc ceph1</span><br><span class="line">ceph-deploy osd create --data /dev/sdd ceph1</span><br><span class="line">ceph-deploy osd create --data /dev/sdb ceph2</span><br><span class="line">ceph-deploy osd create --data /dev/sdc ceph2</span><br><span class="line">ceph-deploy osd create --data /dev/sdd ceph2</span><br><span class="line">ceph-deploy osd create --data /dev/sdb ceph3</span><br><span class="line">ceph-deploy osd create --data /dev/sdc ceph3</span><br><span class="line">ceph-deploy osd create --data /dev/sdd ceph3</span><br></pre></td></tr></table></figure>
<p>创建成功后，查看是否正常</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@ceph1 cluster]<span class="comment"># ceph -scluster:id:     ea192428-05d2-437a-8cce-9d187de82dd5health: HEALTH_OKservices:mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 14m)mgr: ceph1(active, since 6m), standbys: ceph2, ceph3osd: 9 osds: 9 up (since 2m), 9 in (since 2m)data:pools:   0 pools, 0 pgsobjects: 0 objects, 0 Busage:   9.0 GiB used, 135 GiB / 144 GiB availpgs:  </span></span><br></pre></td></tr></table></figure>
<p>6 验证Ceph<br>创建存储池</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd pool create vdbench 10 10</span><br></pre></td></tr></table></figure>
<p>创建块设备</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd create image01 --size 200--pool vdbench --image-format 2 --image-feature layering</span><br><span class="line">rbd <span class="built_in">ls</span> --pool vdbench</span><br><span class="line">[root@ceph1 cluster]<span class="comment"># rbd create image01 --size 200 --pool  vdbench --image-format 2 --image-feature layering</span></span><br><span class="line">[root@ceph1 cluster]<span class="comment"># rbd ls --pool vdbench</span></span><br><span class="line">image01</span><br></pre></td></tr></table></figure>

<p>#PG 分 配 计 算<br>归置组(PG)的数量是由管理员在创建存储池的时候指定的，然后由 CRUSH 负责创建和使用，PG 的数量是 2 的 N 次方的倍数,每个 OSD 的 PG 不要超出 250 个 PG<br>Total PGs &#x3D; (Total_number_of_OSD * 100) &#x2F; max_replication_count<br>单个 pool 的 PG 计算如下：<br>有 100 个 osd，3 副本，5 个 pool<br>Total PGs &#x3D;100*100&#x2F;3&#x3D;3333<br>每个 pool 的 PG&#x3D;3333&#x2F;5&#x3D;512，那么创建 pool 的时候就指定 pg 为 512<br>客户端在读写对象时，需要提供的是对象标识和存储池名称<br>客户端需要在存储池中读写对象时，需要客户端将对象名称，对象名称的hash码，存储池中的PG数量和存储池名称作为输入信息提供给ceph，然后由CRUSH计算出PG的ID以及PG针对的主OSD即可读写OSD中的对象。<br>具体写操作如下：<br>1.APP向ceph客户端发送对某个对象的请求，此请求包含对象和存储池，然后ceph客户端对访问的对象做hash计算，并根据此hash值计算出对象所在的PG，完成对象从Pool至PG的映射。<br>APP 访问 pool ID 和 object ID （比如 pool &#x3D; pool1 and object-id &#x3D; “name1”）<br>ceph client 对 objectID 做哈希<br>ceph client 对该 hash 值取 PG 总数的模，得到 PG 编号(比如 32)<br>ceph client 对 pool ID 取 hash（比如 “pool1” &#x3D; 3）<br>ceph client 将 pool ID 和 PG ID 组合在一起(比如 3.23)得到 PG 的完整 ID。<br>2.然后客户端据 PG、CRUSH 运行图和归置组(placement rules)作为输入参数并再次进行计<br>算，并计算出对象所在的 PG 内的主 OSD ，从而完成对象从 PG 到 OSD 的映射。<br>3.客户端开始对主 OSD 进行读写请求(副本池 IO)，如果发生了写操作，会有 ceph 服务端完<br>成对象从主 OSD 到备份 OSD 的同步  </p>
<p>二.熟练 ceph 的用户管理及授权<br>客户端使用 session key 向 mon 请求所需要的服务，mon 向客户端提供一个 tiket，用于向实际处理数据的 OSD 等服务验证客户端身份，MON 和 OSD 共享同一个 secret.<br>ceph 用户需要拥有存储池访问权限，才能读取和写入数据<br>ceph 用户必须拥有执行权限才能使用 ceph 的管理命令<br>ceph 支持多种类型的用户，但可管理的用户都属于 client 类型<br>通过点号来分割用户类型和用户名，格式为 TYPE.ID，例如 client.admin。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@ceph-deploy:~<span class="comment"># cat /etc/ceph/ceph.client.admin.keyring </span></span><br><span class="line">[client.admin]</span><br><span class="line">        key = AQBnFaNj1iyBMBAAd+9hKWXaNw3GYxT9PEXvrQ==</span><br><span class="line">        caps mds = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">        caps mgr = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">        caps mon = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">        caps osd = <span class="string">&quot;allow *&quot;</span></span><br></pre></td></tr></table></figure>
<p>#列 出 指 定 用 户 信 息</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@ceph-deploy:~<span class="comment"># ceph auth get osd.10</span></span><br><span class="line">[osd.10]</span><br><span class="line">        key = AQB+I6Njk4KWNBAAL09FFayLKF44IgUQ1fjKYQ==</span><br><span class="line">        caps mgr = <span class="string">&quot;allow profile osd&quot;</span></span><br><span class="line">        caps mon = <span class="string">&quot;allow profile osd&quot;</span></span><br><span class="line">        caps osd = <span class="string">&quot;allow *&quot;</span></span><br><span class="line"></span><br><span class="line">exported keyring <span class="keyword">for</span> osd.10</span><br></pre></td></tr></table></figure>
<p>#: 列 出 用 户</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~$ ceph auth list</span><br><span class="line">mds.ceph-mgr1</span><br><span class="line">        key: AQAdRbFjOwBXIRAAUTdwElBzYPHW+4uFicFC7Q==</span><br><span class="line">        caps: [mds] allow</span><br><span class="line">        caps: [mon] allow profile mds</span><br><span class="line">        caps: [osd] allow rwx</span><br><span class="line">osd.0</span><br><span class="line">        key: AQC0IqNjbcgKIxAA+BCNpQeZiMujR+r+69Miig==</span><br><span class="line">        caps: [mgr] allow profile osd</span><br><span class="line">        caps: [mon] allow profile osd</span><br><span class="line">        caps: [osd] allow *</span><br></pre></td></tr></table></figure>
<p>#可以结合使用-o 文件名选项和 ceph auth list 将输出保存到某个文件。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth list -o 123.key</span><br></pre></td></tr></table></figure>

<p>#ceph auth add<br>此命令是添加用户的规范方法。它会创建用户、生成密钥，并添加所有指定的能力<br>添加认证 key：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth add client.tom mon <span class="string">&#x27;allow r&#x27;</span> osd <span class="string">&#x27;allow rwx pool=testpool2&#x27;</span></span><br><span class="line">added key <span class="keyword">for</span> client.tom</span><br></pre></td></tr></table></figure>

<p>##验证 key</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth get client.tom</span><br><span class="line">[client.tom]</span><br><span class="line">        key = AQD2vbJj8fIiDBAArtJBzQiuPy8nDWPSFVs0bw==</span><br><span class="line">        caps mon = <span class="string">&quot;allow r&quot;</span></span><br><span class="line">        caps osd = <span class="string">&quot;allow rwx pool=testpool2&quot;</span></span><br><span class="line">exported keyring <span class="keyword">for</span> client.tom</span><br></pre></td></tr></table></figure>
<p>##创建用户</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth get-or-create client.jack mon <span class="string">&#x27;allow r&#x27;</span> osd <span class="string">&#x27;allow rwx pool=testpool2&#x27;</span></span><br><span class="line">[client.jack]</span><br><span class="line">        key = AQC/vrJj5kenHhAAGeRJpY64feS4Dn6DD/R8VA==</span><br></pre></td></tr></table></figure>
<p>##再次创建用户</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth get-or-create client.jack mon <span class="string">&#x27;allow r&#x27;</span> osd <span class="string">&#x27;allow rwx pool=testpool2&#x27;</span></span><br><span class="line">[client.jack]</span><br><span class="line">        key = AQC/vrJj5kenHhAAGeRJpY64feS4Dn6DD/R8VA==</span><br></pre></td></tr></table></figure>
<p>#ceph auth get-or-create-key:<br>此命令是创建用户并返回用户密钥，对于只需要密钥的客户端(例如 libvrirt),此命令非常有用。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth get-or-create-key client.jack</span><br><span class="line">mon <span class="string">&#x27;allow r&#x27;</span> osd <span class="string">&#x27;allow rwx pool=mypool&#x27;</span></span><br><span class="line">AQAtr8dfi37XMhAADbHWEZ0shY1QZ5A8eBpeoQ==</span><br></pre></td></tr></table></figure>
<p>用户有 key 就显示没有就创建<br>#修 改 用 户 能 力</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth get client.jack</span><br><span class="line">[client.jack]</span><br><span class="line">        key = AQC/vrJj5kenHhAAGeRJpY64feS4Dn6DD/R8VA==</span><br><span class="line">        caps mon = <span class="string">&quot;allow r&quot;</span></span><br><span class="line">        caps osd = <span class="string">&quot;allow rwx pool=testpool2&quot;</span></span><br><span class="line">exported keyring <span class="keyword">for</span> client.jack</span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth caps client.jack mon <span class="string">&#x27;allow r&#x27;</span> osd <span class="string">&#x27;allow rw pool=testpool2&#x27;</span></span><br><span class="line">updated caps <span class="keyword">for</span> client.jack</span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth get client.jack</span><br><span class="line">[client.jack]</span><br><span class="line">        key = AQC/vrJj5kenHhAAGeRJpY64feS4Dn6DD/R8VA==</span><br><span class="line">        caps mon = <span class="string">&quot;allow r&quot;</span></span><br><span class="line">        caps osd = <span class="string">&quot;allow rw pool=testpool2&quot;</span></span><br><span class="line">exported keyring <span class="keyword">for</span> client.jack</span><br></pre></td></tr></table></figure>

<p>#删 除 用 户 </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth del client.tom</span><br><span class="line">updated</span><br></pre></td></tr></table></figure>
<p>#导出 keyring 至指定文件</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth get client.user1 -o</span><br><span class="line">ceph.client.user1.keyring</span><br><span class="line">exported keyring <span class="keyword">for</span> client.user1</span><br></pre></td></tr></table></figure>
<p>#验证指定用户的 keyring 文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ <span class="built_in">cat</span> ceph.client.user1.keyring</span><br><span class="line">[client.user1]</span><br><span class="line">key = AQAUUchfjpMqGRAARV6h0ofdDEneuaRnxuHjoQ==</span><br><span class="line">caps mon = <span class="string">&quot;allow r&quot;</span></span><br><span class="line">caps osd = <span class="string">&quot;allow * pool=mypool&quot;</span></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth del client.user1 <span class="comment">#演示误删除用户</span></span><br><span class="line">Updated</span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth get client.user1 <span class="comment">#确认用户被删除</span></span><br><span class="line">Error ENOENT: failed to find client.user1 <span class="keyword">in</span> keyring</span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth import -i</span><br><span class="line">ceph.client.user1.keyring <span class="comment">#导入用户</span></span><br><span class="line">imported keyring</span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth get client.user1 <span class="comment">#验证已恢复用户</span></span><br><span class="line">exported keyring <span class="keyword">for</span> client.user1</span><br></pre></td></tr></table></figure>
<p>#将多 用 户 导 出 至 秘 钥 环 ：<br>#创建 keyring 文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ceph-authtool --create-keyring ceph.client.user.keyring <span class="comment">#创建空的 keyring 文件</span></span><br><span class="line">creating ceph.client.user.keyring</span><br></pre></td></tr></table></figure>
<p>#把指定的 admin 用户的 keyring 文件内容导入到 user 用户的 keyring 文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="variable">$ceph</span>-authtool ./ceph.client.user.keyring</span><br><span class="line">--import-keyring ./ceph.client.admin.keyring</span><br><span class="line">importing contents of ./ceph.client.admin.keyring into ./ceph.client.user.keyring</span><br></pre></td></tr></table></figure>
<p>#验证 keyring 文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph-authtool -l ./ceph.client.user.keyring</span><br><span class="line">[client.admin]</span><br><span class="line">key = AQAGDKJfQk/dAxAA3Y+9xoE/p8in6QjoHeXmeg==</span><br><span class="line">caps mds = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">caps mgr = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">caps mon = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">caps osd = <span class="string">&quot;allow *&quot;</span></span><br></pre></td></tr></table></figure>
<p>#再导入一个其他用户的 keyring：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph-authtool ./ceph.client.user.keyring</span><br><span class="line">--import-keyring ./ceph.client.user1.keyring</span><br><span class="line">importing contents of ./ceph.client.user1.keyring into ./ceph.client.user.keyring</span><br></pre></td></tr></table></figure>
<p>#再次验证 keyring 文件是否包含多个用户的认证信息：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph-authtool -l ./ceph.client.user.keyring</span><br><span class="line">[client.admin]</span><br><span class="line">key = AQAGDKJfQk/dAxAA3Y+9xoE/p8in6QjoHeXmeg==</span><br><span class="line">caps mds = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">caps mgr = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">caps mon = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">caps osd = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">[client.user1]</span><br><span class="line">key = AQAUUchfjpMqGRAARV6h0ofdDEneuaRnxuHjoQ==</span><br><span class="line">caps mon = <span class="string">&quot;allow r&quot;</span></span><br><span class="line">caps osd = <span class="string">&quot;allow * pool=mypool&quot;</span></span><br></pre></td></tr></table></figure>
<p>三. 使用普通客户挂载块存储<br>#创建存储池：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$ceph</span> osd pool create rbd-data1 32 32</span><br><span class="line"></span><br><span class="line"><span class="comment">#存储池启用 rbd</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$ceph</span> osd pool application <span class="built_in">enable</span> rbd-data1 rbd</span><br><span class="line"><span class="comment">#初始化 rbd</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$rbd</span> pool init -p rbd-data1</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建两个镜像：</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$rbd</span> create data-img1 --size 3G --pool rbd-data1 --image-format 2 --image-feature layering</span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$rbd</span> create data-img2 --size 5G --pool rbd-data1 --image-format 2 --image-feature layering</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#列出镜像信息</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$rbd</span> <span class="built_in">ls</span> --pool rbd-data1</span><br><span class="line"><span class="comment">#以 json 格 式 显 示 镜 像 信 息</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$rbd</span> <span class="built_in">ls</span> --pool rbd-data1 -l --format json --pretty-format</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建普通账户</span></span><br><span class="line">ceph auth add client.shijie mon <span class="string">&#x27;allow r&#x27;</span> osd <span class="string">&#x27;allow rwx pool=rbd-data1&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#验证用户信息</span></span><br><span class="line">ceph auth get client.shijie</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建用 keyring 文件</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph-authtool --create-keyring ceph.client.shijie.keyring</span><br><span class="line"></span><br><span class="line"><span class="comment">#导出用户 keyring</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth get client.shijie -o ceph.client.shijie.keyring </span><br><span class="line"></span><br><span class="line"><span class="comment">#验证指定用户的 keyring 文件</span></span><br><span class="line"></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$cat</span> ceph.client.shijie.keyring </span><br><span class="line">  </span><br><span class="line"><span class="comment">#同 步 普 通 用 户 认 证 文 件</span></span><br><span class="line"></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ scp ceph.client.shijie.keyring root@172.31.6.109:/etc/ceph/</span><br><span class="line"></span><br><span class="line"><span class="comment">#管理端验证镜像状态</span></span><br><span class="line"></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$rbd</span> <span class="built_in">ls</span> -p rbd-data1 -l</span><br><span class="line"></span><br><span class="line"><span class="comment">#在client上操作</span></span><br><span class="line"><span class="comment">#映射 rbd</span></span><br><span class="line"></span><br><span class="line">root@ceph-node4:/<span class="comment"># rbd -p rbd-data1 map data-img1</span></span><br><span class="line">/dev/rbd0</span><br><span class="line">root@ceph-node4:/<span class="comment"># lsblk</span></span><br><span class="line">rbd0</span><br><span class="line">root@ceph-node4:/<span class="comment"># mkfs.xfs /dev/rbd0</span></span><br><span class="line">root@ceph-node4:/<span class="comment"># mount /dev/rbd0 /data</span></span><br><span class="line">root@ceph-node4:/<span class="comment"># docker run -it -d -p 3306:3306 -e MYSQL_ROOT_PASSWORD=&quot;12345678&quot; -v /data:/var/lib/mysql mysql:5.6.46</span></span><br><span class="line">48374db8541a7fa375c00611373051ef21690e89adfd4c156b3f6ffb0dbe95a2</span><br><span class="line">root@ceph-node4:/data<span class="comment"># ls</span></span><br><span class="line">ibdata1  ib_logfile0  ib_logfile1  mysql  performance_schema  <span class="built_in">test</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#在client上操作</span></span><br><span class="line"><span class="comment">#使用普通用户映射 rbd</span></span><br><span class="line"></span><br><span class="line">root@ceph-node4:/etc/ceph<span class="comment"># rbd --user shijie -p rbd-data1 map data-img2</span></span><br><span class="line">/dev/rbd1</span><br><span class="line"><span class="comment">#格式化</span></span><br><span class="line">root@ceph-node4:/etc/ceph<span class="comment">#mkfs.ext4 /dev/rbd0</span></span><br><span class="line">root@ceph-node4:/etc/ceph<span class="comment"># mkdir /data1</span></span><br><span class="line">root@ceph-node4:/etc/ceph<span class="comment"># mount /dev/rbd1 /data1</span></span><br><span class="line">root@ceph-node4:/etc/ceph<span class="comment"># cp /var/log/auth.log /data1</span></span><br><span class="line">root@ceph-node4:/etc/ceph<span class="comment"># cd /data1</span></span><br><span class="line">root@ceph-node4:/data1<span class="comment"># ls</span></span><br><span class="line">auth.log  lost+found</span><br><span class="line">四. 使用普通用户挂载 cephfs（可以通过 secret 或者 secretfile 的形式多主机同时挂载）</span><br><span class="line">Ceph FS 需要运行 Meta Data Services(MDS)服务，其守护进程为 ceph-mds，ceph-mds进程管理与 cephFS 上存储的文件相关的元数据，并协调对 ceph 存储集群的访问。</span><br><span class="line"></span><br><span class="line"><span class="comment">#部署MDS服务:</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ apt-cache madison ceph-mds</span><br><span class="line">  ceph-mds | 16.2.10-1bionic | https://mirrors.tuna.tsinghua.edu.cn/ceph/debian-pacific bionic/main amd64 Packages</span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ apt install ceph-mds=16.2.10-1bionic</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建CephFS meta data和data存储池</span></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph osd pool create cephfs-metadata 32 32</span><br><span class="line">pool <span class="string">&#x27;cephfs-metadata&#x27;</span> created <span class="comment">#保存 metadata 的 pool</span></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph osd pool create cephfs-data 64 64</span><br><span class="line">pool <span class="string">&#x27;cephfs-data&#x27;</span> created <span class="comment">#保存数据的 pool</span></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph fs new mycephfs cephfs-metadata</span><br><span class="line">cephfs-data</span><br><span class="line">new fs with metadata pool 7 and data pool 8</span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph fs <span class="built_in">ls</span></span><br><span class="line">name: mycephfs, metadata pool: cephfs-metadata, data pools: [cephfs-data ]</span><br><span class="line"><span class="comment">#查看指定 cephFS 状态</span></span><br><span class="line"></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph fs status mycephfs</span><br><span class="line">mycephfs - 0 clients</span><br><span class="line">RANK  STATE      MDS        ACTIVITY     DNS    INOS   DIRS   CAPS  </span><br><span class="line"> 0    active  ceph-mgr1  Reqs:    0 /s    10     13     12      0   </span><br><span class="line">      POOL         TYPE     USED  AVAIL  </span><br><span class="line">cephfs-metadata  metadata   146k  18.9T  </span><br><span class="line">  cephfs-data      data       0   18.9T  </span><br><span class="line">MDS version: ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)</span><br><span class="line"><span class="comment">#验证cephFS服务状态</span></span><br><span class="line"></span><br><span class="line">cephadmin@ceph-deploy:~$ ceph mds <span class="built_in">stat</span></span><br><span class="line">mycephfs:1 &#123;0=ceph-mgr1=up:active&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建客户端账户</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$ceph</span> auth add client.yanyan mon <span class="string">&#x27;allow r&#x27;</span> mds <span class="string">&#x27;allow rw&#x27;</span> osd <span class="string">&#x27;allow rwx pool=cephfs-data&#x27;</span></span><br><span class="line"><span class="comment">#验证账户</span></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth get client.yanyan</span><br><span class="line">exported keyring <span class="keyword">for</span> client.yanyan</span><br><span class="line">[client.yanyan]</span><br><span class="line">key = AQCxpdhfjQt1OxAAGe0mqTMveNu2ZMEem3tb0g==</span><br><span class="line">caps mds = <span class="string">&quot;allow rw&quot;</span></span><br><span class="line">caps mon = <span class="string">&quot;allow r&quot;</span></span><br><span class="line">caps osd = <span class="string">&quot;allow rwx pool=cephfs-data&quot;</span></span><br><span class="line"><span class="comment">#创建keyring 文件</span></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth get client.yanyan -o</span><br><span class="line">ceph.client.yanyan.keyring</span><br><span class="line">exported keyring <span class="keyword">for</span> client.yanyan</span><br><span class="line"><span class="comment">#创建 key 文件：</span></span><br><span class="line"></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth print-key client.yanyan &gt; yanyan.key</span><br><span class="line"><span class="comment">#验证用户的 keyring 文件</span></span><br><span class="line"></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ <span class="built_in">cat</span> ceph.client.yanyan.keyring</span><br><span class="line">[client.yanyan]</span><br><span class="line">key = AQCxpdhfjQt1OxAAGe0mqTMveNu2ZMEem3tb0g==</span><br><span class="line">caps mds = <span class="string">&quot;allow rw&quot;</span></span><br><span class="line">caps mon = <span class="string">&quot;allow r&quot;</span></span><br><span class="line">caps osd = <span class="string">&quot;allow rwx pool=cephfs-data&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#同步客户端认证文件 ：</span></span><br><span class="line"></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ scp ceph.conf ceph.client.yanyan.keyring</span><br><span class="line">yanyan.key root@172.31.6.109:/etc/ceph/</span><br><span class="line"></span><br><span class="line"><span class="comment">#客户端验证权限</span></span><br><span class="line"></span><br><span class="line">root@ceph-node4:~<span class="comment"># ceph --user yanyan -s</span></span><br><span class="line">  cluster:</span><br><span class="line">    <span class="built_in">id</span>:     7c088d6f-06b0-4584-b23f-c0f150af51d4</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph-mon1,ceph-mon2,ceph-mon3 (age 24m)</span><br><span class="line">    mgr: ceph-mgr1(active, since 65m)</span><br><span class="line">    mds: 1/1 daemons up</span><br><span class="line">    osd: 16 osds: 16 up (since 24m), 16 <span class="keyword">in</span> (since 13d)</span><br><span class="line">    rgw: 1 daemon active (1 hosts, 1 zones)</span><br><span class="line">  data:</span><br><span class="line">    volumes: 1/1 healthy</span><br><span class="line">    pools:   10 pools, 329 pgs</span><br><span class="line">    objects: 296 objects, 218 MiB</span><br><span class="line">    usage:   948 MiB used, 60 TiB / 60 TiB avail</span><br><span class="line">    pgs:     329 active+clean</span><br><span class="line"></span><br><span class="line"><span class="comment">#客户端通过 key 文件挂载:</span></span><br><span class="line"></span><br><span class="line"> root@ceph-node4:~<span class="comment">#mkdir /data</span></span><br><span class="line"> root@ceph-node4:/etc/ceph<span class="comment"># mount -t ceph 172.31.6.101:6789,172.31.6.102:6789,172.31.6.103:6789:/ /data -o name=yanyan,secretfile=/etc/ceph/yanyan.key</span></span><br><span class="line">root@ceph-node4:/etc/ceph<span class="comment"># df -h</span></span><br><span class="line">Filesystem                                               Size  Used Avail Use% Mounted on</span><br><span class="line">udev                                                     955M     0  955M   0% /dev</span><br><span class="line">172.31.6.101:6789,172.31.6.102:6789,172.31.6.103:6789:/   19T     0   19T   0% /data</span><br><span class="line"></span><br><span class="line"><span class="comment">#客户端通过key挂载</span></span><br><span class="line"></span><br><span class="line">root@ceph-node3:~<span class="comment"># mkdir /data</span></span><br><span class="line">root@ceph-node3:~<span class="comment"># mount -t ceph 172.31.6.101:6789,172.31.6.102:6789,172.31.6.103:6789:/ /data -o name=yanyan,secret=AQAfebVjaIPgABAAzkW4ChX2Qm2Sha/5twdxPA==</span></span><br><span class="line">root@ceph-node3:~<span class="comment"># df -h</span></span><br><span class="line">Filesystem                                               Size  Used Avail Use% Mounted on</span><br><span class="line">udev                                                     955M     0  955M   0% /dev</span><br><span class="line">172.31.6.101:6789,172.31.6.102:6789,172.31.6.103:6789:/   19T     0   19T   0% /data</span><br><span class="line">root@ceph-node3:~<span class="comment"># cp /var/log/auth.log /data</span></span><br><span class="line">root@ceph-node3:~<span class="comment"># cd /data</span></span><br><span class="line">root@ceph-node3:/data<span class="comment"># ls</span></span><br><span class="line">auth.log</span><br><span class="line">root@ceph-node3:/data<span class="comment"># vim auth.log </span></span><br><span class="line">root@ceph-node3:/data<span class="comment"># echo &quot;12345678&quot; &gt;&gt; auth.log </span></span><br><span class="line">root@ceph-node3:/data<span class="comment"># echo &quot;testlog&quot; &gt;&gt; auth.log</span></span><br><span class="line"><span class="comment">#在node4客户端上查看cephfs挂载点/data 目录下内容，已经同步</span></span><br><span class="line"></span><br><span class="line">root@ceph-node4:/<span class="comment"># tail -f /data/auth.log </span></span><br><span class="line">Jan  4 22:25:01 ceph-node3 CRON[4365]: pam_unix(cron:session): session closed <span class="keyword">for</span> user root</span><br><span class="line">12345678</span><br><span class="line">testlog</span><br><span class="line"><span class="comment">#客户端内核加载 ceph.ko 模块挂载 cephfs 文件系统</span></span><br><span class="line"></span><br><span class="line">root@ceph-node4:/<span class="comment"># lsmod|grep ceph</span></span><br><span class="line">ceph                  380928  1</span><br><span class="line">libceph               315392  1 ceph</span><br><span class="line">fscache                65536  1 ceph</span><br><span class="line">libcrc32c              16384  5 nf_conntrack,nf_nat,xfs,raid456,libceph</span><br><span class="line">root@ceph-node4:/<span class="comment"># modinfo ceph</span></span><br><span class="line">filename:       /lib/modules/4.15.0-130-generic/kernel/fs/ceph/ceph.ko</span><br><span class="line">license:        GPL</span><br><span class="line">description:    Ceph filesystem <span class="keyword">for</span> Linux</span><br><span class="line">author:         Patience Warnick &lt;patience@newdream.net&gt;</span><br><span class="line">author:         Yehuda Sadeh &lt;yehuda@hq.newdream.net&gt;</span><br><span class="line">author:         Sage Weil &lt;sage@newdream.net&gt;</span><br><span class="line"><span class="built_in">alias</span>:          fs-ceph</span><br><span class="line">srcversion:     CB79D9E4790452C6A392A1C</span><br><span class="line">depends:        libceph,fscache</span><br><span class="line">retpoline:      Y</span><br><span class="line">intree:         Y</span><br><span class="line">name:           ceph</span><br><span class="line">vermagic:       4.15.0-130-generic SMP mod_unload </span><br><span class="line">signat:         PKCS<span class="comment">#7</span></span><br><span class="line">signer:         </span><br><span class="line">sig_key:        </span><br><span class="line">sig_hashalgo:   md4</span><br></pre></td></tr></table></figure>

<p>五.实现 MDS 服务的多主一备高可用架构</p>
<p>#当前mds服务器状态</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~$ ceph mds <span class="built_in">stat</span></span><br><span class="line">mycephfs:1 &#123;0=ceph-mgr1=up:active&#125;</span><br><span class="line"><span class="comment">#添加MDS服务器</span></span><br><span class="line">将 ceph-mgr2 和 ceph-mon2 和 ceph-mon3 作为 mds 服务角色添加至 ceph 集群，最后实两主两备的 mds 高可用和高性能结构。</span><br><span class="line"><span class="comment">#mds 服务器安装 ceph-mds 服务</span></span><br><span class="line"></span><br><span class="line">[root@ceph-mgr2 ~]<span class="comment"># apt install ceph-mds -y</span></span><br><span class="line">[root@ceph-mon2 ~]<span class="comment"># apt install ceph-mds -y</span></span><br><span class="line">[root@ceph-mon3 ~]<span class="comment"># apt install ceph-mds -y</span></span><br><span class="line"><span class="comment">#添加 mds 服务器</span></span><br><span class="line"></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph-deploy mds create ceph-mgr2</span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph-deploy mds create ceph-mon2</span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph-deploy mds create ceph-mon3</span><br></pre></td></tr></table></figure>
<p>#验证 mds 服务器当前状态：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph mds <span class="built_in">stat</span></span><br><span class="line">mycephfs-1/1/1 up &#123;0=ceph-mgr1=up:active&#125;, 3 up:standby</span><br></pre></td></tr></table></figure>
<p>#验证 ceph集群当前状态<br>当前处于激活状态的 mds 服务器有一台，处于备份状态的 mds 服务器有三台。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph fs status</span><br><span class="line">mycephfs - 0 clients</span><br><span class="line">RANK  STATE      MDS        ACTIVITY     DNS    INOS   DIRS   CAPS  </span><br><span class="line"> 0    active  ceph-mgr1  Reqs:    0 /s    16     14     12      0   </span><br><span class="line">      POOL         TYPE     USED  AVAIL  </span><br><span class="line">cephfs-metadata  metadata   282k  18.9T  </span><br><span class="line">  cephfs-data      data     564k  18.9T  </span><br><span class="line">STANDBY MDS  </span><br></pre></td></tr></table></figure>
<p>#当前的文件系统状态:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph fs get mycephfs</span><br><span class="line">Filesystem <span class="string">&#x27;mycephfs&#x27;</span> (1)</span><br><span class="line">fs_name        mycephfs</span><br><span class="line">epoch        28</span><br><span class="line">flags        12</span><br><span class="line">created        2023-01-01T19:09:29.258956+0800</span><br><span class="line">modified        2023-01-05T14:58:00.369468+0800</span><br><span class="line">tableserver        0</span><br><span class="line">root        0</span><br><span class="line">session_timeout        60</span><br><span class="line">session_autoclose        300</span><br><span class="line">max_file_size        1099511627776</span><br><span class="line">required_client_features        &#123;&#125;</span><br><span class="line">last_failure        0</span><br><span class="line">last_failure_osd_epoch        406</span><br><span class="line">compat        compat=&#123;&#125;,rocompat=&#123;&#125;,incompat=&#123;1=base v0.20,2=client writeable ranges,3=default file layouts on <span class="built_in">dirs</span>,4=<span class="built_in">dir</span> inode <span class="keyword">in</span> separate object,5=mds uses versioned encoding,6=dirfrag is stored <span class="keyword">in</span> omap,7=mds uses inline data,8=no anchor table,9=file layout v2,10=snaprealm v2&#125;</span><br><span class="line">max_mds        1</span><br><span class="line"><span class="keyword">in</span>        0</span><br><span class="line">up        &#123;0=144106&#125;</span><br><span class="line">failed        </span><br><span class="line">damaged        </span><br><span class="line">stopped        </span><br><span class="line">data_pools        [8]</span><br><span class="line">metadata_pool        7</span><br><span class="line">inline_data        disabled</span><br><span class="line">balancer        </span><br><span class="line">standby_count_wanted        1</span><br><span class="line">[mds.ceph-mgr1&#123;0:144106&#125; state up:active <span class="built_in">seq</span> 27 addr [v2:172.31.6.104:6800/428364709,v1:172.31.6.104:6801/428364709] compat &#123;c=[1],r=[1],i=[7ff]&#125;]</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置处于激活状态mds的数量</span></span><br><span class="line">目前有四个 mds 服务器，但是有一个主三个备，可以优化一下部署架构，设置为为两主两备。</span><br><span class="line"></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph fs <span class="built_in">set</span> mycephfs max_mds 2</span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph fs status</span><br><span class="line">mycephfs - 0 clients</span><br><span class="line">RANK  STATE      MDS        ACTIVITY     DNS    INOS   DIRS   CAPS  </span><br><span class="line"> 0    active  ceph-mgr1  Reqs:    0 /s    16     14     12      0   </span><br><span class="line"> 1    active  ceph-mon2  Reqs:    0 /s    10     13     11      0   </span><br><span class="line">      POOL         TYPE     USED  AVAIL  </span><br><span class="line">cephfs-metadata  metadata   354k  18.9T  </span><br><span class="line">  cephfs-data      data     564k  18.9T  </span><br><span class="line">STANDBY MDS  </span><br><span class="line"> ceph-mgr2   </span><br><span class="line"> ceph-mon3   </span><br><span class="line">MDS version: ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)</span><br></pre></td></tr></table></figure>


<p>安装完成后ceph -s提示：“mon is allowing insecure global_id reclaim”。</p>
<p>解决方案：禁用不安全模式</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph config <span class="built_in">set</span> mon auth_allow_insecure_global_id_reclaim <span class="literal">false</span></span><br></pre></td></tr></table></figure>
<p>输入完成后需要等待5-10秒。</p>
<p>PG数量计算公式：</p>
<p>（总OSD数量*100）&#x2F; 副本数（复制分数，默认为3）&#x3D; PG数量</p>
<p>一般情况下结果取2的N次方，尽量先设置小点，后期可以增大。如果先设置的比较大的话后期减小风险较高，所以尽量取小的2的N次方结果。<br>建一个存储池，要想使用ceph的存储功能，必须先创建存储池</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd pool create rbd 128 128 </span><br></pre></td></tr></table></figure>

<p>初始化存储池</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd pool init -p rbd</span><br></pre></td></tr></table></figure>
<p>1.设置存储池副本数</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@controller-1 ~]<span class="comment"># ceph osd pool get rbd size</span></span><br><span class="line">size: 2</span><br><span class="line">[root@controller-1 ~]<span class="comment"># ceph osd pool set rbd size 1</span></span><br><span class="line"><span class="built_in">set</span> pool 1 size to 1</span><br></pre></td></tr></table></figure>
<p>升级client的虚拟机内核到5版本</p>
<p>修改client下文件权限</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">chmod</span> +r /etc/ceph/ceph.client.admin.keyring</span><br><span class="line">client节点创建设备镜像，单位是M</span><br><span class="line">rbd create --size 4096 --pool rbd img</span><br><span class="line">client节点映射镜像到主机：</span><br><span class="line">rbd map img --name client.admin</span><br><span class="line">client节点格式化块设备</span><br><span class="line">mkfs.ext4 -m 0 /dev/rbd/rbd/foo </span><br><span class="line">client节点挂载mount块设备</span><br><span class="line"><span class="built_in">mkdir</span> /mnt/ceph-block-device</span><br><span class="line">mount /dev/rbd/rbd/foo /mnt/ceph-block-device -o discard</span><br><span class="line">创建文件系统时报不支持EC数据池问题</span><br><span class="line">当拥有一个ceph_metadata元数据副本类型池和一个ceph_data数据EC类型池时，建立文件系统：</span><br><span class="line">ceph fs new cephfs cephfs_metadata cephfs_data</span><br><span class="line">N版可能会报：</span><br><span class="line">Error EINVAL: pool <span class="string">&#x27;cephfs_data&#x27;</span> (<span class="built_in">id</span> <span class="string">&#x27;11&#x27;</span>) is an erasure-coded pool. Use of an EC pool <span class="keyword">for</span> the default data pool is discouraged; see the online CephFS documentation <span class="keyword">for</span> more information. Use --force to override.</span><br><span class="line">加上—force后：</span><br><span class="line">ceph fs new cephfs cephfs_metadata cephfs_data --force</span><br><span class="line"></span><br><span class="line">也可能会报：</span><br><span class="line"></span><br><span class="line">Error EINVAL: pool <span class="string">&#x27;cephfs_data&#x27;</span> (<span class="built_in">id</span> <span class="string">&#x27;11&#x27;</span>) is an erasure-coded pool, with no overwrite support</span><br><span class="line"></span><br><span class="line">这是需要手动设置ceph_data池 allow_ec_overwrites=<span class="literal">true</span></span><br><span class="line">ceph osd pool <span class="built_in">set</span> cephfs_data allow_ec_overwrites <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">再执行</span><br><span class="line">ceph fs new cephfs cephfs_metadata cephfs_data –forc</span><br><span class="line">就可以创建文件系统成功。</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>群晖变身airdrop</title>
    <url>/2023/08/16/%E7%BE%A4%E6%99%96%E5%8F%98%E8%BA%ABairdrop/</url>
    <content><![CDATA[<p>把群晖 NAS 变成「时间返回舱」，轻松搞定 Time Machine 无线备份</p>
<p>2018年11月18日</p>
<p>相比 Windows 自带的系统还原功能，macOS 有着更加完善的备份还原机制：通过内置的 Time Machine，我们可以方便地进行整机备份，在关键时候成为系统以及重要资料一颗「后悔药」。</p>
<p>当然，如果需要使用 Time Machine 功能备份，苹果官方提供了两种方式，无论哪一种方式，你都需要外置的存储来解决：一种是准备一个较合适容量的移动硬盘，如果你正好有一个闲置不用的移动硬盘，拿出来做专门的 Mac 的 Time Machine 备份倒是不错，只不过就会比较麻烦——每一次备份都需要插入移动硬盘。</p>
<p>还有一种是使用网络存储器，比如说苹果官方的高存储容量 AirPort Time Capsule（时间返回舱），设置一次就可以轻松完成自动备份工作，但其售价高昂不说，现在也已经全面在苹果在线商店下架，显然无论是从经济角度还是便利性角度而言，以上两个方案似乎都并不合适。</p>
<p>其实，如果你恰好有一台群晖 NAS ，就可以将其轻松打造成一个稳定可靠的「时间返回舱」，而该功能从入门级的 J 系列就已经默认搭载，换句话说，你几乎没有什么额外成本就可以拥有和 AirPort Time Capsule 近乎一样的功能，那么如何使用 Time Machine 将 Mac 文件备份至群晖呢？</p>
<h2 id="如何让群晖-NAS-支持-Time-Machine-功能？"><a href="#如何让群晖-NAS-支持-Time-Machine-功能？" class="headerlink" title="如何让群晖 NAS 支持 Time Machine 功能？"></a>如何让群晖 NAS 支持 Time Machine 功能？</h2><p>要想让群晖支持 Time Machine ，还需要对群晖进行一番设置，首先我们使用管理员账号登录群晖的 DSM 系统1 ，首先来创建一个共享文件夹，专门用来存放 Time Machine 的备份数据：</p>
<p>登录到 DSM 之后，点击「控制面板 - 共享文件夹」，点击「创建」来添加共享文件夹。</p>
<p><img src="https://cdn.sspai.com/2018/11/18/ac135adbe134d0703a7acac6bbc4a04c.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1" alt="img"></p>
<p>然后输入共享文件夹的名字2 ，然后选择一个所在位置（单盘 NAS 选择默认）：</p>
<p><img src="https://cdn.sspai.com/2018/11/18/7992d79626fd2f571cae842bde49b2c1.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1" alt="img"></p>
<p>后面一步就是是否需要加密，因为我是本地家庭环境使用，为了减少一些不必要的麻烦直接跳过，如果你是在公司中来操作则建议设置加密，然后点击下一步。</p>
<p><img src="https://cdn.sspai.com/2018/11/18/940f3d290ab4cb4f91a7e6ee58d08d49.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1" alt="img"></p>
<p>后面是为共享文件夹配置高级功能，这里我就直接跳过，然后确认设置下面选择应用。</p>
<p><img src="https://cdn.sspai.com/2018/11/18/57993635211b319945fe96921a7ef000.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1" alt="img"></p>
<p>最后会弹出读取权限的设置，这里面默认应该只有管理员（当前你登录的账户）读写权限，这里默认不修改，点击确定生成分享文件夹。</p>
<p><img src="https://cdn.sspai.com/2018/11/18/a715c3349fbeca61d476e5050bb05bcf.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1" alt="img"></p>
<p>现在你应该可以看到名为 Time Machine Folder 新共享文件夹了，下面进入到下一步，为这个文件夹配可访问的账号。</p>
<h2 id="在-NAS-中为-Time-Machine-创建管理用户"><a href="#在-NAS-中为-Time-Machine-创建管理用户" class="headerlink" title="在 NAS 中为 Time Machine 创建管理用户"></a>在 NAS 中为 Time Machine 创建管理用户</h2><p>Time Machine Folder 这个文件夹显然应该只需要有一个专门操作的账户来操作，使用管理员账号操作并不安全，因此在这里我们需要给 Time Machine 创建管理用户并为其配备的对应配额限制。</p>
<p><img src="https://cdn.sspai.com/2018/11/18/86234695bbbb0758f2aa1d1567eca86b.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1" alt="img"></p>
<p>首先，还是通过群晖的管理员账户登录 DSM ，在「控制面板 - 用户」中，点击「创建用户」，输入用户名3 并设置一个复杂密码，然后点下一步，在群组中保持默认再点下一步。</p>
<p><img src="https://cdn.sspai.com/2018/11/18/9b3e81fc108b528526b47257a8c8e7a5.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1" alt="img"></p>
<p>在文件夹的权限分配这个设置页中，找到之前设置的共享文件夹「Time Machine Folder」，<strong>勾选中「读写」权限（一定要做），</strong>然后点击下一步，在用户配合设置页面中，选择存储配额。</p>
<p>这一步需要注意的是，为了避免 Time Machine 备份占用你全部的存储空间，建议设置在一个额定的空间范围内，比如我的 MacBook Pro 是 128GB 的存储，而我的群晖 NAS 的存储「空间 1」剩余 2.0 TB ，因此从使用权衡来看，选择 200GB 完全是绰绰有余，制定好之后选择下一步。</p>
<p><img src="https://cdn.sspai.com/2018/11/18/85d0d591291cf483391f7ad169493d12.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1" alt="img"></p>
<p>后续两个设置中都默认不选，直接点下一步，最后点击到应用后完成用户「Time Machine User」的新建，最后你在「控制面板 - 用户」中应该可以看到这个新建的用户了。</p>
<p><img src="https://cdn.sspai.com/2018/11/18/cf650e3ab4e93f9bb7ec52c84e61124b.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1" alt="img"></p>
<p>p.s.  如果你是在企业生产环境中，可以通过生成配对的「共享文件夹」+「对应的用户」来创建多个 Time Machine 备份空间，这样可以为多台 Mac 设备开启单独的备份，管理也更方便。</p>
<h2 id="在-NAS-中开启相关文件服务"><a href="#在-NAS-中开启相关文件服务" class="headerlink" title="在 NAS 中开启相关文件服务"></a>在 NAS 中开启相关文件服务</h2><p>前面共享文件夹和相关的访问用户已经设置完毕，那么接下来就是对相关的文件夹进行一系列的设置。首先在群晖的「控制面板 - 文件服务」设置中，找到 <strong>SMB&#x2F;AFP&#x2F;NFS</strong> 选项卡，然后点击勾选 启用 SMB 服务以及 AFP 服务。注意下方生成的访问地址，例如我这里生成的 SMB 访问地址：<code>smb://DS215J</code>。</p>
<p><img src="https://cdn.sspai.com/2018/11/18/9a0808946eb2a9d0c1e4cebe4919a8bb.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1" alt="img"></p>
<p>然后切换到高级设置中，在 Bonjour 中勾选「启用 Bonjour 服务发现」，以及下方的「启用通过 SMB 进行 Bonjour Time Machine 播送」和「启用通过 AFP 进行 Bonjour Time Machine 播送」，然后点击下方的设置 Time machine 文件夹，选择之前我们建立的 「Time Machine Folder」共享文件夹点击确定即可。</p>
<p><img src="https://cdn.sspai.com/2018/11/18/dbf8339cefcd9273213e57735e8c8d4d.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1" alt="img"></p>
<p>p.s. 如果你的环境中，Mac 设备的系统版本在 10.12 以上，那么只需要启用 SMB 相关服务即可，可以不用勾选 AFP 服务。</p>
<p>至此，在群晖上的相关设置就全部完成了，换言之，这时候的群晖已经脱胎换骨成为一台「时间返回舱」了。</p>
<h2 id="让-Mac-的-Time-Machine-备份至-NAS"><a href="#让-Mac-的-Time-Machine-备份至-NAS" class="headerlink" title="让 Mac 的 Time Machine 备份至 NAS"></a>让 Mac 的 Time Machine 备份至 NAS</h2><p>接下来的步骤就是在 Mac 上设置时间机器，然后将备份目的地改成刚才设置好的群晖，首先在我们需要可以让 Mac 访问群晖中的备份文件夹。</p>
<p><img src="https://cdn.sspai.com/2018/11/18/c67463c703df1874de7c6318052b2430.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1" alt="img"></p>
<p>打开访达，在菜单中点击「前往 - 连接服务器」，然后输入 NAS 的 smb 地址，例如我这里的 <code>smb://DS215J</code>，然后点击链接，在输入之前设置的账户名和密码之后，在弹出的文件夹中，选择时间机器的备份文件夹，之后点击好完成装载。</p>
<p><img src="https://cdn.sspai.com/2018/11/18/1f1c1dea15242ee5d673acbdbc7e7429.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1" alt="img"></p>
<p>接下来从 Dock 栏中打开「系统偏好设置」，在「时间机器」中选择「备份磁盘」，然后选择之前链接的群晖对应的文件夹，然后点击使用磁盘。</p>
<p>这时候还有可能会要求输入之前在群晖中创建的账户和密码，然后点击链接，然后时间机器会自动绑定群晖设置到了备份目的磁盘，然后到这里你的时间机器就算是真正意义上的设置好了，如果不出意外的话就会自动开始进行备份工作。</p>
<p>通过一连串的操作之后，你的群晖可以轻松化身为一台「廉价版 AirPort Time Capsule」，为你的 Mac 设备保驾护航，而无论是性价比上还是便利性上都可以称作是 Mac 时间机器的最佳解决方案，如果你恰好有一台 Mac 和群晖设备，不妨试试这个方案来打造一个最廉价的时间机器备份方案。</p>
]]></content>
  </entry>
  <entry>
    <title>RAID技术简介</title>
    <url>/2023/08/16/RAID%E6%8A%80%E6%9C%AF%E7%AE%80%E4%BB%8B/</url>
    <content><![CDATA[<p>作者：王奥<br>链接：<a href="https://www.zhihu.com/question/20131784/answer/90235520">https://www.zhihu.com/question/20131784/answer/90235520</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
<p>title: RAID磁盘阵列配置和调优小结<br>date: 2016-03-10 15:52:17<br>categories: 学习 | Study<br>description: RAID的本质是平衡可用性与成本<br>-–</p>
<p><img src="https://pica.zhimg.com/50/3d84e446492f99c8082089313c3944ac_720w.jpg?source=1940ef5c" alt="img"><img src="https://pica.zhimg.com/80/3d84e446492f99c8082089313c3944ac_1440w.jpg?source=1940ef5c" alt="img"></p>
<p><strong>## 前言</strong></p>
<p>RAID解释我偷个小懒引用WikipediA，独立硬盘冗余阵列（RAID, Redundant Array of Independent Disks），旧称廉价磁盘冗余阵列（Redundant Array of Inexpensive Disks），简称磁盘阵列。其基本思想就是把多个相对便宜的硬盘组合起来，成为一个硬盘阵列组，使性能达到甚至超过一个价格昂贵、容量巨大的硬盘。根据选择的版本不同，RAID比单颗硬盘有以下一个或多个方面的好处：增强数据集成度，增强容错功能，增加处理量或容量。另外，磁盘阵列对于电脑来说，看起来就像一个单独的硬盘或逻辑存储单元。写这篇文章当然不是单纯的介绍概念和使用方法，更重要的是如何针对不同的业务场景做合理的RAID配置和参数优化，对于SSD固态硬盘的加入我引入小米运维团队的实验数据，同时我也相信分布式存储会逐步走向成熟，以OpenStack，VSAN，Nutanix为代表头顶软件定义和超融合概念的技术也已经开始了暗战。</p>
<p>&gt; RAID的本质是平衡可用性与成本  </p>
<p>-–</p>
<p><strong>## 更新历史</strong></p>
<p>2016年03月11日 - 初稿</p>
<p>阅读原文 - <a href="https://link.zhihu.com/?target=http://wsgzao.github.io/post/raid/">RAID磁盘阵列配置和调优小结</a></p>
<p><strong>扩展阅读</strong></p>
<p>RAID - <a href="https://link.zhihu.com/?target=https://zh.wikipedia.org/wiki/RAID">https://zh.wikipedia.org/wiki/RAID</a><br>RAID技术发展综述 - <a href="https://link.zhihu.com/?target=http://blog.csdn.net/liuaigui/article/details/4581970">RAID技术发展综述</a><br>SSD阵列卡方案优化：考虑使用RAID 50替代RAID 10 - <a href="https://link.zhihu.com/?target=http://noops.me/?p=1805">SSD阵列卡方案优化：考虑使用RAID 50替代RAID 10</a></p>
<p>-–</p>
<p><strong>## RAID基础知识</strong></p>
<p>&gt;感谢@刘爱贵，详细知识点可参考扩展阅读</p>
<p>### 基本原理</p>
<p>RAID （ Redundant Array of Independent Disks ）即独立磁盘冗余阵列，通常简称为磁盘阵列。简单地说， RAID 是由多个独立的高性能磁盘驱动器组成的磁盘子系统，从而提供比单个磁盘更高的存储性能和数据冗余的技术。 RAID 是一类多磁盘管理技术，其向主机环境提供了成本适中、数据可靠性高的高性能存储。 SNIA 对 RAID 的定义是 ：一种磁盘阵列，部分物理存储空间用来记录保存在剩余空间上的用户数据的冗余信息。当其中某一个磁盘或访问路径发生故障时，冗余信息可用来重建用户数据。磁盘条带化虽然与 RAID 定义不符，通常还是称为 RAID （即 RAID0 ）。</p>
<p>RAID 的初衷是为大型服务器提供高端的存储功能和冗余的数据安全。在整个系统中， RAID 被看作是由两个或更多磁盘组成的存储空间，通过并发地在多个磁盘上读写数据来提高存储系统的 I&#x2F;O 性能。大多数 RAID 等级具有完备的数据校验、纠正措施，从而提高系统的容错性，甚至镜像方式，大大增强系统的可靠性， Redundant 也由此而来。</p>
<p>这里要提一下 JBOD （ Just a Bunch of Disks ）。最初 JBOD 用来表示一个没有控制软件提供协调控制的磁盘集合，这是 RAID 区别与 JBOD 的主要因素。目前 JBOD 常指磁盘柜，而不论其是否提供 RAID 功能。</p>
<p>RAID 的两个关键目标是提高数据可靠性和 I&#x2F;O 性能。磁盘阵列中，数据分散在多个磁盘中，然而对于计算机系统来说，就像一个单独的磁盘。通过把相同数据同时写入到多块磁盘（典型地如镜像），或者将计算的校验数据写入阵列中来获得冗余能力，当单块磁盘出现故障时可以保证不会导致数据丢失。有些 RAID 等级允许更多地 磁盘同时发生故障，比如 RAID6 ，可以是两块磁盘同时损坏。在这样的冗余机制下，可以用新磁盘替换故障磁盘， RAID 会自动根据剩余磁盘中的数据和校验数据重建丢失的数据，保证数据一致性和完整性。数据分散保存在 RAID 中的多个不同磁盘上，并发数据读写要大大优于单个磁盘，因此可以获得更高的聚合 I&#x2F;O 带宽。当然，磁盘阵列会减少全体磁盘的总可用存储空间，牺牲空间换取更高的可靠性和性能。比如， RAID1 存储空间利用率仅有 50% ， RAID5 会损失其中一个磁盘的存储容量，空间利用率为 (n-1)&#x2F;n 。</p>
<p>磁盘阵列可以在部分磁盘（单块或多块，根据实现而论）损坏的情况下，仍能保证系统不中断地连续运行。在重建故障磁盘数据至新磁盘的过程中，系统可以继续正常运行，但是性能方面会有一定程度上的降低。一些磁盘阵列在添加或删除磁盘时必须停机，而有些则支持热交换 （ Hot Swapping ），允许不停机下替换磁盘驱动器。这种高端磁盘阵列主要用于要求高可能性的应用系统，系统不能停机或尽可能少的停机时间。一般来说， RAID 不可作为数据备份的替代方案，它对非磁盘故障等造成的数据丢失无能为力，比如病毒、人为破坏、意外删除等情形。此时的数据丢失是相对操作系统、文件系统、卷管理器或者应用系统来说的，对于 RAID 系统来身，数据都是完好的，没有发生丢失。所以，数据备份、灾 备等数据保护措施是非常必要的，与 RAID 相辅相成，保护数据在不同层次的安全性，防止发生数据丢失。</p>
<p>RAID 中主要有三个关键概念和技术：镜像（ Mirroring ）、数据条带（ Data Stripping ）和数据校验（ Data parity ）。镜像，将数据复制到多个磁盘，一方面可以提高可靠性，另一方面可并发从两个或多个副本读取数据来提高读性能。显而易见，镜像的写性能要稍低， 确保数据正确地写到多个磁盘需要更多的时间消耗。数据条带，将数据分片保存在多个不同的磁盘，多个数据分片共同组成一个完整数据副本，这与镜像的多个副本是不同的，它通常用于性能考虑。数据条带具有更高的并发粒度，当访问数据时，可以同时对位于不同磁盘上数据进行读写操作， 从而获得非常可观的 I&#x2F;O 性能提升 。数据校验，利用冗余数据进行数据错误检测和修复，冗余数据通常采用海明码、异或操作等算法来计算获得。利用校验功能，可以很大程度上提高磁盘阵列的可靠性、鲁棒性和容错能力。不过，数据校验需要从多处读取数据并进行计算和对比，会影响系统性能。 不同等级的 RAID 采用一个或多个以上的三种技术，来获得不同的数据可靠性、可用性和 I&#x2F;O 性能。至于设计何种 RAID （甚至新的等级或类型）或采用何种模式的 RAID ，需要在深入理解系统需求的前提下进行合理选择，综合评估可靠性、性能和成本来进行折中的选择。</p>
<p>RAID 思想从提出后就广泛被业界所接纳，存储工业界投入了大量的时间和财力来研究和开发相关产品。而且，随着处理器、内存、计算机接口等技术的不断发展， RAID 不断地发展和革新，在计算机存储领域得到了广泛的应用，从高端系统逐渐延伸到普通的中低端系统。 RAID 技术如此流行，源于其具有显著的特征和优势，基本可以满足大部分的数据存储需求。总体说来， RAID 主要优势有如下几点：<br>(1) 大容量<br>　　这是 RAID 的一个显然优势，它扩大了磁盘的容量，由多个磁盘组成的 RAID 系统具有海量的存储空间。现在单个磁盘的容量就可以到 1TB 以上，这样 RAID 的存储容量就可以达到 PB 级，大多数的存储需求都可以满足。一般来说， RAID 可用容量要小于所有成员磁盘的总容量。不同等级的 RAID 算法需要一定的冗余开销，具体容量开销与采用算法相关。如果已知 RAID 算法和容量，可以计算出 RAID 的可用容量。通常， RAID 容量利用率在 50% ~ 90% 之间。<br>(2) 高性能<br>　　 RAID 的高性能受益于数据条带化技术。单个磁盘的 I&#x2F;O 性能受到接口、带宽等计算机技术的限制，性能往往很有 限，容易成为系统性能的瓶颈。通过数据条带化， RAID 将数据 I&#x2F;O 分散到各个成员磁盘上，从而获得比单个磁盘成倍增长的聚合 I&#x2F;O 性能。<br>(3) 可靠性<br>　　可用性和可靠性是 RAID 的另一个重要特征。从理论上讲，由多个磁盘组成的 RAID 系统在可靠性方面应该比单个磁盘要差。这里有个隐含假定：单个磁盘故障将导致整个 RAID 不可用。 RAID 采用镜像和数据校验等数据冗余技术，打破了这个假定。 镜像是最为原始的冗余技术，把某组磁盘驱动器上的数据完全复制到另一组磁盘驱动器上，保证总有数据副本可用。 比起镜像 50% 的冗余开销 ，数据校验要小很多，它利用校验冗余信息对数据进行校验和纠错。 RAID 冗余技术大幅提升数据可用性和可靠性，保证了若干磁盘出错时，不 会导致数据的丢失，不影响系统的连续运行。<br>(4) 可管理性<br>　　实际上， RAID 是一种虚拟化技术，它对多个物理磁盘驱动器虚拟成一个大容量的逻辑驱动器。对于外部主机系统来说， RAID 是一个单一的、快速可靠的大容量磁盘驱动器。这样，用户就可以在这个虚拟驱动器上来组织和存储应用系统数据。 从用户应用角度看，可使存储系统简单易用，管理也很便利。 由于 RAID 内部完成了大量的存储管理工作，管理员只需要管理单个虚拟驱动器，可以节省大量的管理工作。 RAID 可以动态增减磁盘驱动器，可自动进行数据校验和数据重建，这些都可以 大大简化管理工作。</p>
<p><strong>### 关键技术</strong></p>
<p>&gt; 镜像</p>
<p>镜像是一种冗余技术，为磁盘提供保护功能，防止磁盘发生故障而造成数据丢失。对于 RAID 而言，采用镜像技术 典型地 将会同时在阵列中产生两个完全相同的数据副本，分布在两个不同的磁盘驱动器组上。镜像提供了完全的数据冗余能力，当一个数据副本失效不可用时，外部系统仍可正常访问另一副本，不会对应用系统运行和性能产生影响。而且，镜像不需要额外的计算和校验，故障修复非常快，直接复制即可。镜像技术可以从多个副本进行并发读取数据，提供更高的读 I&#x2F;O 性能，但不能并行写数据，写多个副本会会导致一定的 I&#x2F;O 性能降低。</p>
<p>镜像技术提供了非常高的数据安全性，其代价也是非常昂贵的，需要　　　至少双倍的存储空间。高成本限制了镜像的广泛应用，主要应用于至关重要的数据保护，这种场合下数据丢失会造成巨大的损失。另外，镜像通过 “ 拆分 ” 能获得特定时间点的上数据快照，从而可以实现一种备份窗口几乎为零的数据备份技术。</p>
<p>&gt; 数据条带</p>
<p>磁盘存储的性能瓶颈在于磁头寻道定位，它是一种慢速机械运动，无法与高速的 CPU 匹配。再者，单个磁盘驱动器性能存在物理极限， I&#x2F;O 性能非常有限。 RAID 由多块磁盘组成，数据条带技术将数据以块的方式分布存储在多个磁盘中，从而可以对数据进行并发处理。这样写入和读取数据就可以在多个磁盘上同时进行，并发产生非常高的聚合 I&#x2F;O ，有效提高了整体 I&#x2F;O 性能，而且具有良好的线性扩展性。这对大容量数据尤其显著，如果不分块，数据只能按顺序存储在磁盘阵列的磁盘上，需要时再按顺序读取。而通过条带技术，可获得数倍与顺序访问的性能提升。</p>
<p>数据条带技术的分块大小选择非常关键。条带粒度可以是一个字节至几 KB 大小，分块越小，并行处理能力就越强，数据存取速度就越高，但同时就会增加块存取的随机性和块寻址时间。实际应用中，要根据数据特征和需求来选择合适的分块大小，在数据存取随机性和并发处理能力之间进行平衡，以争取尽可能高的整体性能。</p>
<p>数据条带是基于提高 I&#x2F;O 性能而提出的，也就是说它只关注性能， 而对数据可靠性、可用性没有任何改善。实际上，其中任何一个数据条带损坏都会导致整个数据不可用，采用数据条带技术反而增加了数据发生丢失的概念率。</p>
<p>&gt; 数据校验</p>
<p>镜像具有高安全性、高读性能，但冗余开销太昂贵。数据条带通过并发性来大幅提高性能，然而对数据安全性、可靠性未作考虑。数据校验是一种冗余技术，它用校验数据来提供数据的安全，可以检测数据错误，并在能力允许的前提下进行数据重构。相对镜像，数据校验大幅缩减了冗余开销，用较小的代价换取了极佳的数据完整性和可靠性。数据条带技术提供高性能，数据校验提供数据安全性， RAID 不同等级往往同时结合使用这两种技术。</p>
<p>采用数据校验时， RAID 要在写入数据同时进行校验计算，并将得到的校验数据存储在 RAID 成员磁盘中。校验数据可以集中保存在某个磁盘或分散存储在多个不同磁盘中，甚至校验数据也可以分块，不同 RAID 等级实现各不相同。当其中一部分数据出错时，就可以对剩余数据和校验数据进行反校验计算重建丢失的数据。校验技术相对于镜像技术的优势在于节省大量开销，但由于每次数据读写都要进行大量的校验运算，对计算机的运算速度要求很高，必须使用硬件 RAID 控制器。在数据重建恢复方面，检验技术比镜像技术复杂得多且慢得多。</p>
<p>海明校验码和 异或校验是两种最为常用的 数据校验算法。海明校验码是由理查德 · 海明提出的，不仅能检测错误，还能给出错误位置并自动纠正。海明校验的基本思想是：将有效信息按照某种规律分成若干组，对每一个组作奇偶测试并安排一个校验位，从而能提供多位检错信息，以定位错误点并纠正。可见海明校验实质上是一种多重奇偶校验。异或校验通过异或逻辑运算产生，将一个有效信息与一个给定的初始值进行异或运算，会得到校验信息。如果有效信息出现错误，通过校验信息与初始值的异或运算能还原正确的有效信息。</p>
<p><strong>## 常见RAID类型</strong></p>
<p>&gt; 常见5种RAID类型对比，n位磁盘数量，详细介绍可参考扩展阅读</p>
<table>
<thead>
<tr>
<th>RAID 等级</th>
<th>RAID0</th>
<th>RAID1</th>
<th>RAID5</th>
<th>RAID6</th>
<th>RAID10</th>
</tr>
</thead>
<tbody><tr>
<td>别名</td>
<td>条带</td>
<td>镜像</td>
<td>分布奇偶校验条带</td>
<td>双重奇偶校验条带</td>
<td>镜像加条带</td>
</tr>
<tr>
<td>容错性</td>
<td>无</td>
<td>有</td>
<td>有</td>
<td>有</td>
<td>有</td>
</tr>
<tr>
<td>冗余类型</td>
<td>无</td>
<td>有</td>
<td>有</td>
<td>有</td>
<td>有</td>
</tr>
<tr>
<td>热备盘</td>
<td>无</td>
<td>有</td>
<td>有</td>
<td>有</td>
<td>有</td>
</tr>
<tr>
<td>读性能</td>
<td>高</td>
<td>低</td>
<td>高</td>
<td>高</td>
<td>高</td>
</tr>
<tr>
<td>随机写性能</td>
<td>高</td>
<td>低</td>
<td>一般</td>
<td>低</td>
<td>一般</td>
</tr>
<tr>
<td>连续写性能</td>
<td>高</td>
<td>低</td>
<td>低</td>
<td>低</td>
<td>一般</td>
</tr>
<tr>
<td>需要磁盘数</td>
<td>n≥1</td>
<td>2n (n≥1)</td>
<td>n≥3</td>
<td>n≥4</td>
<td>2n(n≥2)≥4</td>
</tr>
<tr>
<td>可用容量</td>
<td>全部</td>
<td>50%</td>
<td>(n-1)&#x2F;n</td>
<td>(n-2)&#x2F;n</td>
<td>50%</td>
</tr>
</tbody></table>
<p><strong>## RAID 等级</strong></p>
<p><strong>### 标准 RAID 等级</strong></p>
<p>SNIA 、 Berkeley 等组织机构把 RAID0 、 RAID1 、 RAID2 、 RAID3 、 RAID4 、 RAID5 、 RAID6 七个等级定为标准的 RAID 等级，这也被业界和学术界所公认。标准等级是最基本的 RAID 配置集合，单独或综合利用数据条带、镜像和数据校验技术。标准 RAID 可以组合，即 RAID 组合等级，满足 对性能、安全性、可靠性要求更高的存储应用需求。</p>
<p><strong>### JBOD</strong></p>
<p>JBOD （ Just a Bunch Of Disks ）不是标准的 RAID 等级，它通常用来表示一个没有控制软件提供协调控制的磁盘集合。 JBOD 将多个物理磁盘串联起来，提供一个巨大的逻辑磁盘。 JBOD 的数据存放机制是由第一块磁盘开始按顺序往后存储，当前磁盘存储空间用完后，再依次往后面的磁盘存储数据。 JBOD 存储性能完全等同于单块磁盘，而且也不提供数据安全保护。它只是简单提供一种扩展存储空间的机制， JBOD 可用存储容量等于所有成员磁盘的存储空间之和。目前 JBOD 常指磁盘柜，而不论其是否提供 RAID 功能。</p>
<p><img src="https://pic3.zhimg.com/50/4cfb57a1c97a239af039a04697baab27_720w.jpg?source=1940ef5c" alt="img"><img src="https://pic3.zhimg.com/80/4cfb57a1c97a239af039a04697baab27_1440w.jpg?source=1940ef5c" alt="img"></p>
<p><strong>### RAID0</strong></p>
<p>RAID0 是一种简单的、无数据校验的数据条带化技术。实际上不是一种真正的 RAID ，因为它并不提供任何形式的冗余策略。 RAID0 将所在磁盘条带化后组成大容量的存储空间，将数据分散存储在所有磁盘中，以独立访问方式实现多块磁盘的并读访问。由于可以并发执行 I&#x2F;O 操作，总线带宽得到充分利用。再加上不需要进行数据校验， RAID0 的性能在所有 RAID 等级中是最高的。理论上讲，一个由 n 块磁盘组成的 RAID0 ，它的读写性能是单个磁盘性能的 n 倍，但由于总线带宽等多种因素的限制，实际的性能提升低于理论值。</p>
<p>RAID0 具有低成本、高读写性能、 100% 的高存储空间利用率等优点，但是它不提供数据冗余保护，一旦数据损坏，将无法恢复。 因此， RAID0 一般适用于对性能要求严格但对数据安全性和可靠性不高的应用，如视频、音频存储、临时数据缓存空间等。</p>
<p><img src="https://pic3.zhimg.com/50/0ab608c6eef8e74f926f9c1e89753a99_720w.jpg?source=1940ef5c" alt="img"><img src="https://pic3.zhimg.com/80/0ab608c6eef8e74f926f9c1e89753a99_1440w.jpg?source=1940ef5c" alt="img"></p>
<p><strong>### RAID1</strong></p>
<p>RAID1 称为镜像，它将数据完全一致地分别写到工作磁盘和镜像 磁盘，它的磁盘空间利用率为 50% 。 RAID1 在数据写入时，响应时间会有所影响，但是读数据的时候没有影响。 RAID1 提供了最佳的数据保护，一旦工作磁盘发生故障，系统自动从镜像磁盘读取数据，不会影响用户工作。</p>
<p>RAID1 与 RAID0 刚好相反，是为了增强数据安全性使两块 磁盘数据呈现完全镜像，从而达到安全性好、技术简单、管理方便。 RAID1 拥有完全容错的能力，但实现成本高。 RAID1 应用于对顺序读写性能要求高以及对数据保护极为重视的应用，如对邮件系统的数据保护。</p>
<p><img src="https://pic1.zhimg.com/50/595a2d853196c5b38ceee5d98032baeb_720w.jpg?source=1940ef5c" alt="img"><img src="https://pic1.zhimg.com/80/595a2d853196c5b38ceee5d98032baeb_1440w.jpg?source=1940ef5c" alt="img"></p>
<p><strong>### RAID5</strong></p>
<p>RAID5 应该是目前最常见的 RAID 等级，它的原理与 RAID4 相似，区别在于校验数据分布在阵列中的所有磁盘上，而没有采用专门的校验磁盘。对于数据和校验数据，它们的写操作可以同时发生在完全不同的磁盘上。因此， RAID5 不存在 RAID4 中的并发写操作时的校验盘性能瓶颈问题。另外， RAID5 还具备很好的扩展性。当阵列磁盘 数量增加时，并行操作量的能力也随之增长，可比 RAID4 支持更多的磁盘，从而拥有更高的容量以及更高的性能。</p>
<p>RAID5 的磁盘上同时存储数据和校验数据，数据块和对应的校验信息存保存在不同的磁盘上，当一个数据盘损坏时，系统可以根据同一条带的其他数据块和对应的校验数据来重建损坏的数据。与其他 RAID 等级一样，重建数据时， RAID5 的性能会受到较大的影响。</p>
<p>RAID5 兼顾存储性能、数据安全和存储成本等各方面因素，它可以理解为 RAID0 和 RAID1 的折中方案，是目前综合性能最佳的数据保护解决方案。 RAID5 基本上可以满足大部分的存储应用需求，数据中心大多采用它作为应用数据的保护方案。</p>
<p><img src="https://pic1.zhimg.com/50/8ff9b2beeaf295dd1f41d98af50d1ebf_720w.jpg?source=1940ef5c" alt="img"><img src="https://pic1.zhimg.com/80/8ff9b2beeaf295dd1f41d98af50d1ebf_1440w.jpg?source=1940ef5c" alt="img"></p>
<p><strong>### RAID6</strong></p>
<p>前面所 述的各个 RAID 等级都只能保护因单个磁盘失效而造成的数据丢失。如果两个磁盘同时发生故障，数据将无法恢复。 RAID6 引入双重校验的概念，它可以保护阵列中同时出现两个磁盘失效时，阵列仍能够继续工作，不会发生数据丢失。 RAID6 等级是在 RAID5 的基础上为了进一步增强数据保护而设计的一种 RAID 方式，它可以看作是一种扩展的 RAID5 等级。</p>
<p>RAID6 不仅要支持数据的恢复，还要支持校验数据的恢复，因此实现代价很高，控制器的设计也比其他等级更复杂、更昂贵。 RAID6 思想最常见的实现方式是采用两个独立的校验算法，假设称为 P 和 Q ，校验数据可以分别存储在两个不同的校验盘上，或者分散存储在所有成员磁盘中。当两个磁盘同时失效时，即可通过求解两元方程来重建两个磁盘上的数据。</p>
<p>RAID6 具有快速的读取性能、更高的容错能力。但是，它的成本要高于 RAID5 许多，写性能也较差，并有设计和实施非常复杂。因此， RAID6 很少得到实际应用，主要用于对数据安全等级要求非常高的场合。它一般是替代 RAID10 方案的经济性选择</p>
<p><img src="https://pic2.zhimg.com/50/e3d61ad32bd894385be86e3c1fcc8ff7_720w.jpg?source=1940ef5c" alt="img"><img src="https://pic2.zhimg.com/80/e3d61ad32bd894385be86e3c1fcc8ff7_1440w.jpg?source=1940ef5c" alt="img"></p>
<p><strong>### RAID 组合等级</strong></p>
<p>标准 RAID 等级各有优势和不足。自然地，我们想到把多个 RAID 等级组合起来，实现优势互补，弥补相互的不足，从而达到在性能、数据安全性等指标上更高的 RAID 系统。目前在业界和学术研究中提到的 RAID 组合等级主要有 RAID00 、 RAID01 、 RAID10 、 RAID100 、 RAID30 、 RAID50 、 RAID53 、 RAID60 ，但实际得到较为广泛应用的只有 RAID01 和 RAID10 两个等级。当然，组合等级的实现成本一般都非常昂贵，只是在 少数特定场合应用。</p>
<p><strong>### RAID10 和 RAID01</strong></p>
<p>一些文献把这两种 RAID 等级看作是等同的，本文认为是不同的。 RAID01 是先做条带化再作镜像，本质是对物理磁盘实现镜像；而 RAID10 是先做镜像再作条带化，是对虚拟磁盘实现镜像。相同的配置下，通常 RAID01 比 RAID10 具有更好的容错能力。</p>
<p>RAID01 兼备了 RAID0 和 RAID1 的优点，它先用两块磁盘建立镜像，然后再在镜像内部做条带化。 RAID01 的数据将同时写入到两个磁盘阵列中，如果其中一个阵列损坏，仍可继续工作，保证数据安全性的同时又提高了性能。 RAID01 和 RAID10 内部都含有 RAID1 模式，因此整体磁盘利用率均仅为 50% 。</p>
<p><img src="https://pic2.zhimg.com/50/29966aef58264fa7eabd94b2baa2fe43_720w.jpg?source=1940ef5c" alt="img"><img src="https://pic2.zhimg.com/80/29966aef58264fa7eabd94b2baa2fe43_1440w.jpg?source=1940ef5c" alt="img"></p>
<p><img src="https://pica.zhimg.com/50/b522ccd35950243675d164bbdb36011e_720w.jpg?source=1940ef5c" alt="img"><img src="https://pica.zhimg.com/80/b522ccd35950243675d164bbdb36011e_1440w.jpg?source=1940ef5c" alt="img"></p>
<p><strong>### RAID 50</strong></p>
<p>RAID 5与RAID 0的组合，先作RAID 5，再作RAID 0，也就是对多组RAID 5彼此构成Stripe访问。由于RAID 50是以RAID 5为基础，而RAID 5至少需要3颗硬盘，因此要以多组RAID 5构成RAID 50，至少需要6颗硬盘。以RAID 50最小的6颗硬盘配置为例，先把6颗硬盘分为2组，每组3颗构成RAID 5，如此就得到两组RAID 5，然后再把两组RAID 5构成RAID 0。</p>
<p>RAID 50在底层的任一组或多组RAID 5中出现1颗硬盘损坏时，仍能维持运作，不过如果任一组RAID 5中出现2颗或2颗以上硬盘损毁，整组RAID 50就会失效。</p>
<p>RAID 50由于在上层把多组RAID 5构成Stripe，性能比起单纯的RAID 5高，容量利用率比RAID5要低。比如同样使用9颗硬盘，由各3颗RAID 5再组成RAID 0的RAID 50，每组RAID 5浪费一颗硬盘，利用率为(1-3&#x2F;9)，RAID 5则为(1-1&#x2F;9)。</p>
<p><img src="https://pic3.zhimg.com/50/8c26881d67c3fc5d88b2172bbed85f48_720w.jpg?source=1940ef5c" alt="img"><img src="https://pic3.zhimg.com/80/8c26881d67c3fc5d88b2172bbed85f48_1440w.jpg?source=1940ef5c" alt="img"></p>
<p><strong>### RAID 60</strong></p>
<p>RAID 6与RAID 0的组合：先作RAID 6，再作RAID 0。换句话说，就是对两组以上的RAID 6作Stripe访问。RAID 6至少需具备4颗硬盘，所以RAID 60的最小需求是8颗硬盘。</p>
<p>由于底层是以RAID 6组成，所以RAID 60可以容许任一组RAID 6中损毁最多2颗硬盘，而系统仍能维持运作；不过只要底层任一组RAID 6中损毁3颗硬盘，整组RAID 60就会失效，当然这种情况的概率相当低。</p>
<p>比起单纯的RAID 6，RAID 60的上层通过结合多组RAID 6构成Stripe访问，因此性能较高。不过使用门槛高，而且容量利用率低是较大的问题。</p>
<p><img src="https://pic1.zhimg.com/50/7d1922f356f6e2be74cdae8c3cc8b2ac_720w.jpg?source=1940ef5c" alt="img"><img src="https://pic1.zhimg.com/80/7d1922f356f6e2be74cdae8c3cc8b2ac_1440w.jpg?source=1940ef5c" alt="img"></p>
<p><strong>## 关于RAID参数调优</strong></p>
<p>&gt; 通常情况下建议系统(RAID1)与数据(RAID[5|10])分离，这里引用@叶金荣老师的一段话</p>
<p>\1. 使用SSD或者PCIe SSD设备，至少获得数百倍甚至万倍的IOPS提升<br>\2. 购置阵列卡同时配备CACHE及BBU模块，可明显提升IOPS（主要是指机械盘，SSD或PCIe SSD除外。同时需要定期检查CACHE及BBU模块的健康状况，确保意外时不至于丢失数据）<br>\3. 有阵列卡时，设置阵列写策略为WB，甚至FORCE WB（若有双电保护，或对数据安全性要求不是特别高的话），严禁使用WT策略。并且闭阵列预读策略<br>\4. 尽可能选用RAID-10，而非RAID-5(<code>这句话有待商榷</code>)<br>\5. 使用机械盘的话，尽可能选择高转速的，例如选用15KRPM，而不是7.2KRPM的盘，不差几个钱的；</p>
<p><strong>## SSD阵列卡方案优化</strong></p>
<p>&gt; 感谢@小米noops运维团队，详细实验数据请参考扩展阅读</p>
<p><img src="https://pic1.zhimg.com/50/c4132b7b42d2758db39d1644ee5a3963_720w.jpg?source=1940ef5c" alt="img"><img src="https://pic1.zhimg.com/80/c4132b7b42d2758db39d1644ee5a3963_1440w.jpg?source=1940ef5c" alt="img"></p>
<p><img src="https://pica.zhimg.com/50/c76b4b4d0d3877d242ccbc037e3c3aff_720w.jpg?source=1940ef5c" alt="img"><img src="https://pica.zhimg.com/80/c76b4b4d0d3877d242ccbc037e3c3aff_1440w.jpg?source=1940ef5c" alt="img"></p>
<p><strong>性能测试结论</strong></p>
<p>性能测试显示，相同容量的R50和R10性能接近：其中小块文件的随机读R50要全面好于R10，随机写4K虽然R50和R10差距在28%，但是块增大后R50要全面优于R10。顺序读写方面，R50和R10十分接近。</p>
<p>容错方面，R50接近R10：第二块盘容错率R50十分接近R10，两者相差30%。R10的优势主要是在有一定的概率提供第三、甚至第四块磁盘的容错率，但是考虑到并非100%容错，因此从容错角度来看R50虽然和R10有一些差距，但也已体现出较好的容错率，至少优于R5。而且R50搭配灵活，甚至可以指定3组R5以达到最大3块磁盘的容错；</p>
<p>成本方面，R50有很大优势：按这个配置计算R50只有R10的3&#x2F;4。</p>
<p><strong>总结</strong></p>
<p>RAID 50提供了接近RAID 10性能、可用性以及接近RAID 5成本的特性，具有较好的整体性价比优势，所以考虑使用RAID 50替换RAID 10吧</p>
]]></content>
  </entry>
  <entry>
    <title>ceph安装以及测试</title>
    <url>/ceph%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E6%B5%8B%E8%AF%95/index.html</url>
    <content><![CDATA[]]></content>
  </entry>
</search>
