<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>spark null 值处理</title>
    <url>/2023/08/16/spark-null-%E5%80%BC%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<p> scala spark na null处理 | 分子美食家的博客                  </p>
<h1 id="分子美食家的博客"><a href="#分子美食家的博客" class="headerlink" title="分子美食家的博客"></a><a href="/">分子美食家的博客</a></h1><h2 id="野生工程师的专栏"><a href="#野生工程师的专栏" class="headerlink" title="野生工程师的专栏"></a><a href="/">野生工程师的专栏</a></h2><p><a href="/">Home</a> <a href="/archives">Archives</a></p>
<p><a href="/atom.xml" title="RSS Feed"></a></p>
<p></p>
<p><a href="/2022/05/14/scala-spark-na-null%E5%A4%84%E7%90%86/">2022-05-14</a></p>
<h1 id="scala-spark-na-null处理"><a href="#scala-spark-na-null处理" class="headerlink" title="scala spark na null处理"></a>scala spark na null处理</h1><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parseCheck</span></span>(s: <span class="type">String</span>,min:<span class="type">Double</span>,max:<span class="type">Double</span>): <span class="type">Option</span>[<span class="type">Double</span>] = {</span><br><span class="line">      <span class="keyword">try</span> {</span><br><span class="line">        <span class="keyword">val</span> va = s.toDouble</span><br><span class="line">        <span class="keyword">if</span> (va &lt; min || va &gt; max) {</span><br><span class="line">          <span class="type">Some</span>(va)</span><br><span class="line">        }<span class="keyword">else</span> {</span><br><span class="line">          <span class="type">Some</span>(<span class="type">Double</span>.<span class="type">NaN</span>)</span><br><span class="line">        }</span><br><span class="line">      }</span><br><span class="line">      <span class="keyword">catch</span> { <span class="keyword">case</span> _ =&gt; <span class="type">Some</span>(<span class="type">Double</span>.<span class="type">NaN</span>)</span><br><span class="line">      }</span><br><span class="line">    }</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parseDouble</span></span>(s: <span class="type">String</span>): <span class="type">Option</span>[<span class="type">Double</span>] = <span class="keyword">try</span> { <span class="type">Some</span>(s.toDouble) } <span class="keyword">catch</span> { <span class="keyword">case</span> _ =&gt; <span class="type">Some</span>(<span class="type">Double</span>.<span class="type">NaN</span>) }</span><br></pre></td></tr></tbody></table>

<p>Share</p>
<p>[<strong>Older</strong></p>
<p>scala-spark-dataframe—groupby基本操作</p>
<p>](&#x2F;2022&#x2F;05&#x2F;07&#x2F;scala-spark-dataframe%E2%80%94groupby%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C&#x2F;)</p>
<h3 id="Archives"><a href="#Archives" class="headerlink" title="Archives"></a>Archives</h3><ul>
<li><a href="/archives/2022/05/">May 2022</a></li>
</ul>
<h3 id="Recent-Posts"><a href="#Recent-Posts" class="headerlink" title="Recent Posts"></a>Recent Posts</h3><ul>
<li><a href="/2022/05/14/scala-spark-na-null%E5%A4%84%E7%90%86/">scala spark na null处理</a></li>
<li><a href="/2022/05/07/scala-spark-dataframe%E2%80%94groupby%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/">scala-spark-dataframe—groupby基本操作</a></li>
<li><a href="/2022/05/04/fedora-k8s/">fedora-k8s</a></li>
<li><a href="/2022/05/04/openwrt%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/">openwrt安装配置</a></li>
<li><a href="/2022/05/04/ubuntu-supervisor%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/">ubuntu supervisor进程管理</a></li>
</ul>
<p>© 2022 andrew<br>Powered by <a href="https://hexo.io/">Hexo</a></p>
<p><a href="/">Home</a> <a href="/archives">Archives</a></p>
<p>%</p>
]]></content>
  </entry>
  <entry>
    <title>spark group by 操作</title>
    <url>/2023/08/16/spark-group-by-%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<p>scala-spark-dataframe—groupby基本操作<br>[andrew@hadoop102 bin]$ .&#x2F;spark-shell<br>2022-05-07 20:35:13,392 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>Setting default log level to “WARN”.<br>To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).<br>Spark context Web UI available at <a href="http://hadoop102:4040/">http://hadoop102:4040</a><br>Spark context available as ‘sc’ (master &#x3D; local[*], app id &#x3D; local-1651926921962).<br>Spark session available as ‘spark’.<br>Welcome to<br>      ____              __<br>     &#x2F; <strong>&#x2F;</strong>  ___ _<em><em><strong>&#x2F; &#x2F;</strong><br>    <em>\ / _ / _ &#96;&#x2F; _<em>&#x2F;  ‘</em>&#x2F;<br>   &#x2F;_</em></em>&#x2F; .__&#x2F;_,</em>&#x2F;<em>&#x2F; &#x2F;</em>&#x2F;_\   version 3.0.0<br>      &#x2F;_&#x2F;</p>
<p>Using Scala version 2.12.10 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_212)<br>Type in expressions to have them evaluated.<br>Type :help for more information.</p>
<p>scala&gt; val df &#x3D; spark.createDataset(Seq(<br>     |   (“aaa”,1,2),(“bbb”,3,4),(“ccc”,3,5),(“bbb”,4, 6))   ).toDF(“key1”,”key2”,”key3”)<br>df: org.apache.spark.sql.DataFrame &#x3D; [key1: string, key2: int … 1 more field]</p>
<p>scala&gt; df.show()<br>+—-+—-+—-+<br>|key1|key2|key3|<br>+—-+—-+—-+<br>| aaa|   1|   2|<br>| bbb|   3|   4|<br>| ccc|   3|   5|<br>| bbb|   4|   6|<br>+—-+—-+—-+</p>
<p>scala&gt; df.printSchema()<br>root<br> |– key1: string (nullable &#x3D; true)<br> |– key2: integer (nullable &#x3D; false)<br> |– key3: integer (nullable &#x3D; false)</p>
<p>scala&gt; df.groupBy(“key1”).count.show<br>+—-+—–+<br>|key1|count|<br>+—-+—–+<br>| ccc|    1|<br>| aaa|    1|<br>| bbb|    2|<br>+—-+—–+</p>
<p>scala&gt; df.select(“key1”).distinct.show<br>+—-+<br>|key1|<br>+—-+<br>| ccc|<br>| aaa|<br>| bbb|<br>+—-+</p>
<p>scala&gt; df.select(“key1”).distinct.count<br>res4: Long &#x3D; 3</p>
<p>scala&gt; f.groupBy(“key1”).count.sort(“key1”).show<br><console>:24: error: not found: value f<br>       f.groupBy(“key1”).count.sort(“key1”).show<br>       ^</p>
<p>scala&gt; df.groupBy(“key1”).count.sort(“key1”).show<br>+—-+—–+<br>|key1|count|<br>+—-+—–+<br>| aaa|    1|<br>| bbb|    2|<br>| ccc|    1|<br>+—-+—–+</p>
<p>scala&gt; df.groupBy(“key1”).count.sort($”count”.desc).show<br>+—-+—–+<br>|key1|count|<br>+—-+—–+<br>| bbb|    2|<br>| ccc|    1|<br>| aaa|    1|<br>+—-+—–+</p>
<p>scala&gt; df.groupBy(“key1”).count.withColumnRenamed(“count”, “cnt”).sort($”cnt”.desc).show<br>+—-+—+<br>|key1|cnt|<br>+—-+—+<br>| bbb|  2|<br>| aaa|  1|<br>| ccc|  1|<br>+—-+—+</p>
<p>scala&gt; df.groupBy(“key1”).agg(count(“key1”).as(“cnt”)).show<br>+—-+—+<br>|key1|cnt|<br>+—-+—+<br>| ccc|  1|<br>| aaa|  1|<br>| bbb|  2|<br>+—-+—+</p>
<p>scala&gt;  df.groupBy(“key1”).agg(count(“key1”), max(“key2”), avg(“key3”)).show<br>+—-+———–+———+———+<br>|key1|count(key1)|max(key2)|avg(key3)|<br>+—-+———–+———+———+<br>| ccc|          1|        3|      5.0|<br>| aaa|          1|        1|      2.0|<br>| bbb|          2|        4|      5.0|<br>+—-+———–+———+———+</p>
<p>scala&gt; f.groupBy(“key1”)<br><console>:24: error: not found: value f<br>       f.groupBy(“key1”)<br>       ^</p>
<p>scala&gt;        df.groupBy(“key1”).agg(“key1”-&gt;”count”, “key2”-&gt;”max”, “key3”-&gt;”avg”).show<br>+—-+———–+———+———+<br>|key1|count(key1)|max(key2)|avg(key3)|<br>+—-+———–+———+———+<br>| ccc|          1|        3|      5.0|<br>| aaa|          1|        1|      2.0|<br>| bbb|          2|        4|      5.0|<br>+—-+———–+———+———+</p>
<p>scala&gt; df.groupBy(“key1”).agg(Map((“key1”,”count”), (“key2”,”max”), (“key3”,”avg”))).show<br>+—-+———–+———+———+<br>|key1|count(key1)|max(key2)|avg(key3)|<br>+—-+———–+———+———+<br>| ccc|          1|        3|      5.0|<br>| aaa|          1|        1|      2.0|<br>| bbb|          2|        4|      5.0|<br>+—-+———–+———+———+</p>
<p>scala&gt; df.groupBy(“key1”).agg(count(“key1”).as(“cnt”), max(“key2”).as(“max_key2”), avg(“key3”).as(“avg_key3”)).sort($”cnt”,$”max_key2”.desc).show<br>+—-+—+——–+——–+<br>|key1|cnt|max_key2|avg_key3|<br>+—-+—+——–+——–+<br>| ccc|  1|       3|     5.0|<br>| aaa|  1|       1|     2.0|<br>| bbb|  2|       4|     5.0|<br>+—-+—+——–+——–+</p>
<p>package groupby</p>
<p>import org.apache.spark.SparkConf<br>import org.apache.spark.sql.SparkSession</p>
<p>object demos {</p>
<p>  def main(args: Array[String]): Unit &#x3D; {<br>    val conf &#x3D; new SparkConf().setAppName(“LzSparkDatasetExamples”).setMaster(“local[*]”)<br>    val sparkSession &#x3D; SparkSession.builder().enableHiveSupport().config(conf).getOrCreate()</p>
<p>&#x2F;&#x2F;    &#x2F;&#x2F;LOGGER.info(“——– this is info ——–”)<br>    import sparkSession.implicits._<br>    val df &#x3D; sparkSession.createDataset(Seq(<br>      (“aaa”, 1, 2),<br>      (“bbb”, 3, 4),<br>      (“ccc”, 3, 5),<br>      (“bbb”, 4, 6)<br>    )).toDF(“key1”, “key2”, “key3”)</p>
<pre><code>import org.apache.spark.sql.functions._
//LOGGER.info(&quot;--------df.groupBy(\&quot;key1\&quot;).count().show()-----------&quot;)
df.groupBy(&quot;key1&quot;).count().show()
//LOGGER.info(&quot;--------df.select(\&quot;key1\&quot;).distinct().show()-----------&quot;)
df.select(&quot;key1&quot;).distinct().show()
val key1Count = df.select(&quot;key1&quot;).distinct().count()
//LOGGER.info(&quot;--------df.select(\&quot;key1\&quot;).distinct().count()-----------&quot; +key1Count)
//LOGGER.info(&quot;--------df.groupBy(\&quot;key1\&quot;).count().sort(\&quot;key1\&quot;).show()-----------&quot;)
df.groupBy(&quot;key1&quot;).count().sort(&quot;key1&quot;).show()
//LOGGER.info(&quot;--------df.groupBy(\&quot;key1\&quot;).count().sort($\&quot;key1\&quot;.desc).show()-----------&quot;)
df.groupBy(&quot;key1&quot;).count().sort($&quot;key1&quot;.desc).show()
//LOGGER.info(&quot;--------df.groupBy(\&quot;key1\&quot;).count.withColumnRenamed(\&quot;count\&quot;, \&quot;cnt\&quot;).sort($\&quot;cnt\&quot;.desc).show-----------&quot;)
df.groupBy(&quot;key1&quot;).count
  .withColumnRenamed(&quot;count&quot;, &quot;cnt&quot;).sort($&quot;cnt&quot;.desc).show()
//LOGGER.info(&quot;--------df.groupBy(\&quot;key1\&quot;).agg(count(\&quot;key1\&quot;).as(\&quot;cnt\&quot;)).show-----------&quot;)
df.groupBy(&quot;key1&quot;).agg(count(&quot;key1&quot;).as(&quot;cnt&quot;)).show()

// 使用agg聚合函数
df.groupBy(&quot;key1&quot;).agg(count(&quot;key1&quot;), max(&quot;key2&quot;), avg(&quot;key3&quot;)).show
df.groupBy(&quot;key1&quot;).agg(&quot;key1&quot;-&gt;&quot;count&quot;, &quot;key2&quot;-&gt;&quot;max&quot;, &quot;key3&quot;-&gt;&quot;avg&quot;).show()
df.groupBy(&quot;key1&quot;).agg(Map((&quot;key1&quot;,&quot;count&quot;), (&quot;key2&quot;,&quot;max&quot;), (&quot;key3&quot;,&quot;avg&quot;))).show()
df.groupBy(&quot;key1&quot;).agg((&quot;key1&quot;,&quot;count&quot;), (&quot;key2&quot;,&quot;max&quot;), (&quot;key3&quot;,&quot;avg&quot;)).show
df.groupBy(&quot;key1&quot;)
  .agg(count(&quot;key1&quot;).as(&quot;cnt&quot;), max(&quot;key2&quot;).as(&quot;max_key2&quot;), avg(&quot;key3&quot;).as(&quot;avg_key3&quot;))
  .sort($&quot;cnt&quot;,$&quot;max_key2&quot;.desc).show
</code></pre>
<p>  }</p>
<p>}</p>
]]></content>
  </entry>
  <entry>
    <title>ceph分布式安装</title>
    <url>/2023/08/16/ceph%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>docker基本操作</title>
    <url>/2023/08/16/docker%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>elasticsearch分布式安装</title>
    <url>/2023/08/16/elasticsearch%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>fedora k8s 安装</title>
    <url>/2023/08/16/fedora-k8s-%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>gitlab_ci runners</title>
    <url>/2023/08/16/gitlab-ci-runners/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>hadoop安装</title>
    <url>/2023/08/16/hadoop%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>k8s安装</title>
    <url>/2023/08/16/k8s%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>kubeflow基础搭建</title>
    <url>/2023/08/16/kubeflow%E5%9F%BA%E7%A1%80%E6%90%AD%E5%BB%BA/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>linux硬盘格式化与挂载</title>
    <url>/2023/08/16/linux%E7%A1%AC%E7%9B%98%E6%A0%BC%E5%BC%8F%E5%8C%96%E4%B8%8E%E6%8C%82%E8%BD%BD/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>loguru elasticsearch kibana 日志处理</title>
    <url>/2023/08/16/loguru-elasticsearch-kibana-%E6%97%A5%E5%BF%97%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>minio分布式安装</title>
    <url>/2023/08/16/minio%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>mongodb 分布式安装</title>
    <url>/2023/08/16/mongodb-%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>redis分布式安装</title>
    <url>/2023/08/16/redis%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<p>1.环境配置<br>#在所有节点配置YUM：<br>#清空原来自带配置文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /etc/yum.repos.d/</span><br><span class="line"><span class="built_in">mkdir</span> /tmp/bak</span><br><span class="line"><span class="built_in">mv</span> * /tmp/bak/</span><br></pre></td></tr></table></figure>
<p>#配置系统源码，epel源：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo</span><br><span class="line">yum install wget -y</span><br><span class="line">wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo</span><br><span class="line"><span class="comment">#YUM优先级别：</span></span><br><span class="line">yum -y install yum-plugin-priorities.noarch</span><br></pre></td></tr></table></figure>
<p>#配置ceph源：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cat</span> &lt;&lt; <span class="string">EOF | tee /etc/yum.repos.d/ceph.repo</span></span><br><span class="line"><span class="string">[Ceph]</span></span><br><span class="line"><span class="string">name=Ceph packages for $basearch</span></span><br><span class="line"><span class="string">baseurl=http://mirrors.163.com/ceph/rpm-nautilus/el7/\$basearch</span></span><br><span class="line"><span class="string">enabled=1</span></span><br><span class="line"><span class="string">gpgcheck=1</span></span><br><span class="line"><span class="string">type=rpm-md</span></span><br><span class="line"><span class="string">gpgkey=https://download.ceph.com/keys/release.asc</span></span><br><span class="line"><span class="string">priority=1</span></span><br><span class="line"><span class="string">[Ceph-noarch]</span></span><br><span class="line"><span class="string">name=Ceph noarch packages</span></span><br><span class="line"><span class="string">baseurl=http://mirrors.163.com/ceph/rpm-nautilus/el7/noarch</span></span><br><span class="line"><span class="string">enabled=1</span></span><br><span class="line"><span class="string">gpgcheck=1</span></span><br><span class="line"><span class="string">type=rpm-md</span></span><br><span class="line"><span class="string">gpgkey=https://download.ceph.com/keys/release.asc</span></span><br><span class="line"><span class="string">priority=1</span></span><br><span class="line"><span class="string">[ceph-source]</span></span><br><span class="line"><span class="string">name=Ceph source packages</span></span><br><span class="line"><span class="string">baseurl=http://mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS</span></span><br><span class="line"><span class="string">enabled=1</span></span><br><span class="line"><span class="string">gpgcheck=1</span></span><br><span class="line"><span class="string">type=rpm-md</span></span><br><span class="line"><span class="string">gpgkey=https://download.ceph.com/keys/release.asc</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure>
<p>#关闭防火墙：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld</span><br><span class="line">systemctl status firewalld</span><br></pre></td></tr></table></figure>
<p>#配置主机名称：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph1节点：</span><br><span class="line">hostnamectl --static set-hostname ceph1</span><br><span class="line">ceph2节点：</span><br><span class="line">hostnamectl --static set-hostname ceph2</span><br><span class="line">ceph3节点：</span><br><span class="line">hostnamectl --static set-hostname ceph3</span><br></pre></td></tr></table></figure>
<p>#所有节点配置hosts文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/etc/hosts</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">192.168.0.231    ceph1</span><br><span class="line">192.168.0.232    ceph2</span><br><span class="line">192.168.0.233    ceph3</span><br></pre></td></tr></table></figure>

<p>#所有节点NTP配置：<br>在所有集群和客户端节点安装NTP，修改配置。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum -y install ntp ntpdate</span><br><span class="line"><span class="comment"># 以ceph1为NTP服务端节点，在ceph1新建NTP文件。</span></span><br><span class="line">vi /etc/ntp.conf</span><br><span class="line"><span class="comment"># 并新增如下内容作为NTP服务端：</span></span><br><span class="line">restrict 127.0.0.1</span><br><span class="line">restrict ::1</span><br><span class="line">restrict 192.168.3.0 mask 255.255.255.0 //ceph1的网段与掩码</span><br><span class="line">server 127.127.1.0</span><br><span class="line">fudge 127.127.1.0 stratum 8</span><br></pre></td></tr></table></figure>
<p>在ceph2、ceph3及所有客户机节点新建NTP文件。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vi /etc/ntp.conf</span><br><span class="line"><span class="comment"># 并新增如下内容作为客户端：</span></span><br><span class="line">server 192.168.3.166</span><br><span class="line"></span><br><span class="line">systemctl start ntpd</span><br><span class="line">systemctl <span class="built_in">enable</span> ntpd</span><br><span class="line">systemctl status ntpd</span><br></pre></td></tr></table></figure>
<p>#ssh配置，在ceph1节点生成公钥，并发放到各个主机&#x2F;客户机节点。：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa <span class="comment">#回车采取默认配置</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..3&#125;; <span class="keyword">do</span> ssh-copy-id ceph<span class="variable">$i</span>; <span class="keyword">done</span> <span class="comment">#根据提示输入yes及节点密码</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..3&#125;; <span class="keyword">do</span> ssh-copy-id client<span class="variable">$i</span>; <span class="keyword">done</span></span><br></pre></td></tr></table></figure>

<p>#在所有节点，关闭SELinux</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sed -i <span class="string">&#x27;s/enforcing/disabled/&#x27;</span> /etc/selinux/config</span><br><span class="line">setenforce 0</span><br></pre></td></tr></table></figure>
<ol>
<li>安装Ceph软件<br>使用yum install安装ceph的时候会默认安装当前已有的最新版，如果不想安装最新版本，可以在&#x2F;etc&#x2F;yum.conf文件中加以限制。<br>2.1 在所有集群和客户端节点安装Ceph<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum -y install ceph</span><br><span class="line">ceph -v命令查看版本:</span><br><span class="line">[root@ceph1 ~]<span class="comment"># ceph -v</span></span><br><span class="line">ceph version 14.2.9 (581f22da52345dba46ee232b73b990f06029a2a0) nautilus (stable)</span><br><span class="line">[root@ceph2 ~]<span class="comment"># ceph -v</span></span><br><span class="line">ceph version 14.2.9 (581f22da52345dba46ee232b73b990f06029a2a0) nautilus (stable)</span><br><span class="line">[root@ceph3 ~]<span class="comment"># ceph -v</span></span><br><span class="line">ceph version 14.2.9 (581f22da52345dba46ee232b73b990f06029a2a0) nautilus (stable)</span><br></pre></td></tr></table></figure>
2.2 在ceph1节点额外安装ceph-deploy。<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum -y install ceph-deploy</span><br></pre></td></tr></table></figure>
3.部署MON节点<br>3.1 创建目录生成配置文件<figure class="highlight stata"><table><tr><td class="code"><pre><span class="line"><span class="keyword">mkdir</span> <span class="keyword">cluster</span></span><br><span class="line"><span class="keyword">cd</span> <span class="keyword">cluster</span></span><br><span class="line">ceph-deploy new ceph1 ceph2 ceph3</span><br></pre></td></tr></table></figure></li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@ceph1 ~]<span class="comment"># cd cluster/</span></span><br><span class="line">[root@ceph1 cluster]<span class="comment"># ceph-deploy new ceph1 ceph2 ceph3</span></span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy new ceph1 ceph2 ceph3</span><br><span class="line">[ceph_deploy.cli][INFO  ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO  ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  func                          : &lt;<span class="keyword">function</span> new at 0x7ffb7dc07de8&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  overwrite_conf                : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  quiet                         : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7ffb7d58c6c8&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO  ]  ssh_copykey                   : True</span><br><span class="line">[ceph_deploy.cli][INFO  ]  mon                           : [<span class="string">&#x27;ceph1&#x27;</span>, <span class="string">&#x27;ceph2&#x27;</span>, <span class="string">&#x27;ceph3&#x27;</span>]</span><br><span class="line">[ceph_deploy.cli][INFO  ]  public_network                : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  ceph_conf                     : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cluster_network               : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  default_release               : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  fsid                          : None</span><br><span class="line">[ceph_deploy.new][DEBUG ] Creating new cluster named ceph</span><br><span class="line">[ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds</span><br><span class="line">[ceph1][DEBUG ] connected to host: ceph1 </span><br><span class="line">[ceph1][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph1][DEBUG ] detect machine <span class="built_in">type</span></span><br><span class="line">[ceph1][DEBUG ] find the location of an executable</span><br><span class="line">[ceph1][INFO  ] Running <span class="built_in">command</span>: /usr/sbin/ip <span class="built_in">link</span> show</span><br><span class="line">[ceph1][INFO  ] Running <span class="built_in">command</span>: /usr/sbin/ip addr show</span><br><span class="line">[ceph1][DEBUG ] IP addresses found: [u<span class="string">&#x27;192.168.0.231&#x27;</span>]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Resolving host ceph1</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor ceph1 at 192.168.0.231</span><br><span class="line">[ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds</span><br><span class="line">[ceph2][DEBUG ] connected to host: ceph1 </span><br><span class="line">[ceph2][INFO  ] Running <span class="built_in">command</span>: ssh -CT -o BatchMode=<span class="built_in">yes</span> ceph2</span><br><span class="line">[ceph2][DEBUG ] connected to host: ceph2 </span><br><span class="line">[ceph2][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph2][DEBUG ] detect machine <span class="built_in">type</span></span><br><span class="line">[ceph2][DEBUG ] find the location of an executable</span><br><span class="line">[ceph2][INFO  ] Running <span class="built_in">command</span>: /usr/sbin/ip <span class="built_in">link</span> show</span><br><span class="line">[ceph2][INFO  ] Running <span class="built_in">command</span>: /usr/sbin/ip addr show</span><br><span class="line">[ceph2][DEBUG ] IP addresses found: [u<span class="string">&#x27;192.168.0.232&#x27;</span>]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Resolving host ceph2</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor ceph2 at 192.168.0.232</span><br><span class="line">[ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds</span><br><span class="line">[ceph3][DEBUG ] connected to host: ceph1 </span><br><span class="line">[ceph3][INFO  ] Running <span class="built_in">command</span>: ssh -CT -o BatchMode=<span class="built_in">yes</span> ceph3</span><br><span class="line">[ceph3][DEBUG ] connected to host: ceph3 </span><br><span class="line">[ceph3][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph3][DEBUG ] detect machine <span class="built_in">type</span></span><br><span class="line">[ceph3][DEBUG ] find the location of an executable</span><br><span class="line">[ceph3][INFO  ] Running <span class="built_in">command</span>: /usr/sbin/ip <span class="built_in">link</span> show</span><br><span class="line">[ceph3][INFO  ] Running <span class="built_in">command</span>: /usr/sbin/ip addr show</span><br><span class="line">[ceph3][DEBUG ] IP addresses found: [u<span class="string">&#x27;192.168.0.233&#x27;</span>]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Resolving host ceph3</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor ceph3 at 192.168.0.233</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor initial members are [<span class="string">&#x27;ceph1&#x27;</span>, <span class="string">&#x27;ceph2&#x27;</span>, <span class="string">&#x27;ceph3&#x27;</span>]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor addrs are [<span class="string">&#x27;192.168.0.231&#x27;</span>, <span class="string">&#x27;192.168.0.232&#x27;</span>, <span class="string">&#x27;192.168.0.233&#x27;</span>]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Creating a random mon key...</span><br><span class="line">[ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...</span><br><span class="line">[ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...</span><br></pre></td></tr></table></figure>
<p>3.2 初始化密钥</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-deploy mon create-initial</span><br></pre></td></tr></table></figure>
<p>3.3 将ceph.client.admin.keyring拷贝到各个节点上</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-deploy --overwrite-conf admin ceph1 ceph2 ceph3</span><br></pre></td></tr></table></figure>
<p>3.4 查看是否配置成功。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@ceph1 cluster]<span class="comment"># ceph -scluster:id:     ea192428-05d2-437a-8cce-9d187de82dd5health: HEALTH_OKservices:mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 5m)mgr: no daemons activeosd: 0 osds: 0 up, 0 indata:pools:   0 pools, 0 pgsobjects: 0 objects, 0 Busage:   0 B used, 0 B / 0 B availpgs:  </span></span><br></pre></td></tr></table></figure>
<p>4 部署MGR节点</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-deploy mgr create ceph1 ceph2 ceph3</span><br></pre></td></tr></table></figure>
<p>查看MGR是否部署成功。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph -s</span><br><span class="line">[root@ceph1 cluster]<span class="comment"># ceph -scluster:id:     ea192428-05d2-437a-8cce-9d187de82dd5health: HEALTH_WARNOSD count 0 &lt; osd_pool_default_size 3services:mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 8m)mgr: ceph1(active, since 22s), standbys: ceph2, ceph3osd: 0 osds: 0 up, 0 indata:pools:   0 pools, 0 pgsobjects: 0 objects, 0 Busage:   0 B used, 0 B / 0 B availpgs: </span></span><br><span class="line">```  </span><br><span class="line">5 部署OSD节点</span><br><span class="line">```bash</span><br><span class="line">ceph-deploy osd create --data /dev/sdb ceph1</span><br><span class="line">ceph-deploy osd create --data /dev/sdc ceph1</span><br><span class="line">ceph-deploy osd create --data /dev/sdd ceph1</span><br><span class="line">ceph-deploy osd create --data /dev/sdb ceph2</span><br><span class="line">ceph-deploy osd create --data /dev/sdc ceph2</span><br><span class="line">ceph-deploy osd create --data /dev/sdd ceph2</span><br><span class="line">ceph-deploy osd create --data /dev/sdb ceph3</span><br><span class="line">ceph-deploy osd create --data /dev/sdc ceph3</span><br><span class="line">ceph-deploy osd create --data /dev/sdd ceph3</span><br></pre></td></tr></table></figure>
<p>创建成功后，查看是否正常</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@ceph1 cluster]<span class="comment"># ceph -scluster:id:     ea192428-05d2-437a-8cce-9d187de82dd5health: HEALTH_OKservices:mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 14m)mgr: ceph1(active, since 6m), standbys: ceph2, ceph3osd: 9 osds: 9 up (since 2m), 9 in (since 2m)data:pools:   0 pools, 0 pgsobjects: 0 objects, 0 Busage:   9.0 GiB used, 135 GiB / 144 GiB availpgs:  </span></span><br></pre></td></tr></table></figure>
<p>6 验证Ceph<br>创建存储池</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd pool create vdbench 10 10</span><br></pre></td></tr></table></figure>
<p>创建块设备</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd create image01 --size 200--pool vdbench --image-format 2 --image-feature layering</span><br><span class="line">rbd <span class="built_in">ls</span> --pool vdbench</span><br><span class="line">[root@ceph1 cluster]<span class="comment"># rbd create image01 --size 200 --pool  vdbench --image-format 2 --image-feature layering</span></span><br><span class="line">[root@ceph1 cluster]<span class="comment"># rbd ls --pool vdbench</span></span><br><span class="line">image01</span><br></pre></td></tr></table></figure>

<p>#PG 分 配 计 算<br>归置组(PG)的数量是由管理员在创建存储池的时候指定的，然后由 CRUSH 负责创建和使用，PG 的数量是 2 的 N 次方的倍数,每个 OSD 的 PG 不要超出 250 个 PG<br>Total PGs &#x3D; (Total_number_of_OSD * 100) &#x2F; max_replication_count<br>单个 pool 的 PG 计算如下：<br>有 100 个 osd，3 副本，5 个 pool<br>Total PGs &#x3D;100*100&#x2F;3&#x3D;3333<br>每个 pool 的 PG&#x3D;3333&#x2F;5&#x3D;512，那么创建 pool 的时候就指定 pg 为 512<br>客户端在读写对象时，需要提供的是对象标识和存储池名称<br>客户端需要在存储池中读写对象时，需要客户端将对象名称，对象名称的hash码，存储池中的PG数量和存储池名称作为输入信息提供给ceph，然后由CRUSH计算出PG的ID以及PG针对的主OSD即可读写OSD中的对象。<br>具体写操作如下：<br>1.APP向ceph客户端发送对某个对象的请求，此请求包含对象和存储池，然后ceph客户端对访问的对象做hash计算，并根据此hash值计算出对象所在的PG，完成对象从Pool至PG的映射。<br>APP 访问 pool ID 和 object ID （比如 pool &#x3D; pool1 and object-id &#x3D; “name1”）<br>ceph client 对 objectID 做哈希<br>ceph client 对该 hash 值取 PG 总数的模，得到 PG 编号(比如 32)<br>ceph client 对 pool ID 取 hash（比如 “pool1” &#x3D; 3）<br>ceph client 将 pool ID 和 PG ID 组合在一起(比如 3.23)得到 PG 的完整 ID。<br>2.然后客户端据 PG、CRUSH 运行图和归置组(placement rules)作为输入参数并再次进行计<br>算，并计算出对象所在的 PG 内的主 OSD ，从而完成对象从 PG 到 OSD 的映射。<br>3.客户端开始对主 OSD 进行读写请求(副本池 IO)，如果发生了写操作，会有 ceph 服务端完<br>成对象从主 OSD 到备份 OSD 的同步  </p>
<p>二.熟练 ceph 的用户管理及授权<br>客户端使用 session key 向 mon 请求所需要的服务，mon 向客户端提供一个 tiket，用于向实际处理数据的 OSD 等服务验证客户端身份，MON 和 OSD 共享同一个 secret.<br>ceph 用户需要拥有存储池访问权限，才能读取和写入数据<br>ceph 用户必须拥有执行权限才能使用 ceph 的管理命令<br>ceph 支持多种类型的用户，但可管理的用户都属于 client 类型<br>通过点号来分割用户类型和用户名，格式为 TYPE.ID，例如 client.admin。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@ceph-deploy:~<span class="comment"># cat /etc/ceph/ceph.client.admin.keyring </span></span><br><span class="line">[client.admin]</span><br><span class="line">        key = AQBnFaNj1iyBMBAAd+9hKWXaNw3GYxT9PEXvrQ==</span><br><span class="line">        caps mds = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">        caps mgr = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">        caps mon = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">        caps osd = <span class="string">&quot;allow *&quot;</span></span><br></pre></td></tr></table></figure>
<p>#列 出 指 定 用 户 信 息</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@ceph-deploy:~<span class="comment"># ceph auth get osd.10</span></span><br><span class="line">[osd.10]</span><br><span class="line">        key = AQB+I6Njk4KWNBAAL09FFayLKF44IgUQ1fjKYQ==</span><br><span class="line">        caps mgr = <span class="string">&quot;allow profile osd&quot;</span></span><br><span class="line">        caps mon = <span class="string">&quot;allow profile osd&quot;</span></span><br><span class="line">        caps osd = <span class="string">&quot;allow *&quot;</span></span><br><span class="line"></span><br><span class="line">exported keyring <span class="keyword">for</span> osd.10</span><br></pre></td></tr></table></figure>
<p>#: 列 出 用 户</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~$ ceph auth list</span><br><span class="line">mds.ceph-mgr1</span><br><span class="line">        key: AQAdRbFjOwBXIRAAUTdwElBzYPHW+4uFicFC7Q==</span><br><span class="line">        caps: [mds] allow</span><br><span class="line">        caps: [mon] allow profile mds</span><br><span class="line">        caps: [osd] allow rwx</span><br><span class="line">osd.0</span><br><span class="line">        key: AQC0IqNjbcgKIxAA+BCNpQeZiMujR+r+69Miig==</span><br><span class="line">        caps: [mgr] allow profile osd</span><br><span class="line">        caps: [mon] allow profile osd</span><br><span class="line">        caps: [osd] allow *</span><br></pre></td></tr></table></figure>
<p>#可以结合使用-o 文件名选项和 ceph auth list 将输出保存到某个文件。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth list -o 123.key</span><br></pre></td></tr></table></figure>

<p>#ceph auth add<br>此命令是添加用户的规范方法。它会创建用户、生成密钥，并添加所有指定的能力<br>添加认证 key：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth add client.tom mon <span class="string">&#x27;allow r&#x27;</span> osd <span class="string">&#x27;allow rwx pool=testpool2&#x27;</span></span><br><span class="line">added key <span class="keyword">for</span> client.tom</span><br></pre></td></tr></table></figure>

<p>##验证 key</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth get client.tom</span><br><span class="line">[client.tom]</span><br><span class="line">        key = AQD2vbJj8fIiDBAArtJBzQiuPy8nDWPSFVs0bw==</span><br><span class="line">        caps mon = <span class="string">&quot;allow r&quot;</span></span><br><span class="line">        caps osd = <span class="string">&quot;allow rwx pool=testpool2&quot;</span></span><br><span class="line">exported keyring <span class="keyword">for</span> client.tom</span><br></pre></td></tr></table></figure>
<p>##创建用户</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth get-or-create client.jack mon <span class="string">&#x27;allow r&#x27;</span> osd <span class="string">&#x27;allow rwx pool=testpool2&#x27;</span></span><br><span class="line">[client.jack]</span><br><span class="line">        key = AQC/vrJj5kenHhAAGeRJpY64feS4Dn6DD/R8VA==</span><br></pre></td></tr></table></figure>
<p>##再次创建用户</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth get-or-create client.jack mon <span class="string">&#x27;allow r&#x27;</span> osd <span class="string">&#x27;allow rwx pool=testpool2&#x27;</span></span><br><span class="line">[client.jack]</span><br><span class="line">        key = AQC/vrJj5kenHhAAGeRJpY64feS4Dn6DD/R8VA==</span><br></pre></td></tr></table></figure>
<p>#ceph auth get-or-create-key:<br>此命令是创建用户并返回用户密钥，对于只需要密钥的客户端(例如 libvrirt),此命令非常有用。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth get-or-create-key client.jack</span><br><span class="line">mon <span class="string">&#x27;allow r&#x27;</span> osd <span class="string">&#x27;allow rwx pool=mypool&#x27;</span></span><br><span class="line">AQAtr8dfi37XMhAADbHWEZ0shY1QZ5A8eBpeoQ==</span><br></pre></td></tr></table></figure>
<p>用户有 key 就显示没有就创建<br>#修 改 用 户 能 力</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth get client.jack</span><br><span class="line">[client.jack]</span><br><span class="line">        key = AQC/vrJj5kenHhAAGeRJpY64feS4Dn6DD/R8VA==</span><br><span class="line">        caps mon = <span class="string">&quot;allow r&quot;</span></span><br><span class="line">        caps osd = <span class="string">&quot;allow rwx pool=testpool2&quot;</span></span><br><span class="line">exported keyring <span class="keyword">for</span> client.jack</span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth caps client.jack mon <span class="string">&#x27;allow r&#x27;</span> osd <span class="string">&#x27;allow rw pool=testpool2&#x27;</span></span><br><span class="line">updated caps <span class="keyword">for</span> client.jack</span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth get client.jack</span><br><span class="line">[client.jack]</span><br><span class="line">        key = AQC/vrJj5kenHhAAGeRJpY64feS4Dn6DD/R8VA==</span><br><span class="line">        caps mon = <span class="string">&quot;allow r&quot;</span></span><br><span class="line">        caps osd = <span class="string">&quot;allow rw pool=testpool2&quot;</span></span><br><span class="line">exported keyring <span class="keyword">for</span> client.jack</span><br></pre></td></tr></table></figure>

<p>#删 除 用 户 </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth del client.tom</span><br><span class="line">updated</span><br></pre></td></tr></table></figure>
<p>#导出 keyring 至指定文件</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth get client.user1 -o</span><br><span class="line">ceph.client.user1.keyring</span><br><span class="line">exported keyring <span class="keyword">for</span> client.user1</span><br></pre></td></tr></table></figure>
<p>#验证指定用户的 keyring 文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ <span class="built_in">cat</span> ceph.client.user1.keyring</span><br><span class="line">[client.user1]</span><br><span class="line">key = AQAUUchfjpMqGRAARV6h0ofdDEneuaRnxuHjoQ==</span><br><span class="line">caps mon = <span class="string">&quot;allow r&quot;</span></span><br><span class="line">caps osd = <span class="string">&quot;allow * pool=mypool&quot;</span></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth del client.user1 <span class="comment">#演示误删除用户</span></span><br><span class="line">Updated</span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth get client.user1 <span class="comment">#确认用户被删除</span></span><br><span class="line">Error ENOENT: failed to find client.user1 <span class="keyword">in</span> keyring</span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth import -i</span><br><span class="line">ceph.client.user1.keyring <span class="comment">#导入用户</span></span><br><span class="line">imported keyring</span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth get client.user1 <span class="comment">#验证已恢复用户</span></span><br><span class="line">exported keyring <span class="keyword">for</span> client.user1</span><br></pre></td></tr></table></figure>
<p>#将多 用 户 导 出 至 秘 钥 环 ：<br>#创建 keyring 文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ceph-authtool --create-keyring ceph.client.user.keyring <span class="comment">#创建空的 keyring 文件</span></span><br><span class="line">creating ceph.client.user.keyring</span><br></pre></td></tr></table></figure>
<p>#把指定的 admin 用户的 keyring 文件内容导入到 user 用户的 keyring 文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="variable">$ceph</span>-authtool ./ceph.client.user.keyring</span><br><span class="line">--import-keyring ./ceph.client.admin.keyring</span><br><span class="line">importing contents of ./ceph.client.admin.keyring into ./ceph.client.user.keyring</span><br></pre></td></tr></table></figure>
<p>#验证 keyring 文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph-authtool -l ./ceph.client.user.keyring</span><br><span class="line">[client.admin]</span><br><span class="line">key = AQAGDKJfQk/dAxAA3Y+9xoE/p8in6QjoHeXmeg==</span><br><span class="line">caps mds = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">caps mgr = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">caps mon = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">caps osd = <span class="string">&quot;allow *&quot;</span></span><br></pre></td></tr></table></figure>
<p>#再导入一个其他用户的 keyring：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph-authtool ./ceph.client.user.keyring</span><br><span class="line">--import-keyring ./ceph.client.user1.keyring</span><br><span class="line">importing contents of ./ceph.client.user1.keyring into ./ceph.client.user.keyring</span><br></pre></td></tr></table></figure>
<p>#再次验证 keyring 文件是否包含多个用户的认证信息：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph-authtool -l ./ceph.client.user.keyring</span><br><span class="line">[client.admin]</span><br><span class="line">key = AQAGDKJfQk/dAxAA3Y+9xoE/p8in6QjoHeXmeg==</span><br><span class="line">caps mds = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">caps mgr = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">caps mon = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">caps osd = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">[client.user1]</span><br><span class="line">key = AQAUUchfjpMqGRAARV6h0ofdDEneuaRnxuHjoQ==</span><br><span class="line">caps mon = <span class="string">&quot;allow r&quot;</span></span><br><span class="line">caps osd = <span class="string">&quot;allow * pool=mypool&quot;</span></span><br></pre></td></tr></table></figure>
<p>三. 使用普通客户挂载块存储<br>#创建存储池：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$ceph</span> osd pool create rbd-data1 32 32</span><br><span class="line"></span><br><span class="line"><span class="comment">#存储池启用 rbd</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$ceph</span> osd pool application <span class="built_in">enable</span> rbd-data1 rbd</span><br><span class="line"><span class="comment">#初始化 rbd</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$rbd</span> pool init -p rbd-data1</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建两个镜像：</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$rbd</span> create data-img1 --size 3G --pool rbd-data1 --image-format 2 --image-feature layering</span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$rbd</span> create data-img2 --size 5G --pool rbd-data1 --image-format 2 --image-feature layering</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#列出镜像信息</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$rbd</span> <span class="built_in">ls</span> --pool rbd-data1</span><br><span class="line"><span class="comment">#以 json 格 式 显 示 镜 像 信 息</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$rbd</span> <span class="built_in">ls</span> --pool rbd-data1 -l --format json --pretty-format</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建普通账户</span></span><br><span class="line">ceph auth add client.shijie mon <span class="string">&#x27;allow r&#x27;</span> osd <span class="string">&#x27;allow rwx pool=rbd-data1&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#验证用户信息</span></span><br><span class="line">ceph auth get client.shijie</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建用 keyring 文件</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph-authtool --create-keyring ceph.client.shijie.keyring</span><br><span class="line"></span><br><span class="line"><span class="comment">#导出用户 keyring</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth get client.shijie -o ceph.client.shijie.keyring </span><br><span class="line"></span><br><span class="line"><span class="comment">#验证指定用户的 keyring 文件</span></span><br><span class="line"></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$cat</span> ceph.client.shijie.keyring </span><br><span class="line">  </span><br><span class="line"><span class="comment">#同 步 普 通 用 户 认 证 文 件</span></span><br><span class="line"></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ scp ceph.client.shijie.keyring root@172.31.6.109:/etc/ceph/</span><br><span class="line"></span><br><span class="line"><span class="comment">#管理端验证镜像状态</span></span><br><span class="line"></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$rbd</span> <span class="built_in">ls</span> -p rbd-data1 -l</span><br><span class="line"></span><br><span class="line"><span class="comment">#在client上操作</span></span><br><span class="line"><span class="comment">#映射 rbd</span></span><br><span class="line"></span><br><span class="line">root@ceph-node4:/<span class="comment"># rbd -p rbd-data1 map data-img1</span></span><br><span class="line">/dev/rbd0</span><br><span class="line">root@ceph-node4:/<span class="comment"># lsblk</span></span><br><span class="line">rbd0</span><br><span class="line">root@ceph-node4:/<span class="comment"># mkfs.xfs /dev/rbd0</span></span><br><span class="line">root@ceph-node4:/<span class="comment"># mount /dev/rbd0 /data</span></span><br><span class="line">root@ceph-node4:/<span class="comment"># docker run -it -d -p 3306:3306 -e MYSQL_ROOT_PASSWORD=&quot;12345678&quot; -v /data:/var/lib/mysql mysql:5.6.46</span></span><br><span class="line">48374db8541a7fa375c00611373051ef21690e89adfd4c156b3f6ffb0dbe95a2</span><br><span class="line">root@ceph-node4:/data<span class="comment"># ls</span></span><br><span class="line">ibdata1  ib_logfile0  ib_logfile1  mysql  performance_schema  <span class="built_in">test</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#在client上操作</span></span><br><span class="line"><span class="comment">#使用普通用户映射 rbd</span></span><br><span class="line"></span><br><span class="line">root@ceph-node4:/etc/ceph<span class="comment"># rbd --user shijie -p rbd-data1 map data-img2</span></span><br><span class="line">/dev/rbd1</span><br><span class="line"><span class="comment">#格式化</span></span><br><span class="line">root@ceph-node4:/etc/ceph<span class="comment">#mkfs.ext4 /dev/rbd0</span></span><br><span class="line">root@ceph-node4:/etc/ceph<span class="comment"># mkdir /data1</span></span><br><span class="line">root@ceph-node4:/etc/ceph<span class="comment"># mount /dev/rbd1 /data1</span></span><br><span class="line">root@ceph-node4:/etc/ceph<span class="comment"># cp /var/log/auth.log /data1</span></span><br><span class="line">root@ceph-node4:/etc/ceph<span class="comment"># cd /data1</span></span><br><span class="line">root@ceph-node4:/data1<span class="comment"># ls</span></span><br><span class="line">auth.log  lost+found</span><br><span class="line">四. 使用普通用户挂载 cephfs（可以通过 secret 或者 secretfile 的形式多主机同时挂载）</span><br><span class="line">Ceph FS 需要运行 Meta Data Services(MDS)服务，其守护进程为 ceph-mds，ceph-mds进程管理与 cephFS 上存储的文件相关的元数据，并协调对 ceph 存储集群的访问。</span><br><span class="line"></span><br><span class="line"><span class="comment">#部署MDS服务:</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ apt-cache madison ceph-mds</span><br><span class="line">  ceph-mds | 16.2.10-1bionic | https://mirrors.tuna.tsinghua.edu.cn/ceph/debian-pacific bionic/main amd64 Packages</span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ apt install ceph-mds=16.2.10-1bionic</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建CephFS meta data和data存储池</span></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph osd pool create cephfs-metadata 32 32</span><br><span class="line">pool <span class="string">&#x27;cephfs-metadata&#x27;</span> created <span class="comment">#保存 metadata 的 pool</span></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph osd pool create cephfs-data 64 64</span><br><span class="line">pool <span class="string">&#x27;cephfs-data&#x27;</span> created <span class="comment">#保存数据的 pool</span></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph fs new mycephfs cephfs-metadata</span><br><span class="line">cephfs-data</span><br><span class="line">new fs with metadata pool 7 and data pool 8</span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph fs <span class="built_in">ls</span></span><br><span class="line">name: mycephfs, metadata pool: cephfs-metadata, data pools: [cephfs-data ]</span><br><span class="line"><span class="comment">#查看指定 cephFS 状态</span></span><br><span class="line"></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph fs status mycephfs</span><br><span class="line">mycephfs - 0 clients</span><br><span class="line">RANK  STATE      MDS        ACTIVITY     DNS    INOS   DIRS   CAPS  </span><br><span class="line"> 0    active  ceph-mgr1  Reqs:    0 /s    10     13     12      0   </span><br><span class="line">      POOL         TYPE     USED  AVAIL  </span><br><span class="line">cephfs-metadata  metadata   146k  18.9T  </span><br><span class="line">  cephfs-data      data       0   18.9T  </span><br><span class="line">MDS version: ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)</span><br><span class="line"><span class="comment">#验证cephFS服务状态</span></span><br><span class="line"></span><br><span class="line">cephadmin@ceph-deploy:~$ ceph mds <span class="built_in">stat</span></span><br><span class="line">mycephfs:1 &#123;0=ceph-mgr1=up:active&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建客户端账户</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$ceph</span> auth add client.yanyan mon <span class="string">&#x27;allow r&#x27;</span> mds <span class="string">&#x27;allow rw&#x27;</span> osd <span class="string">&#x27;allow rwx pool=cephfs-data&#x27;</span></span><br><span class="line"><span class="comment">#验证账户</span></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth get client.yanyan</span><br><span class="line">exported keyring <span class="keyword">for</span> client.yanyan</span><br><span class="line">[client.yanyan]</span><br><span class="line">key = AQCxpdhfjQt1OxAAGe0mqTMveNu2ZMEem3tb0g==</span><br><span class="line">caps mds = <span class="string">&quot;allow rw&quot;</span></span><br><span class="line">caps mon = <span class="string">&quot;allow r&quot;</span></span><br><span class="line">caps osd = <span class="string">&quot;allow rwx pool=cephfs-data&quot;</span></span><br><span class="line"><span class="comment">#创建keyring 文件</span></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth get client.yanyan -o</span><br><span class="line">ceph.client.yanyan.keyring</span><br><span class="line">exported keyring <span class="keyword">for</span> client.yanyan</span><br><span class="line"><span class="comment">#创建 key 文件：</span></span><br><span class="line"></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth print-key client.yanyan &gt; yanyan.key</span><br><span class="line"><span class="comment">#验证用户的 keyring 文件</span></span><br><span class="line"></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ <span class="built_in">cat</span> ceph.client.yanyan.keyring</span><br><span class="line">[client.yanyan]</span><br><span class="line">key = AQCxpdhfjQt1OxAAGe0mqTMveNu2ZMEem3tb0g==</span><br><span class="line">caps mds = <span class="string">&quot;allow rw&quot;</span></span><br><span class="line">caps mon = <span class="string">&quot;allow r&quot;</span></span><br><span class="line">caps osd = <span class="string">&quot;allow rwx pool=cephfs-data&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#同步客户端认证文件 ：</span></span><br><span class="line"></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ scp ceph.conf ceph.client.yanyan.keyring</span><br><span class="line">yanyan.key root@172.31.6.109:/etc/ceph/</span><br><span class="line"></span><br><span class="line"><span class="comment">#客户端验证权限</span></span><br><span class="line"></span><br><span class="line">root@ceph-node4:~<span class="comment"># ceph --user yanyan -s</span></span><br><span class="line">  cluster:</span><br><span class="line">    <span class="built_in">id</span>:     7c088d6f-06b0-4584-b23f-c0f150af51d4</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph-mon1,ceph-mon2,ceph-mon3 (age 24m)</span><br><span class="line">    mgr: ceph-mgr1(active, since 65m)</span><br><span class="line">    mds: 1/1 daemons up</span><br><span class="line">    osd: 16 osds: 16 up (since 24m), 16 <span class="keyword">in</span> (since 13d)</span><br><span class="line">    rgw: 1 daemon active (1 hosts, 1 zones)</span><br><span class="line">  data:</span><br><span class="line">    volumes: 1/1 healthy</span><br><span class="line">    pools:   10 pools, 329 pgs</span><br><span class="line">    objects: 296 objects, 218 MiB</span><br><span class="line">    usage:   948 MiB used, 60 TiB / 60 TiB avail</span><br><span class="line">    pgs:     329 active+clean</span><br><span class="line"></span><br><span class="line"><span class="comment">#客户端通过 key 文件挂载:</span></span><br><span class="line"></span><br><span class="line"> root@ceph-node4:~<span class="comment">#mkdir /data</span></span><br><span class="line"> root@ceph-node4:/etc/ceph<span class="comment"># mount -t ceph 172.31.6.101:6789,172.31.6.102:6789,172.31.6.103:6789:/ /data -o name=yanyan,secretfile=/etc/ceph/yanyan.key</span></span><br><span class="line">root@ceph-node4:/etc/ceph<span class="comment"># df -h</span></span><br><span class="line">Filesystem                                               Size  Used Avail Use% Mounted on</span><br><span class="line">udev                                                     955M     0  955M   0% /dev</span><br><span class="line">172.31.6.101:6789,172.31.6.102:6789,172.31.6.103:6789:/   19T     0   19T   0% /data</span><br><span class="line"></span><br><span class="line"><span class="comment">#客户端通过key挂载</span></span><br><span class="line"></span><br><span class="line">root@ceph-node3:~<span class="comment"># mkdir /data</span></span><br><span class="line">root@ceph-node3:~<span class="comment"># mount -t ceph 172.31.6.101:6789,172.31.6.102:6789,172.31.6.103:6789:/ /data -o name=yanyan,secret=AQAfebVjaIPgABAAzkW4ChX2Qm2Sha/5twdxPA==</span></span><br><span class="line">root@ceph-node3:~<span class="comment"># df -h</span></span><br><span class="line">Filesystem                                               Size  Used Avail Use% Mounted on</span><br><span class="line">udev                                                     955M     0  955M   0% /dev</span><br><span class="line">172.31.6.101:6789,172.31.6.102:6789,172.31.6.103:6789:/   19T     0   19T   0% /data</span><br><span class="line">root@ceph-node3:~<span class="comment"># cp /var/log/auth.log /data</span></span><br><span class="line">root@ceph-node3:~<span class="comment"># cd /data</span></span><br><span class="line">root@ceph-node3:/data<span class="comment"># ls</span></span><br><span class="line">auth.log</span><br><span class="line">root@ceph-node3:/data<span class="comment"># vim auth.log </span></span><br><span class="line">root@ceph-node3:/data<span class="comment"># echo &quot;12345678&quot; &gt;&gt; auth.log </span></span><br><span class="line">root@ceph-node3:/data<span class="comment"># echo &quot;testlog&quot; &gt;&gt; auth.log</span></span><br><span class="line"><span class="comment">#在node4客户端上查看cephfs挂载点/data 目录下内容，已经同步</span></span><br><span class="line"></span><br><span class="line">root@ceph-node4:/<span class="comment"># tail -f /data/auth.log </span></span><br><span class="line">Jan  4 22:25:01 ceph-node3 CRON[4365]: pam_unix(cron:session): session closed <span class="keyword">for</span> user root</span><br><span class="line">12345678</span><br><span class="line">testlog</span><br><span class="line"><span class="comment">#客户端内核加载 ceph.ko 模块挂载 cephfs 文件系统</span></span><br><span class="line"></span><br><span class="line">root@ceph-node4:/<span class="comment"># lsmod|grep ceph</span></span><br><span class="line">ceph                  380928  1</span><br><span class="line">libceph               315392  1 ceph</span><br><span class="line">fscache                65536  1 ceph</span><br><span class="line">libcrc32c              16384  5 nf_conntrack,nf_nat,xfs,raid456,libceph</span><br><span class="line">root@ceph-node4:/<span class="comment"># modinfo ceph</span></span><br><span class="line">filename:       /lib/modules/4.15.0-130-generic/kernel/fs/ceph/ceph.ko</span><br><span class="line">license:        GPL</span><br><span class="line">description:    Ceph filesystem <span class="keyword">for</span> Linux</span><br><span class="line">author:         Patience Warnick &lt;patience@newdream.net&gt;</span><br><span class="line">author:         Yehuda Sadeh &lt;yehuda@hq.newdream.net&gt;</span><br><span class="line">author:         Sage Weil &lt;sage@newdream.net&gt;</span><br><span class="line"><span class="built_in">alias</span>:          fs-ceph</span><br><span class="line">srcversion:     CB79D9E4790452C6A392A1C</span><br><span class="line">depends:        libceph,fscache</span><br><span class="line">retpoline:      Y</span><br><span class="line">intree:         Y</span><br><span class="line">name:           ceph</span><br><span class="line">vermagic:       4.15.0-130-generic SMP mod_unload </span><br><span class="line">signat:         PKCS<span class="comment">#7</span></span><br><span class="line">signer:         </span><br><span class="line">sig_key:        </span><br><span class="line">sig_hashalgo:   md4</span><br></pre></td></tr></table></figure>

<p>五.实现 MDS 服务的多主一备高可用架构</p>
<p>#当前mds服务器状态</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~$ ceph mds <span class="built_in">stat</span></span><br><span class="line">mycephfs:1 &#123;0=ceph-mgr1=up:active&#125;</span><br><span class="line"><span class="comment">#添加MDS服务器</span></span><br><span class="line">将 ceph-mgr2 和 ceph-mon2 和 ceph-mon3 作为 mds 服务角色添加至 ceph 集群，最后实两主两备的 mds 高可用和高性能结构。</span><br><span class="line"><span class="comment">#mds 服务器安装 ceph-mds 服务</span></span><br><span class="line"></span><br><span class="line">[root@ceph-mgr2 ~]<span class="comment"># apt install ceph-mds -y</span></span><br><span class="line">[root@ceph-mon2 ~]<span class="comment"># apt install ceph-mds -y</span></span><br><span class="line">[root@ceph-mon3 ~]<span class="comment"># apt install ceph-mds -y</span></span><br><span class="line"><span class="comment">#添加 mds 服务器</span></span><br><span class="line"></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph-deploy mds create ceph-mgr2</span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph-deploy mds create ceph-mon2</span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph-deploy mds create ceph-mon3</span><br></pre></td></tr></table></figure>
<p>#验证 mds 服务器当前状态：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph mds <span class="built_in">stat</span></span><br><span class="line">mycephfs-1/1/1 up &#123;0=ceph-mgr1=up:active&#125;, 3 up:standby</span><br></pre></td></tr></table></figure>
<p>#验证 ceph集群当前状态<br>当前处于激活状态的 mds 服务器有一台，处于备份状态的 mds 服务器有三台。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph fs status</span><br><span class="line">mycephfs - 0 clients</span><br><span class="line">RANK  STATE      MDS        ACTIVITY     DNS    INOS   DIRS   CAPS  </span><br><span class="line"> 0    active  ceph-mgr1  Reqs:    0 /s    16     14     12      0   </span><br><span class="line">      POOL         TYPE     USED  AVAIL  </span><br><span class="line">cephfs-metadata  metadata   282k  18.9T  </span><br><span class="line">  cephfs-data      data     564k  18.9T  </span><br><span class="line">STANDBY MDS  </span><br></pre></td></tr></table></figure>
<p>#当前的文件系统状态:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph fs get mycephfs</span><br><span class="line">Filesystem <span class="string">&#x27;mycephfs&#x27;</span> (1)</span><br><span class="line">fs_name        mycephfs</span><br><span class="line">epoch        28</span><br><span class="line">flags        12</span><br><span class="line">created        2023-01-01T19:09:29.258956+0800</span><br><span class="line">modified        2023-01-05T14:58:00.369468+0800</span><br><span class="line">tableserver        0</span><br><span class="line">root        0</span><br><span class="line">session_timeout        60</span><br><span class="line">session_autoclose        300</span><br><span class="line">max_file_size        1099511627776</span><br><span class="line">required_client_features        &#123;&#125;</span><br><span class="line">last_failure        0</span><br><span class="line">last_failure_osd_epoch        406</span><br><span class="line">compat        compat=&#123;&#125;,rocompat=&#123;&#125;,incompat=&#123;1=base v0.20,2=client writeable ranges,3=default file layouts on <span class="built_in">dirs</span>,4=<span class="built_in">dir</span> inode <span class="keyword">in</span> separate object,5=mds uses versioned encoding,6=dirfrag is stored <span class="keyword">in</span> omap,7=mds uses inline data,8=no anchor table,9=file layout v2,10=snaprealm v2&#125;</span><br><span class="line">max_mds        1</span><br><span class="line"><span class="keyword">in</span>        0</span><br><span class="line">up        &#123;0=144106&#125;</span><br><span class="line">failed        </span><br><span class="line">damaged        </span><br><span class="line">stopped        </span><br><span class="line">data_pools        [8]</span><br><span class="line">metadata_pool        7</span><br><span class="line">inline_data        disabled</span><br><span class="line">balancer        </span><br><span class="line">standby_count_wanted        1</span><br><span class="line">[mds.ceph-mgr1&#123;0:144106&#125; state up:active <span class="built_in">seq</span> 27 addr [v2:172.31.6.104:6800/428364709,v1:172.31.6.104:6801/428364709] compat &#123;c=[1],r=[1],i=[7ff]&#125;]</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置处于激活状态mds的数量</span></span><br><span class="line">目前有四个 mds 服务器，但是有一个主三个备，可以优化一下部署架构，设置为为两主两备。</span><br><span class="line"></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph fs <span class="built_in">set</span> mycephfs max_mds 2</span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph fs status</span><br><span class="line">mycephfs - 0 clients</span><br><span class="line">RANK  STATE      MDS        ACTIVITY     DNS    INOS   DIRS   CAPS  </span><br><span class="line"> 0    active  ceph-mgr1  Reqs:    0 /s    16     14     12      0   </span><br><span class="line"> 1    active  ceph-mon2  Reqs:    0 /s    10     13     11      0   </span><br><span class="line">      POOL         TYPE     USED  AVAIL  </span><br><span class="line">cephfs-metadata  metadata   354k  18.9T  </span><br><span class="line">  cephfs-data      data     564k  18.9T  </span><br><span class="line">STANDBY MDS  </span><br><span class="line"> ceph-mgr2   </span><br><span class="line"> ceph-mon3   </span><br><span class="line">MDS version: ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)</span><br></pre></td></tr></table></figure>


<p>安装完成后ceph -s提示：“mon is allowing insecure global_id reclaim”。</p>
<p>解决方案：禁用不安全模式</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph config <span class="built_in">set</span> mon auth_allow_insecure_global_id_reclaim <span class="literal">false</span></span><br></pre></td></tr></table></figure>
<p>输入完成后需要等待5-10秒。</p>
<p>PG数量计算公式：</p>
<p>（总OSD数量*100）&#x2F; 副本数（复制分数，默认为3）&#x3D; PG数量</p>
<p>一般情况下结果取2的N次方，尽量先设置小点，后期可以增大。如果先设置的比较大的话后期减小风险较高，所以尽量取小的2的N次方结果。<br>建一个存储池，要想使用ceph的存储功能，必须先创建存储池</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd pool create rbd 128 128 </span><br></pre></td></tr></table></figure>

<p>初始化存储池</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd pool init -p rbd</span><br></pre></td></tr></table></figure>
<p>1.设置存储池副本数</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@controller-1 ~]<span class="comment"># ceph osd pool get rbd size</span></span><br><span class="line">size: 2</span><br><span class="line">[root@controller-1 ~]<span class="comment"># ceph osd pool set rbd size 1</span></span><br><span class="line"><span class="built_in">set</span> pool 1 size to 1</span><br></pre></td></tr></table></figure>
<p>升级client的虚拟机内核到5版本</p>
<p>修改client下文件权限</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">chmod</span> +r /etc/ceph/ceph.client.admin.keyring</span><br><span class="line">client节点创建设备镜像，单位是M</span><br><span class="line">rbd create --size 4096 --pool rbd img</span><br><span class="line">client节点映射镜像到主机：</span><br><span class="line">rbd map img --name client.admin</span><br><span class="line">client节点格式化块设备</span><br><span class="line">mkfs.ext4 -m 0 /dev/rbd/rbd/foo </span><br><span class="line">client节点挂载mount块设备</span><br><span class="line"><span class="built_in">mkdir</span> /mnt/ceph-block-device</span><br><span class="line">mount /dev/rbd/rbd/foo /mnt/ceph-block-device -o discard</span><br><span class="line">创建文件系统时报不支持EC数据池问题</span><br><span class="line">当拥有一个ceph_metadata元数据副本类型池和一个ceph_data数据EC类型池时，建立文件系统：</span><br><span class="line">ceph fs new cephfs cephfs_metadata cephfs_data</span><br><span class="line">N版可能会报：</span><br><span class="line">Error EINVAL: pool <span class="string">&#x27;cephfs_data&#x27;</span> (<span class="built_in">id</span> <span class="string">&#x27;11&#x27;</span>) is an erasure-coded pool. Use of an EC pool <span class="keyword">for</span> the default data pool is discouraged; see the online CephFS documentation <span class="keyword">for</span> more information. Use --force to override.</span><br><span class="line">加上—force后：</span><br><span class="line">ceph fs new cephfs cephfs_metadata cephfs_data --force</span><br><span class="line"></span><br><span class="line">也可能会报：</span><br><span class="line"></span><br><span class="line">Error EINVAL: pool <span class="string">&#x27;cephfs_data&#x27;</span> (<span class="built_in">id</span> <span class="string">&#x27;11&#x27;</span>) is an erasure-coded pool, with no overwrite support</span><br><span class="line"></span><br><span class="line">这是需要手动设置ceph_data池 allow_ec_overwrites=<span class="literal">true</span></span><br><span class="line">ceph osd pool <span class="built_in">set</span> cephfs_data allow_ec_overwrites <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">再执行</span><br><span class="line">ceph fs new cephfs cephfs_metadata cephfs_data –forc</span><br><span class="line">就可以创建文件系统成功。</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>ceph安装以及测试</title>
    <url>/ceph%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E6%B5%8B%E8%AF%95/index.html</url>
    <content><![CDATA[]]></content>
  </entry>
</search>
