<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>spark null 值处理</title>
    <url>/2023/08/16/spark-null-%E5%80%BC%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<p> scala spark na null处理 | 分子美食家的博客                  </p>
<h1 id="分子美食家的博客"><a href="#分子美食家的博客" class="headerlink" title="分子美食家的博客"></a><a href="/">分子美食家的博客</a></h1><h2 id="野生工程师的专栏"><a href="#野生工程师的专栏" class="headerlink" title="野生工程师的专栏"></a><a href="/">野生工程师的专栏</a></h2><p><a href="/">Home</a> <a href="/archives">Archives</a></p>
<p><a href="/atom.xml" title="RSS Feed"></a></p>
<p></p>
<p><a href="/2022/05/14/scala-spark-na-null%E5%A4%84%E7%90%86/">2022-05-14</a></p>
<h1 id="scala-spark-na-null处理"><a href="#scala-spark-na-null处理" class="headerlink" title="scala spark na null处理"></a>scala spark na null处理</h1><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parseCheck</span></span>(s: <span class="type">String</span>,min:<span class="type">Double</span>,max:<span class="type">Double</span>): <span class="type">Option</span>[<span class="type">Double</span>] = {</span><br><span class="line">      <span class="keyword">try</span> {</span><br><span class="line">        <span class="keyword">val</span> va = s.toDouble</span><br><span class="line">        <span class="keyword">if</span> (va &lt; min || va &gt; max) {</span><br><span class="line">          <span class="type">Some</span>(va)</span><br><span class="line">        }<span class="keyword">else</span> {</span><br><span class="line">          <span class="type">Some</span>(<span class="type">Double</span>.<span class="type">NaN</span>)</span><br><span class="line">        }</span><br><span class="line">      }</span><br><span class="line">      <span class="keyword">catch</span> { <span class="keyword">case</span> _ =&gt; <span class="type">Some</span>(<span class="type">Double</span>.<span class="type">NaN</span>)</span><br><span class="line">      }</span><br><span class="line">    }</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parseDouble</span></span>(s: <span class="type">String</span>): <span class="type">Option</span>[<span class="type">Double</span>] = <span class="keyword">try</span> { <span class="type">Some</span>(s.toDouble) } <span class="keyword">catch</span> { <span class="keyword">case</span> _ =&gt; <span class="type">Some</span>(<span class="type">Double</span>.<span class="type">NaN</span>) }</span><br></pre></td></tr></tbody></table>

<p>Share</p>
<p>[<strong>Older</strong></p>
<p>scala-spark-dataframe—groupby基本操作</p>
<p>](&#x2F;2022&#x2F;05&#x2F;07&#x2F;scala-spark-dataframe%E2%80%94groupby%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C&#x2F;)</p>
<h3 id="Archives"><a href="#Archives" class="headerlink" title="Archives"></a>Archives</h3><ul>
<li><a href="/archives/2022/05/">May 2022</a></li>
</ul>
<h3 id="Recent-Posts"><a href="#Recent-Posts" class="headerlink" title="Recent Posts"></a>Recent Posts</h3><ul>
<li><a href="/2022/05/14/scala-spark-na-null%E5%A4%84%E7%90%86/">scala spark na null处理</a></li>
<li><a href="/2022/05/07/scala-spark-dataframe%E2%80%94groupby%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/">scala-spark-dataframe—groupby基本操作</a></li>
<li><a href="/2022/05/04/fedora-k8s/">fedora-k8s</a></li>
<li><a href="/2022/05/04/openwrt%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/">openwrt安装配置</a></li>
<li><a href="/2022/05/04/ubuntu-supervisor%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/">ubuntu supervisor进程管理</a></li>
</ul>
<p>© 2022 andrew<br>Powered by <a href="https://hexo.io/">Hexo</a></p>
<p><a href="/">Home</a> <a href="/archives">Archives</a></p>
<p>%</p>
]]></content>
  </entry>
  <entry>
    <title>spark group by 操作</title>
    <url>/2023/08/16/spark-group-by-%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<p>scala-spark-dataframe—groupby基本操作<br>[andrew@hadoop102 bin]$ .&#x2F;spark-shell<br>2022-05-07 20:35:13,392 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>Setting default log level to “WARN”.<br>To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).<br>Spark context Web UI available at <a href="http://hadoop102:4040/">http://hadoop102:4040</a><br>Spark context available as ‘sc’ (master &#x3D; local[*], app id &#x3D; local-1651926921962).<br>Spark session available as ‘spark’.<br>Welcome to<br>      ____              __<br>     &#x2F; <strong>&#x2F;</strong>  ___ _<em><em><strong>&#x2F; &#x2F;</strong><br>    <em>\ / _ / _ &#96;&#x2F; _<em>&#x2F;  ‘</em>&#x2F;<br>   &#x2F;_</em></em>&#x2F; .__&#x2F;_,</em>&#x2F;<em>&#x2F; &#x2F;</em>&#x2F;_\   version 3.0.0<br>      &#x2F;_&#x2F;</p>
<p>Using Scala version 2.12.10 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_212)<br>Type in expressions to have them evaluated.<br>Type :help for more information.</p>
<p>scala&gt; val df &#x3D; spark.createDataset(Seq(<br>     |   (“aaa”,1,2),(“bbb”,3,4),(“ccc”,3,5),(“bbb”,4, 6))   ).toDF(“key1”,”key2”,”key3”)<br>df: org.apache.spark.sql.DataFrame &#x3D; [key1: string, key2: int … 1 more field]</p>
<p>scala&gt; df.show()<br>+—-+—-+—-+<br>|key1|key2|key3|<br>+—-+—-+—-+<br>| aaa|   1|   2|<br>| bbb|   3|   4|<br>| ccc|   3|   5|<br>| bbb|   4|   6|<br>+—-+—-+—-+</p>
<p>scala&gt; df.printSchema()<br>root<br> |– key1: string (nullable &#x3D; true)<br> |– key2: integer (nullable &#x3D; false)<br> |– key3: integer (nullable &#x3D; false)</p>
<p>scala&gt; df.groupBy(“key1”).count.show<br>+—-+—–+<br>|key1|count|<br>+—-+—–+<br>| ccc|    1|<br>| aaa|    1|<br>| bbb|    2|<br>+—-+—–+</p>
<p>scala&gt; df.select(“key1”).distinct.show<br>+—-+<br>|key1|<br>+—-+<br>| ccc|<br>| aaa|<br>| bbb|<br>+—-+</p>
<p>scala&gt; df.select(“key1”).distinct.count<br>res4: Long &#x3D; 3</p>
<p>scala&gt; f.groupBy(“key1”).count.sort(“key1”).show<br><console>:24: error: not found: value f<br>       f.groupBy(“key1”).count.sort(“key1”).show<br>       ^</p>
<p>scala&gt; df.groupBy(“key1”).count.sort(“key1”).show<br>+—-+—–+<br>|key1|count|<br>+—-+—–+<br>| aaa|    1|<br>| bbb|    2|<br>| ccc|    1|<br>+—-+—–+</p>
<p>scala&gt; df.groupBy(“key1”).count.sort($”count”.desc).show<br>+—-+—–+<br>|key1|count|<br>+—-+—–+<br>| bbb|    2|<br>| ccc|    1|<br>| aaa|    1|<br>+—-+—–+</p>
<p>scala&gt; df.groupBy(“key1”).count.withColumnRenamed(“count”, “cnt”).sort($”cnt”.desc).show<br>+—-+—+<br>|key1|cnt|<br>+—-+—+<br>| bbb|  2|<br>| aaa|  1|<br>| ccc|  1|<br>+—-+—+</p>
<p>scala&gt; df.groupBy(“key1”).agg(count(“key1”).as(“cnt”)).show<br>+—-+—+<br>|key1|cnt|<br>+—-+—+<br>| ccc|  1|<br>| aaa|  1|<br>| bbb|  2|<br>+—-+—+</p>
<p>scala&gt;  df.groupBy(“key1”).agg(count(“key1”), max(“key2”), avg(“key3”)).show<br>+—-+———–+———+———+<br>|key1|count(key1)|max(key2)|avg(key3)|<br>+—-+———–+———+———+<br>| ccc|          1|        3|      5.0|<br>| aaa|          1|        1|      2.0|<br>| bbb|          2|        4|      5.0|<br>+—-+———–+———+———+</p>
<p>scala&gt; f.groupBy(“key1”)<br><console>:24: error: not found: value f<br>       f.groupBy(“key1”)<br>       ^</p>
<p>scala&gt;        df.groupBy(“key1”).agg(“key1”-&gt;”count”, “key2”-&gt;”max”, “key3”-&gt;”avg”).show<br>+—-+———–+———+———+<br>|key1|count(key1)|max(key2)|avg(key3)|<br>+—-+———–+———+———+<br>| ccc|          1|        3|      5.0|<br>| aaa|          1|        1|      2.0|<br>| bbb|          2|        4|      5.0|<br>+—-+———–+———+———+</p>
<p>scala&gt; df.groupBy(“key1”).agg(Map((“key1”,”count”), (“key2”,”max”), (“key3”,”avg”))).show<br>+—-+———–+———+———+<br>|key1|count(key1)|max(key2)|avg(key3)|<br>+—-+———–+———+———+<br>| ccc|          1|        3|      5.0|<br>| aaa|          1|        1|      2.0|<br>| bbb|          2|        4|      5.0|<br>+—-+———–+———+———+</p>
<p>scala&gt; df.groupBy(“key1”).agg(count(“key1”).as(“cnt”), max(“key2”).as(“max_key2”), avg(“key3”).as(“avg_key3”)).sort($”cnt”,$”max_key2”.desc).show<br>+—-+—+——–+——–+<br>|key1|cnt|max_key2|avg_key3|<br>+—-+—+——–+——–+<br>| ccc|  1|       3|     5.0|<br>| aaa|  1|       1|     2.0|<br>| bbb|  2|       4|     5.0|<br>+—-+—+——–+——–+</p>
<p>package groupby</p>
<p>import org.apache.spark.SparkConf<br>import org.apache.spark.sql.SparkSession</p>
<p>object demos {</p>
<p>  def main(args: Array[String]): Unit &#x3D; {<br>    val conf &#x3D; new SparkConf().setAppName(“LzSparkDatasetExamples”).setMaster(“local[*]”)<br>    val sparkSession &#x3D; SparkSession.builder().enableHiveSupport().config(conf).getOrCreate()</p>
<p>&#x2F;&#x2F;    &#x2F;&#x2F;LOGGER.info(“——– this is info ——–”)<br>    import sparkSession.implicits._<br>    val df &#x3D; sparkSession.createDataset(Seq(<br>      (“aaa”, 1, 2),<br>      (“bbb”, 3, 4),<br>      (“ccc”, 3, 5),<br>      (“bbb”, 4, 6)<br>    )).toDF(“key1”, “key2”, “key3”)</p>
<pre><code>import org.apache.spark.sql.functions._
//LOGGER.info(&quot;--------df.groupBy(\&quot;key1\&quot;).count().show()-----------&quot;)
df.groupBy(&quot;key1&quot;).count().show()
//LOGGER.info(&quot;--------df.select(\&quot;key1\&quot;).distinct().show()-----------&quot;)
df.select(&quot;key1&quot;).distinct().show()
val key1Count = df.select(&quot;key1&quot;).distinct().count()
//LOGGER.info(&quot;--------df.select(\&quot;key1\&quot;).distinct().count()-----------&quot; +key1Count)
//LOGGER.info(&quot;--------df.groupBy(\&quot;key1\&quot;).count().sort(\&quot;key1\&quot;).show()-----------&quot;)
df.groupBy(&quot;key1&quot;).count().sort(&quot;key1&quot;).show()
//LOGGER.info(&quot;--------df.groupBy(\&quot;key1\&quot;).count().sort($\&quot;key1\&quot;.desc).show()-----------&quot;)
df.groupBy(&quot;key1&quot;).count().sort($&quot;key1&quot;.desc).show()
//LOGGER.info(&quot;--------df.groupBy(\&quot;key1\&quot;).count.withColumnRenamed(\&quot;count\&quot;, \&quot;cnt\&quot;).sort($\&quot;cnt\&quot;.desc).show-----------&quot;)
df.groupBy(&quot;key1&quot;).count
  .withColumnRenamed(&quot;count&quot;, &quot;cnt&quot;).sort($&quot;cnt&quot;.desc).show()
//LOGGER.info(&quot;--------df.groupBy(\&quot;key1\&quot;).agg(count(\&quot;key1\&quot;).as(\&quot;cnt\&quot;)).show-----------&quot;)
df.groupBy(&quot;key1&quot;).agg(count(&quot;key1&quot;).as(&quot;cnt&quot;)).show()

// 使用agg聚合函数
df.groupBy(&quot;key1&quot;).agg(count(&quot;key1&quot;), max(&quot;key2&quot;), avg(&quot;key3&quot;)).show
df.groupBy(&quot;key1&quot;).agg(&quot;key1&quot;-&gt;&quot;count&quot;, &quot;key2&quot;-&gt;&quot;max&quot;, &quot;key3&quot;-&gt;&quot;avg&quot;).show()
df.groupBy(&quot;key1&quot;).agg(Map((&quot;key1&quot;,&quot;count&quot;), (&quot;key2&quot;,&quot;max&quot;), (&quot;key3&quot;,&quot;avg&quot;))).show()
df.groupBy(&quot;key1&quot;).agg((&quot;key1&quot;,&quot;count&quot;), (&quot;key2&quot;,&quot;max&quot;), (&quot;key3&quot;,&quot;avg&quot;)).show
df.groupBy(&quot;key1&quot;)
  .agg(count(&quot;key1&quot;).as(&quot;cnt&quot;), max(&quot;key2&quot;).as(&quot;max_key2&quot;), avg(&quot;key3&quot;).as(&quot;avg_key3&quot;))
  .sort($&quot;cnt&quot;,$&quot;max_key2&quot;.desc).show
</code></pre>
<p>  }</p>
<p>}</p>
]]></content>
  </entry>
  <entry>
    <title>ceph分布式安装</title>
    <url>/2023/08/16/ceph%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>docker基本操作</title>
    <url>/2023/08/16/docker%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>elasticsearch分布式安装</title>
    <url>/2023/08/16/elasticsearch%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>fedora k8s 安装</title>
    <url>/2023/08/16/fedora-k8s-%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>gitlab_ci runners</title>
    <url>/2023/08/16/gitlab-ci-runners/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>hadoop安装</title>
    <url>/2023/08/16/hadoop%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>k8s安装</title>
    <url>/2023/08/16/k8s%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>kubeflow基础搭建</title>
    <url>/2023/08/16/kubeflow%E5%9F%BA%E7%A1%80%E6%90%AD%E5%BB%BA/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>linux硬盘格式化与挂载</title>
    <url>/2023/08/16/linux%E7%A1%AC%E7%9B%98%E6%A0%BC%E5%BC%8F%E5%8C%96%E4%B8%8E%E6%8C%82%E8%BD%BD/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>loguru elasticsearch kibana 日志处理</title>
    <url>/2023/08/16/loguru-elasticsearch-kibana-%E6%97%A5%E5%BF%97%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>minio分布式安装</title>
    <url>/2023/08/16/minio%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>mongodb 分布式安装</title>
    <url>/2023/08/16/mongodb-%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<p>参考搭建文档<a href="https://blog.csdn.net/msh6453/article/details/131161845">https://blog.csdn.net/msh6453/article/details/131161845</a></p>
<p>前言<br>官方文档：<a href="https://www.mongodb.com/docs/%EF%BC%88%E5%8F%AF%E4%BB%A5%E5%8F%82%E8%80%83%EF%BC%89">https://www.mongodb.com/docs/（可以参考）</a></p>
<p>一，安装说明<br>1.1环境说明<br>1、首先确定部署的环境，确定下服务器的端口，一般默认是22的端口；<br>2、操作系统Centos7.9；<br>3、 数据库mongodb-linux-x86_64-rhel70-4.4.22。<br>mongodb版本4.4.22</p>
<p><img src="/image-1.png" alt="Alt text"></p>
<p>mongos，数据库集群请求的入口，所有的请求都通过mongos进行协调，不需要在应用程序添加一个路由选择器，mongos自己就是一个请求分发中心，它负责把对应的数据请求请求转发到对应的shard服务器上。在生产环境通常有多mongos作为请求的入口，防止其中一个挂掉所有的mongodb请求都没有办法操作。</p>
<p>config server，顾名思义为配置服务器，存储所有数据库元信息（路由、分片）的配置。mongos本身没有物理存储分片服务器和数据路由信息，只是缓存在内存里，配置服务器则实际存储这些数据。mongos第一次启动或者关掉重启就会从 config server 加载配置信息，以后如果配置服务器信息变化会通知到所有的 mongos 更新自己的状态，这样 mongos 就能继续准确路由。在生产环境通常有多个 config server 配置服务器，因为它存储了分片路由的元数据，防止数据丢失！</p>
<p>shard，分片（sharding）是指将数据库拆分，将其分散在不同的机器上的过程。将数据分散到不同的机器上，不需要功能强大的服务器就可以存储更多的数据和处理更大的负载。基本思想就是将集合切成小块，这些块分散到若干片里，每个片只负责总数据的一部分，最后通过一个均衡器来对各个分片进行均衡（数据迁移）。</p>
<p>replica set，中文翻译副本集，其实就是shard的备份，防止shard挂掉之后数据丢失。复制提供了数据的冗余备份，并在多个服务器上存储数据副本，提高了数据的可用性， 并可以保证数据的安全性。</p>
<p>仲裁者（Arbiter），是复制集中的一个MongoDB实例，它并不保存数据。仲裁节点使用最小的资源并且不要求硬件设备，不能将Arbiter部署在同一个数据集节点中，可以部署在其他应用服务器或者监视服务器中，也可部署在单独的虚拟机中。为了确保复制集中有奇数的投票成员（包括primary），需要添加仲裁节点做为投票，否则primary不能运行时不会自动切换primary。</p>
<p>简单了解之后，我们可以这样总结一下，应用请求mongos来操作mongodb的增删改查，配置服务器存储数据库元信息，并且和mongos做同步，数据最终存入在shard（分片）上，为了防止数据丢失同步在副本集中存储了一份，仲裁在数据存储到分片的时候决定存储到哪个节点。</p>
<table>
<thead>
<tr>
<th>服务器</th>
<th>ceph-node-1</th>
<th>ceph-node-2</th>
<th>ceph-node-3</th>
</tr>
</thead>
<tbody><tr>
<td>ip</td>
<td>10.30.0.48</td>
<td>10.30.0.49</td>
<td>10.30.0.50</td>
</tr>
<tr>
<td>server-route</td>
<td>mongos</td>
<td>mongos</td>
<td>mongos</td>
</tr>
<tr>
<td>server-config</td>
<td>config server</td>
<td>config server</td>
<td>config server</td>
</tr>
<tr>
<td>server-config</td>
<td>shard server1 主节点</td>
<td>shard server1 副节点</td>
<td>shard server1 仲裁</td>
</tr>
<tr>
<td>server-config</td>
<td>shard server2 仲裁</td>
<td>shard server2 主节点</td>
<td>shard server2 副节点</td>
</tr>
<tr>
<td>server-config</td>
<td>shard server3 副节点</td>
<td>shard server3 仲裁</td>
<td>shard server3 主节点</td>
</tr>
</tbody></table>
<h2 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -p /opt/mongo/MongoDB/conf</span><br><span class="line"><span class="built_in">mkdir</span> -p /opt/mongo/MongoDB/server</span><br><span class="line"><span class="built_in">mkdir</span> -p /opt/mongo/MongoDB/mongos/log</span><br><span class="line"><span class="built_in">mkdir</span> -p /opt/mongo/MongoDB/config/data</span><br><span class="line"><span class="built_in">mkdir</span> -p /opt/mongo/MongoDB/config/log</span><br><span class="line"><span class="built_in">mkdir</span> -p /opt/mongo/MongoDB/shard1/data</span><br><span class="line"><span class="built_in">mkdir</span> -p /opt/mongo/MongoDB/shard1/log</span><br><span class="line"><span class="built_in">mkdir</span> -p /opt/mongo/MongoDB/shard2/data</span><br><span class="line"><span class="built_in">mkdir</span> -p /opt/mongo/MongoDB/shard2/log</span><br><span class="line"><span class="built_in">mkdir</span> -p /opt/mongo/MongoDB/shard3/data</span><br><span class="line"><span class="built_in">mkdir</span> -p /opt/mongo/MongoDB/shard3/log</span><br><span class="line"></span><br><span class="line">wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-4.4.22.tgz</span><br><span class="line">tar -xvzf mongodb-linux-x86_64-rhel70-4.4.22.tgz -C /opt/mongo/MongoDB/server/</span><br><span class="line"><span class="built_in">mv</span> /opt/mongo/MongoDB/server/mongodb-linux-x86_64-rhel70-4.4.22 /opt/mongo/MongoDB/server/mongodb</span><br><span class="line"></span><br><span class="line">vim /etc/profile</span><br><span class="line"><span class="built_in">export</span> MONGODB_HOME=/opt/mongo/MongoDB/server/mongodb</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$MONGODB_HOME</span>/bin:<span class="variable">$PATH</span></span><br><span class="line">:wq!</span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br><span class="line">vim /opt/mongo/MongoDB/conf/config.conf </span><br><span class="line">vim /opt/mongo/MongoDB/conf/shard1.conf</span><br><span class="line">vim /opt/mongo/MongoDB/conf/shard2.conf</span><br><span class="line">vim /opt/mongo/MongoDB/conf/shard3.conf</span><br><span class="line">vim /opt/mongo/MongoDB/conf/mongos.conf</span><br></pre></td></tr></table></figure>

<p>配置文件如mongo&#x2F;conf 下面<br>注意安装的时候不要设置最后的安全密钥，待完毕后添加</p>
<p>启动服务顺序</p>
<p>mongod -f &#x2F;opt&#x2F;mongo&#x2F;MongoDB&#x2F;conf&#x2F;config.conf<br>mongod -f &#x2F;opt&#x2F;mongo&#x2F;MongoDB&#x2F;conf&#x2F;shard1.conf<br>mongod -f &#x2F;opt&#x2F;mongo&#x2F;MongoDB&#x2F;conf&#x2F;shard2.conf<br>mongod -f &#x2F;opt&#x2F;mongo&#x2F;MongoDB&#x2F;conf&#x2F;shard3.conf<br>mongos -f &#x2F;opt&#x2F;mongo&#x2F;MongoDB&#x2F;conf&#x2F;mongos.conf</p>
<p>随便登入一台</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mongo 10.30.0.48:21000</span><br><span class="line">use admin</span><br><span class="line">config = &#123;_id : &quot;config&quot;,members : [&#123;_id : 0, host : &quot;10.30.0.48:21000&quot; &#125;,</span><br><span class="line">&#123;_id : 1, host : &quot;10.30.0.49:21000&quot; &#125;,&#123;_id : 2, host : &quot;10.30.0.50:21000&quot; &#125;]&#125;</span><br><span class="line">rs.initiate(config)</span><br></pre></td></tr></table></figure>


<p>同理 server sharded1</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mongo 10.30.0.48:27001</span><br><span class="line">use admin</span><br><span class="line">config = &#123; _id : &quot;shard1&quot;,members : [&#123;_id : 0, host : &quot;10.30.0.48:27001&quot; ,priority: 2 &#125;,&#123;_id : 1, host : &quot;10.30.0.49:27001&quot; ,priority: 1 &#125;,&#123;_id : 2, host : &quot;10.30.0.50:27001&quot;,arbiterOnly: true&#125;]&#125;</span><br><span class="line">//(“priority”优先级，数字越大，优先等级越高；“arbiterOnly”冲裁节点；冲裁节点根据优先等级判断哪个节点作为主节点)</span><br><span class="line">rs.initiate(config)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mongo 10.30.0.48:27002</span><br><span class="line">server shard2</span><br><span class="line">use admin</span><br><span class="line">config = &#123; _id : <span class="string">&quot;shard2&quot;</span>,members : [&#123;_id : 0, host : <span class="string">&quot;10.30.0.48:27002&quot;</span> ,arbiterOnly: <span class="literal">true</span> &#125;,</span><br><span class="line">&#123;_id : 1, host : <span class="string">&quot;10.30.0.49:27002&quot;</span> ,priority: 2 &#125;,&#123;_id : 2, host : <span class="string">&quot;10.30.0.50:27002&quot;</span>,priority: 1&#125;]&#125;</span><br><span class="line">rs.initiate(config)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>server shard3</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mongo 10.30.0.48:27003</span><br><span class="line">use admin</span><br><span class="line">config = &#123; _id : &quot;shard3&quot;,members : [&#123;_id : 0, host : &quot;10.30.0.48:27003&quot; ,priority: 1 &#125;,</span><br><span class="line"></span><br><span class="line">&#123;_id : 1, host : &quot;10.30.0.49:27003&quot; ,arbiterOnly: true &#125;,&#123;_id : 2, host : &quot;10.30.0.50:27003&quot;,priority: 2&#125;]&#125;</span><br><span class="line"></span><br><span class="line">rs.initiate(config)</span><br></pre></td></tr></table></figure>



<p>添加分片服务器</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">use admin</span><br><span class="line">sh.addShard(<span class="string">&quot;shard1/10.30.0.48:27001,10.30.0.49:27001,10.30.0.50:27001&quot;</span>)</span><br><span class="line"></span><br><span class="line">sh.addShard(<span class="string">&quot;shard2/10.30.0.48:27002,10.30.0.49:27002,10.30.0.50:27002&quot;</span>)</span><br><span class="line"></span><br><span class="line">sh.addShard(<span class="string">&quot;shard3/10.30.0.48:27003,10.30.0.49:27003,10.30.0.50:27003&quot;</span>)</span><br><span class="line"></span><br><span class="line">sh.status()</span><br></pre></td></tr></table></figure>


<p>创建用户</p>
<p>执行命令： mongo -port 20000<br>执行命令： use admin&#x2F;&#x2F;这个条件是必须的<br>执行命令：db.createUser(</p>
<pre><code>&#123;

    user:&quot;ml_grp&quot;,

    pwd:&quot;ml&amp;dl#mongodb&quot;,

    roles:[&#123;role:&quot;root&quot;,db:&quot;admin&quot;&#125;]

&#125;
</code></pre>
<p>)</p>
<p>use admin<br>执行命令：db.auth(‘ml_grp’,’passwd’)<br>执行命令：db.runCommand( { enablesharding :”zjxndc”});&#x2F;&#x2F;为zjxndc开启分片功能<br>执行命令：db.runCommand( { shardcollection : “zjxndc.measureHisvalues”,key : {_id: 1} } )&#x2F;</p>
<p>use config<br>db.settings.save({“_id”:”chunksize”,”value”:1})<br>use zjxndc<br>d、执行命令：for (var i &#x3D; 1; i &lt;&#x3D; 100000; i++){</p>
<p>db.measureHisvalues.insert({“_id”:i,”test1”:”testval1”+i});</p>
<p>}</p>
<p><img src="/1408af41c022dc62726ecf884c95ff0a.png" alt="enable partition"><br><img src="/image.png" alt="查看是否分区成功"></p>
<h3 id="配置安全"><a href="#配置安全" class="headerlink" title="配置安全"></a>配置安全</h3><p>4.1 安全验证设置用户<br>1、副本集和共享集群的各个节点成员之间使用内部身份验证，可以使用密钥文件或x.509证书。密钥文件比较简单，官方推荐如果是测试环境可以使用密钥文件，但是正是环境，官方推荐x.509证书。原理就是，集群中每一个实例彼此连接的时候都检验彼此使用的证书的内容是否相同。只有证书相同的实例彼此才可以访问。使用客户端连接到mongodb集群时，开启访问授权。对于集群外部的访问。如通过可视化客户端，或者通过代码连接的时候，需要开启授权。<br>a、生成密钥文件，在keyfile身份验证中，副本集中的每个mongod实例都使用keyfile的内容作为共享密码，只有具有正确密钥文件的mongod或者mongos实例可以连接到副本集。密钥文件的内容必须在6到1024个字符之间，并且在unix&#x2F;linux系统中文件所有者必须有对文件至少有读的权限。可以用任何方式生成密钥文件例如：(任意一台机器即可)</p>
<p>执行命令：openssl rand -base64 756 &gt; &#x2F;opt&#x2F;mongo&#x2F;MongoDB&#x2F;testKeyFile.file &#x2F;&#x2F;生成密钥</p>
<p>执行命令：chmod 400 &#x2F;opt&#x2F;mongo&#x2F;MongoDB&#x2F;testKeyFile.file &#x2F;&#x2F;赋予权限</p>
<p>执行命令：scp -P22 &#x2F;opt&#x2F;mongo&#x2F;MongoDB&#x2F;testKeyFile.file <a href="mailto:&#114;&#x6f;&#111;&#116;&#64;&#x31;&#x30;&#46;&#51;&#x30;&#x2e;&#48;&#46;&#52;&#57;">&#114;&#x6f;&#111;&#116;&#64;&#x31;&#x30;&#46;&#51;&#x30;&#x2e;&#48;&#46;&#52;&#57;</a>:&#x2F;opt&#x2F;mongo&#x2F;MongoDB &#x2F;&#x2F;拷贝至其他0.55服务器上（“-P22”是端口，根据实际情况来）</p>
<p>执行命令：scp -P22 &#x2F;opt&#x2F;mongo&#x2F;MongoDB&#x2F;testKeyFile.file root@:10.30.0.49 &#x2F;opt&#x2F;mongo&#x2F;MongoDB &#x2F;&#x2F;拷贝至其他0.56服务器上</p>
<p>创建用户成功后，关闭所有的节点（三台机都需要操作）</p>
<p>执行命令：按照先后顺序来处理关闭，mongos&gt;config&gt;shadr3&gt;shadr2&gt;shadr1</p>
<p>&#x2F;&#x2F;注意的是每一个服务的关闭都需要在三台机上关闭，在关闭其他服务。例如关闭shadr3服务，先关闭0.54服务器上的shadr3服务，其次0.55服务器上的shadr3服务，再是0.56服务器上的shadr3服务；然后在关闭shadr2服务，也是按照这个顺序处理。（这个地方主要新手操作避免出错）</p>
<p>执行命令：</p>
<p>mongod -f &#x2F;opt&#x2F;mongo&#x2F;MongoDB&#x2F;conf&#x2F;shard1.conf –shutdown</p>
<p>mongod -f &#x2F;opt&#x2F;mongo&#x2F;MongoDB&#x2F;conf&#x2F;shard2.conf –shutdown</p>
<p>mongod -f &#x2F;opt&#x2F;mongo&#x2F;MongoDB&#x2F;conf&#x2F;shard3.conf –shutdown</p>
<p>mongod -f &#x2F;opt&#x2F;mongo&#x2F;MongoDB&#x2F;conf&#x2F;config.conf –shutdown</p>
<p>mongo 10.30.0.48:20000&#x2F;&#x2F;mongos需要这样关闭，用上面的命令有问题。</p>
<p>use admin<br>db.auth(‘ml_grp’,’passwd’)<br>db.shutdownServer()</p>
<p>3、配置testKeyFile.file，依次在每台机器上的mongos.conf、config.conf、shard1.conf、shard2.conf、shard3.conf的配置和开启授权验证。<br>a、先是config.conf、shard1.conf、shard2.conf、shard3.conf的配置和开启授权验证。（三台机器的这些文件都需要添加）<br>在conf这几个文件的的最后添加：<br>security:</p>
<p>  keyFile: &#x2F;opt&#x2F;mongo&#x2F;MongoDB&#x2F;testKeyFile.file</p>
<p>  authorization: enabled</p>
<p>b、然后在三台机器的mongos.conf配置文件中最后添加：<br>security:</p>
<p>  keyFile: &#x2F;opt&#x2F;mongo&#x2F;MongoDB&#x2F;testKeyFile.file</p>
<p>&#x2F;&#x2F;这里就说明了testKeyFile.file最好在每台机器放在一个位置，为了后面复制粘贴处理</p>
<p>&#x2F;&#x2F;解释说明： mongos比mongod少了authorization：enabled的配置。原因是，副本集加分片的安全认证需要配置两方面的，副本集各个节点之间使用内部身份验证，用于内部各个mongo实例的通信，只有相同keyfile才能相互访问。所以都要开启keyFile: &#x2F;data&#x2F;mongodb&#x2F;testKeyFile.file</p>
<pre><code>然而对于所有的mongod，才是真正的保存数据的分片。mongos只做路由，不保存数据。所以所有的mongod开启访问数据的授权authorization:enabled。这样用户只有账号密码正确才能访问到数据
</code></pre>
]]></content>
  </entry>
  <entry>
    <title>redis分布式安装</title>
    <url>/2023/08/16/redis%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<p>1.环境配置<br>#在所有节点配置YUM：<br>#清空原来自带配置文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /etc/yum.repos.d/</span><br><span class="line"><span class="built_in">mkdir</span> /tmp/bak</span><br><span class="line"><span class="built_in">mv</span> * /tmp/bak/</span><br></pre></td></tr></table></figure>
<p>#配置系统源码，epel源：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo</span><br><span class="line">yum install wget -y</span><br><span class="line">wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo</span><br><span class="line"><span class="comment">#YUM优先级别：</span></span><br><span class="line">yum -y install yum-plugin-priorities.noarch</span><br></pre></td></tr></table></figure>
<p>#配置ceph源：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cat</span> &lt;&lt; <span class="string">EOF | tee /etc/yum.repos.d/ceph.repo</span></span><br><span class="line"><span class="string">[Ceph]</span></span><br><span class="line"><span class="string">name=Ceph packages for $basearch</span></span><br><span class="line"><span class="string">baseurl=http://mirrors.163.com/ceph/rpm-nautilus/el7/\$basearch</span></span><br><span class="line"><span class="string">enabled=1</span></span><br><span class="line"><span class="string">gpgcheck=1</span></span><br><span class="line"><span class="string">type=rpm-md</span></span><br><span class="line"><span class="string">gpgkey=https://download.ceph.com/keys/release.asc</span></span><br><span class="line"><span class="string">priority=1</span></span><br><span class="line"><span class="string">[Ceph-noarch]</span></span><br><span class="line"><span class="string">name=Ceph noarch packages</span></span><br><span class="line"><span class="string">baseurl=http://mirrors.163.com/ceph/rpm-nautilus/el7/noarch</span></span><br><span class="line"><span class="string">enabled=1</span></span><br><span class="line"><span class="string">gpgcheck=1</span></span><br><span class="line"><span class="string">type=rpm-md</span></span><br><span class="line"><span class="string">gpgkey=https://download.ceph.com/keys/release.asc</span></span><br><span class="line"><span class="string">priority=1</span></span><br><span class="line"><span class="string">[ceph-source]</span></span><br><span class="line"><span class="string">name=Ceph source packages</span></span><br><span class="line"><span class="string">baseurl=http://mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS</span></span><br><span class="line"><span class="string">enabled=1</span></span><br><span class="line"><span class="string">gpgcheck=1</span></span><br><span class="line"><span class="string">type=rpm-md</span></span><br><span class="line"><span class="string">gpgkey=https://download.ceph.com/keys/release.asc</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure>
<p>#关闭防火墙：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld</span><br><span class="line">systemctl status firewalld</span><br></pre></td></tr></table></figure>
<p>#配置主机名称：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph1节点：</span><br><span class="line">hostnamectl --static set-hostname ceph1</span><br><span class="line">ceph2节点：</span><br><span class="line">hostnamectl --static set-hostname ceph2</span><br><span class="line">ceph3节点：</span><br><span class="line">hostnamectl --static set-hostname ceph3</span><br></pre></td></tr></table></figure>
<p>#所有节点配置hosts文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/etc/hosts</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">192.168.0.231    ceph1</span><br><span class="line">192.168.0.232    ceph2</span><br><span class="line">192.168.0.233    ceph3</span><br></pre></td></tr></table></figure>

<p>#所有节点NTP配置：<br>在所有集群和客户端节点安装NTP，修改配置。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum -y install ntp ntpdate</span><br><span class="line"><span class="comment"># 以ceph1为NTP服务端节点，在ceph1新建NTP文件。</span></span><br><span class="line">vi /etc/ntp.conf</span><br><span class="line"><span class="comment"># 并新增如下内容作为NTP服务端：</span></span><br><span class="line">restrict 127.0.0.1</span><br><span class="line">restrict ::1</span><br><span class="line">restrict 192.168.3.0 mask 255.255.255.0 //ceph1的网段与掩码</span><br><span class="line">server 127.127.1.0</span><br><span class="line">fudge 127.127.1.0 stratum 8</span><br></pre></td></tr></table></figure>
<p>在ceph2、ceph3及所有客户机节点新建NTP文件。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vi /etc/ntp.conf</span><br><span class="line"><span class="comment"># 并新增如下内容作为客户端：</span></span><br><span class="line">server 192.168.3.166</span><br><span class="line"></span><br><span class="line">systemctl start ntpd</span><br><span class="line">systemctl <span class="built_in">enable</span> ntpd</span><br><span class="line">systemctl status ntpd</span><br></pre></td></tr></table></figure>
<p>#ssh配置，在ceph1节点生成公钥，并发放到各个主机&#x2F;客户机节点。：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa <span class="comment">#回车采取默认配置</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..3&#125;; <span class="keyword">do</span> ssh-copy-id ceph<span class="variable">$i</span>; <span class="keyword">done</span> <span class="comment">#根据提示输入yes及节点密码</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..3&#125;; <span class="keyword">do</span> ssh-copy-id client<span class="variable">$i</span>; <span class="keyword">done</span></span><br></pre></td></tr></table></figure>

<p>#在所有节点，关闭SELinux</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sed -i <span class="string">&#x27;s/enforcing/disabled/&#x27;</span> /etc/selinux/config</span><br><span class="line">setenforce 0</span><br></pre></td></tr></table></figure>
<ol>
<li>安装Ceph软件<br>使用yum install安装ceph的时候会默认安装当前已有的最新版，如果不想安装最新版本，可以在&#x2F;etc&#x2F;yum.conf文件中加以限制。<br>2.1 在所有集群和客户端节点安装Ceph<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum -y install ceph</span><br><span class="line">ceph -v命令查看版本:</span><br><span class="line">[root@ceph1 ~]<span class="comment"># ceph -v</span></span><br><span class="line">ceph version 14.2.9 (581f22da52345dba46ee232b73b990f06029a2a0) nautilus (stable)</span><br><span class="line">[root@ceph2 ~]<span class="comment"># ceph -v</span></span><br><span class="line">ceph version 14.2.9 (581f22da52345dba46ee232b73b990f06029a2a0) nautilus (stable)</span><br><span class="line">[root@ceph3 ~]<span class="comment"># ceph -v</span></span><br><span class="line">ceph version 14.2.9 (581f22da52345dba46ee232b73b990f06029a2a0) nautilus (stable)</span><br></pre></td></tr></table></figure>
2.2 在ceph1节点额外安装ceph-deploy。<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum -y install ceph-deploy</span><br></pre></td></tr></table></figure>
3.部署MON节点<br>3.1 创建目录生成配置文件<figure class="highlight stata"><table><tr><td class="code"><pre><span class="line"><span class="keyword">mkdir</span> <span class="keyword">cluster</span></span><br><span class="line"><span class="keyword">cd</span> <span class="keyword">cluster</span></span><br><span class="line">ceph-deploy new ceph1 ceph2 ceph3</span><br></pre></td></tr></table></figure></li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@ceph1 ~]<span class="comment"># cd cluster/</span></span><br><span class="line">[root@ceph1 cluster]<span class="comment"># ceph-deploy new ceph1 ceph2 ceph3</span></span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy new ceph1 ceph2 ceph3</span><br><span class="line">[ceph_deploy.cli][INFO  ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO  ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  func                          : &lt;<span class="keyword">function</span> new at 0x7ffb7dc07de8&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  overwrite_conf                : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  quiet                         : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7ffb7d58c6c8&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO  ]  ssh_copykey                   : True</span><br><span class="line">[ceph_deploy.cli][INFO  ]  mon                           : [<span class="string">&#x27;ceph1&#x27;</span>, <span class="string">&#x27;ceph2&#x27;</span>, <span class="string">&#x27;ceph3&#x27;</span>]</span><br><span class="line">[ceph_deploy.cli][INFO  ]  public_network                : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  ceph_conf                     : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cluster_network               : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  default_release               : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  fsid                          : None</span><br><span class="line">[ceph_deploy.new][DEBUG ] Creating new cluster named ceph</span><br><span class="line">[ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds</span><br><span class="line">[ceph1][DEBUG ] connected to host: ceph1 </span><br><span class="line">[ceph1][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph1][DEBUG ] detect machine <span class="built_in">type</span></span><br><span class="line">[ceph1][DEBUG ] find the location of an executable</span><br><span class="line">[ceph1][INFO  ] Running <span class="built_in">command</span>: /usr/sbin/ip <span class="built_in">link</span> show</span><br><span class="line">[ceph1][INFO  ] Running <span class="built_in">command</span>: /usr/sbin/ip addr show</span><br><span class="line">[ceph1][DEBUG ] IP addresses found: [u<span class="string">&#x27;192.168.0.231&#x27;</span>]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Resolving host ceph1</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor ceph1 at 192.168.0.231</span><br><span class="line">[ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds</span><br><span class="line">[ceph2][DEBUG ] connected to host: ceph1 </span><br><span class="line">[ceph2][INFO  ] Running <span class="built_in">command</span>: ssh -CT -o BatchMode=<span class="built_in">yes</span> ceph2</span><br><span class="line">[ceph2][DEBUG ] connected to host: ceph2 </span><br><span class="line">[ceph2][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph2][DEBUG ] detect machine <span class="built_in">type</span></span><br><span class="line">[ceph2][DEBUG ] find the location of an executable</span><br><span class="line">[ceph2][INFO  ] Running <span class="built_in">command</span>: /usr/sbin/ip <span class="built_in">link</span> show</span><br><span class="line">[ceph2][INFO  ] Running <span class="built_in">command</span>: /usr/sbin/ip addr show</span><br><span class="line">[ceph2][DEBUG ] IP addresses found: [u<span class="string">&#x27;192.168.0.232&#x27;</span>]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Resolving host ceph2</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor ceph2 at 192.168.0.232</span><br><span class="line">[ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds</span><br><span class="line">[ceph3][DEBUG ] connected to host: ceph1 </span><br><span class="line">[ceph3][INFO  ] Running <span class="built_in">command</span>: ssh -CT -o BatchMode=<span class="built_in">yes</span> ceph3</span><br><span class="line">[ceph3][DEBUG ] connected to host: ceph3 </span><br><span class="line">[ceph3][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph3][DEBUG ] detect machine <span class="built_in">type</span></span><br><span class="line">[ceph3][DEBUG ] find the location of an executable</span><br><span class="line">[ceph3][INFO  ] Running <span class="built_in">command</span>: /usr/sbin/ip <span class="built_in">link</span> show</span><br><span class="line">[ceph3][INFO  ] Running <span class="built_in">command</span>: /usr/sbin/ip addr show</span><br><span class="line">[ceph3][DEBUG ] IP addresses found: [u<span class="string">&#x27;192.168.0.233&#x27;</span>]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Resolving host ceph3</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor ceph3 at 192.168.0.233</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor initial members are [<span class="string">&#x27;ceph1&#x27;</span>, <span class="string">&#x27;ceph2&#x27;</span>, <span class="string">&#x27;ceph3&#x27;</span>]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor addrs are [<span class="string">&#x27;192.168.0.231&#x27;</span>, <span class="string">&#x27;192.168.0.232&#x27;</span>, <span class="string">&#x27;192.168.0.233&#x27;</span>]</span><br><span class="line">[ceph_deploy.new][DEBUG ] Creating a random mon key...</span><br><span class="line">[ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...</span><br><span class="line">[ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...</span><br></pre></td></tr></table></figure>
<p>3.2 初始化密钥</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-deploy mon create-initial</span><br></pre></td></tr></table></figure>
<p>3.3 将ceph.client.admin.keyring拷贝到各个节点上</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-deploy --overwrite-conf admin ceph1 ceph2 ceph3</span><br></pre></td></tr></table></figure>
<p>3.4 查看是否配置成功。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@ceph1 cluster]<span class="comment"># ceph -scluster:id:     ea192428-05d2-437a-8cce-9d187de82dd5health: HEALTH_OKservices:mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 5m)mgr: no daemons activeosd: 0 osds: 0 up, 0 indata:pools:   0 pools, 0 pgsobjects: 0 objects, 0 Busage:   0 B used, 0 B / 0 B availpgs:  </span></span><br></pre></td></tr></table></figure>
<p>4 部署MGR节点</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-deploy mgr create ceph1 ceph2 ceph3</span><br></pre></td></tr></table></figure>
<p>查看MGR是否部署成功。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph -s</span><br><span class="line">[root@ceph1 cluster]<span class="comment"># ceph -scluster:id:     ea192428-05d2-437a-8cce-9d187de82dd5health: HEALTH_WARNOSD count 0 &lt; osd_pool_default_size 3services:mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 8m)mgr: ceph1(active, since 22s), standbys: ceph2, ceph3osd: 0 osds: 0 up, 0 indata:pools:   0 pools, 0 pgsobjects: 0 objects, 0 Busage:   0 B used, 0 B / 0 B availpgs: </span></span><br><span class="line">```  </span><br><span class="line">5 部署OSD节点</span><br><span class="line">```bash</span><br><span class="line">ceph-deploy osd create --data /dev/sdb ceph1</span><br><span class="line">ceph-deploy osd create --data /dev/sdc ceph1</span><br><span class="line">ceph-deploy osd create --data /dev/sdd ceph1</span><br><span class="line">ceph-deploy osd create --data /dev/sdb ceph2</span><br><span class="line">ceph-deploy osd create --data /dev/sdc ceph2</span><br><span class="line">ceph-deploy osd create --data /dev/sdd ceph2</span><br><span class="line">ceph-deploy osd create --data /dev/sdb ceph3</span><br><span class="line">ceph-deploy osd create --data /dev/sdc ceph3</span><br><span class="line">ceph-deploy osd create --data /dev/sdd ceph3</span><br></pre></td></tr></table></figure>
<p>创建成功后，查看是否正常</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@ceph1 cluster]<span class="comment"># ceph -scluster:id:     ea192428-05d2-437a-8cce-9d187de82dd5health: HEALTH_OKservices:mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 14m)mgr: ceph1(active, since 6m), standbys: ceph2, ceph3osd: 9 osds: 9 up (since 2m), 9 in (since 2m)data:pools:   0 pools, 0 pgsobjects: 0 objects, 0 Busage:   9.0 GiB used, 135 GiB / 144 GiB availpgs:  </span></span><br></pre></td></tr></table></figure>
<p>6 验证Ceph<br>创建存储池</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd pool create vdbench 10 10</span><br></pre></td></tr></table></figure>
<p>创建块设备</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd create image01 --size 200--pool vdbench --image-format 2 --image-feature layering</span><br><span class="line">rbd <span class="built_in">ls</span> --pool vdbench</span><br><span class="line">[root@ceph1 cluster]<span class="comment"># rbd create image01 --size 200 --pool  vdbench --image-format 2 --image-feature layering</span></span><br><span class="line">[root@ceph1 cluster]<span class="comment"># rbd ls --pool vdbench</span></span><br><span class="line">image01</span><br></pre></td></tr></table></figure>

<p>#PG 分 配 计 算<br>归置组(PG)的数量是由管理员在创建存储池的时候指定的，然后由 CRUSH 负责创建和使用，PG 的数量是 2 的 N 次方的倍数,每个 OSD 的 PG 不要超出 250 个 PG<br>Total PGs &#x3D; (Total_number_of_OSD * 100) &#x2F; max_replication_count<br>单个 pool 的 PG 计算如下：<br>有 100 个 osd，3 副本，5 个 pool<br>Total PGs &#x3D;100*100&#x2F;3&#x3D;3333<br>每个 pool 的 PG&#x3D;3333&#x2F;5&#x3D;512，那么创建 pool 的时候就指定 pg 为 512<br>客户端在读写对象时，需要提供的是对象标识和存储池名称<br>客户端需要在存储池中读写对象时，需要客户端将对象名称，对象名称的hash码，存储池中的PG数量和存储池名称作为输入信息提供给ceph，然后由CRUSH计算出PG的ID以及PG针对的主OSD即可读写OSD中的对象。<br>具体写操作如下：<br>1.APP向ceph客户端发送对某个对象的请求，此请求包含对象和存储池，然后ceph客户端对访问的对象做hash计算，并根据此hash值计算出对象所在的PG，完成对象从Pool至PG的映射。<br>APP 访问 pool ID 和 object ID （比如 pool &#x3D; pool1 and object-id &#x3D; “name1”）<br>ceph client 对 objectID 做哈希<br>ceph client 对该 hash 值取 PG 总数的模，得到 PG 编号(比如 32)<br>ceph client 对 pool ID 取 hash（比如 “pool1” &#x3D; 3）<br>ceph client 将 pool ID 和 PG ID 组合在一起(比如 3.23)得到 PG 的完整 ID。<br>2.然后客户端据 PG、CRUSH 运行图和归置组(placement rules)作为输入参数并再次进行计<br>算，并计算出对象所在的 PG 内的主 OSD ，从而完成对象从 PG 到 OSD 的映射。<br>3.客户端开始对主 OSD 进行读写请求(副本池 IO)，如果发生了写操作，会有 ceph 服务端完<br>成对象从主 OSD 到备份 OSD 的同步  </p>
<p>二.熟练 ceph 的用户管理及授权<br>客户端使用 session key 向 mon 请求所需要的服务，mon 向客户端提供一个 tiket，用于向实际处理数据的 OSD 等服务验证客户端身份，MON 和 OSD 共享同一个 secret.<br>ceph 用户需要拥有存储池访问权限，才能读取和写入数据<br>ceph 用户必须拥有执行权限才能使用 ceph 的管理命令<br>ceph 支持多种类型的用户，但可管理的用户都属于 client 类型<br>通过点号来分割用户类型和用户名，格式为 TYPE.ID，例如 client.admin。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@ceph-deploy:~<span class="comment"># cat /etc/ceph/ceph.client.admin.keyring </span></span><br><span class="line">[client.admin]</span><br><span class="line">        key = AQBnFaNj1iyBMBAAd+9hKWXaNw3GYxT9PEXvrQ==</span><br><span class="line">        caps mds = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">        caps mgr = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">        caps mon = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">        caps osd = <span class="string">&quot;allow *&quot;</span></span><br></pre></td></tr></table></figure>
<p>#列 出 指 定 用 户 信 息</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@ceph-deploy:~<span class="comment"># ceph auth get osd.10</span></span><br><span class="line">[osd.10]</span><br><span class="line">        key = AQB+I6Njk4KWNBAAL09FFayLKF44IgUQ1fjKYQ==</span><br><span class="line">        caps mgr = <span class="string">&quot;allow profile osd&quot;</span></span><br><span class="line">        caps mon = <span class="string">&quot;allow profile osd&quot;</span></span><br><span class="line">        caps osd = <span class="string">&quot;allow *&quot;</span></span><br><span class="line"></span><br><span class="line">exported keyring <span class="keyword">for</span> osd.10</span><br></pre></td></tr></table></figure>
<p>#: 列 出 用 户</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~$ ceph auth list</span><br><span class="line">mds.ceph-mgr1</span><br><span class="line">        key: AQAdRbFjOwBXIRAAUTdwElBzYPHW+4uFicFC7Q==</span><br><span class="line">        caps: [mds] allow</span><br><span class="line">        caps: [mon] allow profile mds</span><br><span class="line">        caps: [osd] allow rwx</span><br><span class="line">osd.0</span><br><span class="line">        key: AQC0IqNjbcgKIxAA+BCNpQeZiMujR+r+69Miig==</span><br><span class="line">        caps: [mgr] allow profile osd</span><br><span class="line">        caps: [mon] allow profile osd</span><br><span class="line">        caps: [osd] allow *</span><br></pre></td></tr></table></figure>
<p>#可以结合使用-o 文件名选项和 ceph auth list 将输出保存到某个文件。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth list -o 123.key</span><br></pre></td></tr></table></figure>

<p>#ceph auth add<br>此命令是添加用户的规范方法。它会创建用户、生成密钥，并添加所有指定的能力<br>添加认证 key：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth add client.tom mon <span class="string">&#x27;allow r&#x27;</span> osd <span class="string">&#x27;allow rwx pool=testpool2&#x27;</span></span><br><span class="line">added key <span class="keyword">for</span> client.tom</span><br></pre></td></tr></table></figure>

<p>##验证 key</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth get client.tom</span><br><span class="line">[client.tom]</span><br><span class="line">        key = AQD2vbJj8fIiDBAArtJBzQiuPy8nDWPSFVs0bw==</span><br><span class="line">        caps mon = <span class="string">&quot;allow r&quot;</span></span><br><span class="line">        caps osd = <span class="string">&quot;allow rwx pool=testpool2&quot;</span></span><br><span class="line">exported keyring <span class="keyword">for</span> client.tom</span><br></pre></td></tr></table></figure>
<p>##创建用户</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth get-or-create client.jack mon <span class="string">&#x27;allow r&#x27;</span> osd <span class="string">&#x27;allow rwx pool=testpool2&#x27;</span></span><br><span class="line">[client.jack]</span><br><span class="line">        key = AQC/vrJj5kenHhAAGeRJpY64feS4Dn6DD/R8VA==</span><br></pre></td></tr></table></figure>
<p>##再次创建用户</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth get-or-create client.jack mon <span class="string">&#x27;allow r&#x27;</span> osd <span class="string">&#x27;allow rwx pool=testpool2&#x27;</span></span><br><span class="line">[client.jack]</span><br><span class="line">        key = AQC/vrJj5kenHhAAGeRJpY64feS4Dn6DD/R8VA==</span><br></pre></td></tr></table></figure>
<p>#ceph auth get-or-create-key:<br>此命令是创建用户并返回用户密钥，对于只需要密钥的客户端(例如 libvrirt),此命令非常有用。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth get-or-create-key client.jack</span><br><span class="line">mon <span class="string">&#x27;allow r&#x27;</span> osd <span class="string">&#x27;allow rwx pool=mypool&#x27;</span></span><br><span class="line">AQAtr8dfi37XMhAADbHWEZ0shY1QZ5A8eBpeoQ==</span><br></pre></td></tr></table></figure>
<p>用户有 key 就显示没有就创建<br>#修 改 用 户 能 力</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth get client.jack</span><br><span class="line">[client.jack]</span><br><span class="line">        key = AQC/vrJj5kenHhAAGeRJpY64feS4Dn6DD/R8VA==</span><br><span class="line">        caps mon = <span class="string">&quot;allow r&quot;</span></span><br><span class="line">        caps osd = <span class="string">&quot;allow rwx pool=testpool2&quot;</span></span><br><span class="line">exported keyring <span class="keyword">for</span> client.jack</span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth caps client.jack mon <span class="string">&#x27;allow r&#x27;</span> osd <span class="string">&#x27;allow rw pool=testpool2&#x27;</span></span><br><span class="line">updated caps <span class="keyword">for</span> client.jack</span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth get client.jack</span><br><span class="line">[client.jack]</span><br><span class="line">        key = AQC/vrJj5kenHhAAGeRJpY64feS4Dn6DD/R8VA==</span><br><span class="line">        caps mon = <span class="string">&quot;allow r&quot;</span></span><br><span class="line">        caps osd = <span class="string">&quot;allow rw pool=testpool2&quot;</span></span><br><span class="line">exported keyring <span class="keyword">for</span> client.jack</span><br></pre></td></tr></table></figure>

<p>#删 除 用 户 </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth del client.tom</span><br><span class="line">updated</span><br></pre></td></tr></table></figure>
<p>#导出 keyring 至指定文件</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth get client.user1 -o</span><br><span class="line">ceph.client.user1.keyring</span><br><span class="line">exported keyring <span class="keyword">for</span> client.user1</span><br></pre></td></tr></table></figure>
<p>#验证指定用户的 keyring 文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ <span class="built_in">cat</span> ceph.client.user1.keyring</span><br><span class="line">[client.user1]</span><br><span class="line">key = AQAUUchfjpMqGRAARV6h0ofdDEneuaRnxuHjoQ==</span><br><span class="line">caps mon = <span class="string">&quot;allow r&quot;</span></span><br><span class="line">caps osd = <span class="string">&quot;allow * pool=mypool&quot;</span></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth del client.user1 <span class="comment">#演示误删除用户</span></span><br><span class="line">Updated</span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth get client.user1 <span class="comment">#确认用户被删除</span></span><br><span class="line">Error ENOENT: failed to find client.user1 <span class="keyword">in</span> keyring</span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth import -i</span><br><span class="line">ceph.client.user1.keyring <span class="comment">#导入用户</span></span><br><span class="line">imported keyring</span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth get client.user1 <span class="comment">#验证已恢复用户</span></span><br><span class="line">exported keyring <span class="keyword">for</span> client.user1</span><br></pre></td></tr></table></figure>
<p>#将多 用 户 导 出 至 秘 钥 环 ：<br>#创建 keyring 文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ceph-authtool --create-keyring ceph.client.user.keyring <span class="comment">#创建空的 keyring 文件</span></span><br><span class="line">creating ceph.client.user.keyring</span><br></pre></td></tr></table></figure>
<p>#把指定的 admin 用户的 keyring 文件内容导入到 user 用户的 keyring 文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="variable">$ceph</span>-authtool ./ceph.client.user.keyring</span><br><span class="line">--import-keyring ./ceph.client.admin.keyring</span><br><span class="line">importing contents of ./ceph.client.admin.keyring into ./ceph.client.user.keyring</span><br></pre></td></tr></table></figure>
<p>#验证 keyring 文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph-authtool -l ./ceph.client.user.keyring</span><br><span class="line">[client.admin]</span><br><span class="line">key = AQAGDKJfQk/dAxAA3Y+9xoE/p8in6QjoHeXmeg==</span><br><span class="line">caps mds = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">caps mgr = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">caps mon = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">caps osd = <span class="string">&quot;allow *&quot;</span></span><br></pre></td></tr></table></figure>
<p>#再导入一个其他用户的 keyring：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph-authtool ./ceph.client.user.keyring</span><br><span class="line">--import-keyring ./ceph.client.user1.keyring</span><br><span class="line">importing contents of ./ceph.client.user1.keyring into ./ceph.client.user.keyring</span><br></pre></td></tr></table></figure>
<p>#再次验证 keyring 文件是否包含多个用户的认证信息：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph-authtool -l ./ceph.client.user.keyring</span><br><span class="line">[client.admin]</span><br><span class="line">key = AQAGDKJfQk/dAxAA3Y+9xoE/p8in6QjoHeXmeg==</span><br><span class="line">caps mds = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">caps mgr = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">caps mon = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">caps osd = <span class="string">&quot;allow *&quot;</span></span><br><span class="line">[client.user1]</span><br><span class="line">key = AQAUUchfjpMqGRAARV6h0ofdDEneuaRnxuHjoQ==</span><br><span class="line">caps mon = <span class="string">&quot;allow r&quot;</span></span><br><span class="line">caps osd = <span class="string">&quot;allow * pool=mypool&quot;</span></span><br></pre></td></tr></table></figure>
<p>三. 使用普通客户挂载块存储<br>#创建存储池：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$ceph</span> osd pool create rbd-data1 32 32</span><br><span class="line"></span><br><span class="line"><span class="comment">#存储池启用 rbd</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$ceph</span> osd pool application <span class="built_in">enable</span> rbd-data1 rbd</span><br><span class="line"><span class="comment">#初始化 rbd</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$rbd</span> pool init -p rbd-data1</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建两个镜像：</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$rbd</span> create data-img1 --size 3G --pool rbd-data1 --image-format 2 --image-feature layering</span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$rbd</span> create data-img2 --size 5G --pool rbd-data1 --image-format 2 --image-feature layering</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#列出镜像信息</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$rbd</span> <span class="built_in">ls</span> --pool rbd-data1</span><br><span class="line"><span class="comment">#以 json 格 式 显 示 镜 像 信 息</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$rbd</span> <span class="built_in">ls</span> --pool rbd-data1 -l --format json --pretty-format</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建普通账户</span></span><br><span class="line">ceph auth add client.shijie mon <span class="string">&#x27;allow r&#x27;</span> osd <span class="string">&#x27;allow rwx pool=rbd-data1&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#验证用户信息</span></span><br><span class="line">ceph auth get client.shijie</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建用 keyring 文件</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph-authtool --create-keyring ceph.client.shijie.keyring</span><br><span class="line"></span><br><span class="line"><span class="comment">#导出用户 keyring</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph auth get client.shijie -o ceph.client.shijie.keyring </span><br><span class="line"></span><br><span class="line"><span class="comment">#验证指定用户的 keyring 文件</span></span><br><span class="line"></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$cat</span> ceph.client.shijie.keyring </span><br><span class="line">  </span><br><span class="line"><span class="comment">#同 步 普 通 用 户 认 证 文 件</span></span><br><span class="line"></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ scp ceph.client.shijie.keyring root@172.31.6.109:/etc/ceph/</span><br><span class="line"></span><br><span class="line"><span class="comment">#管理端验证镜像状态</span></span><br><span class="line"></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$rbd</span> <span class="built_in">ls</span> -p rbd-data1 -l</span><br><span class="line"></span><br><span class="line"><span class="comment">#在client上操作</span></span><br><span class="line"><span class="comment">#映射 rbd</span></span><br><span class="line"></span><br><span class="line">root@ceph-node4:/<span class="comment"># rbd -p rbd-data1 map data-img1</span></span><br><span class="line">/dev/rbd0</span><br><span class="line">root@ceph-node4:/<span class="comment"># lsblk</span></span><br><span class="line">rbd0</span><br><span class="line">root@ceph-node4:/<span class="comment"># mkfs.xfs /dev/rbd0</span></span><br><span class="line">root@ceph-node4:/<span class="comment"># mount /dev/rbd0 /data</span></span><br><span class="line">root@ceph-node4:/<span class="comment"># docker run -it -d -p 3306:3306 -e MYSQL_ROOT_PASSWORD=&quot;12345678&quot; -v /data:/var/lib/mysql mysql:5.6.46</span></span><br><span class="line">48374db8541a7fa375c00611373051ef21690e89adfd4c156b3f6ffb0dbe95a2</span><br><span class="line">root@ceph-node4:/data<span class="comment"># ls</span></span><br><span class="line">ibdata1  ib_logfile0  ib_logfile1  mysql  performance_schema  <span class="built_in">test</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#在client上操作</span></span><br><span class="line"><span class="comment">#使用普通用户映射 rbd</span></span><br><span class="line"></span><br><span class="line">root@ceph-node4:/etc/ceph<span class="comment"># rbd --user shijie -p rbd-data1 map data-img2</span></span><br><span class="line">/dev/rbd1</span><br><span class="line"><span class="comment">#格式化</span></span><br><span class="line">root@ceph-node4:/etc/ceph<span class="comment">#mkfs.ext4 /dev/rbd0</span></span><br><span class="line">root@ceph-node4:/etc/ceph<span class="comment"># mkdir /data1</span></span><br><span class="line">root@ceph-node4:/etc/ceph<span class="comment"># mount /dev/rbd1 /data1</span></span><br><span class="line">root@ceph-node4:/etc/ceph<span class="comment"># cp /var/log/auth.log /data1</span></span><br><span class="line">root@ceph-node4:/etc/ceph<span class="comment"># cd /data1</span></span><br><span class="line">root@ceph-node4:/data1<span class="comment"># ls</span></span><br><span class="line">auth.log  lost+found</span><br><span class="line">四. 使用普通用户挂载 cephfs（可以通过 secret 或者 secretfile 的形式多主机同时挂载）</span><br><span class="line">Ceph FS 需要运行 Meta Data Services(MDS)服务，其守护进程为 ceph-mds，ceph-mds进程管理与 cephFS 上存储的文件相关的元数据，并协调对 ceph 存储集群的访问。</span><br><span class="line"></span><br><span class="line"><span class="comment">#部署MDS服务:</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ apt-cache madison ceph-mds</span><br><span class="line">  ceph-mds | 16.2.10-1bionic | https://mirrors.tuna.tsinghua.edu.cn/ceph/debian-pacific bionic/main amd64 Packages</span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ apt install ceph-mds=16.2.10-1bionic</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建CephFS meta data和data存储池</span></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph osd pool create cephfs-metadata 32 32</span><br><span class="line">pool <span class="string">&#x27;cephfs-metadata&#x27;</span> created <span class="comment">#保存 metadata 的 pool</span></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph osd pool create cephfs-data 64 64</span><br><span class="line">pool <span class="string">&#x27;cephfs-data&#x27;</span> created <span class="comment">#保存数据的 pool</span></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph fs new mycephfs cephfs-metadata</span><br><span class="line">cephfs-data</span><br><span class="line">new fs with metadata pool 7 and data pool 8</span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph fs <span class="built_in">ls</span></span><br><span class="line">name: mycephfs, metadata pool: cephfs-metadata, data pools: [cephfs-data ]</span><br><span class="line"><span class="comment">#查看指定 cephFS 状态</span></span><br><span class="line"></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph fs status mycephfs</span><br><span class="line">mycephfs - 0 clients</span><br><span class="line">RANK  STATE      MDS        ACTIVITY     DNS    INOS   DIRS   CAPS  </span><br><span class="line"> 0    active  ceph-mgr1  Reqs:    0 /s    10     13     12      0   </span><br><span class="line">      POOL         TYPE     USED  AVAIL  </span><br><span class="line">cephfs-metadata  metadata   146k  18.9T  </span><br><span class="line">  cephfs-data      data       0   18.9T  </span><br><span class="line">MDS version: ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)</span><br><span class="line"><span class="comment">#验证cephFS服务状态</span></span><br><span class="line"></span><br><span class="line">cephadmin@ceph-deploy:~$ ceph mds <span class="built_in">stat</span></span><br><span class="line">mycephfs:1 &#123;0=ceph-mgr1=up:active&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建客户端账户</span></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster<span class="variable">$ceph</span> auth add client.yanyan mon <span class="string">&#x27;allow r&#x27;</span> mds <span class="string">&#x27;allow rw&#x27;</span> osd <span class="string">&#x27;allow rwx pool=cephfs-data&#x27;</span></span><br><span class="line"><span class="comment">#验证账户</span></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth get client.yanyan</span><br><span class="line">exported keyring <span class="keyword">for</span> client.yanyan</span><br><span class="line">[client.yanyan]</span><br><span class="line">key = AQCxpdhfjQt1OxAAGe0mqTMveNu2ZMEem3tb0g==</span><br><span class="line">caps mds = <span class="string">&quot;allow rw&quot;</span></span><br><span class="line">caps mon = <span class="string">&quot;allow r&quot;</span></span><br><span class="line">caps osd = <span class="string">&quot;allow rwx pool=cephfs-data&quot;</span></span><br><span class="line"><span class="comment">#创建keyring 文件</span></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth get client.yanyan -o</span><br><span class="line">ceph.client.yanyan.keyring</span><br><span class="line">exported keyring <span class="keyword">for</span> client.yanyan</span><br><span class="line"><span class="comment">#创建 key 文件：</span></span><br><span class="line"></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph auth print-key client.yanyan &gt; yanyan.key</span><br><span class="line"><span class="comment">#验证用户的 keyring 文件</span></span><br><span class="line"></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ <span class="built_in">cat</span> ceph.client.yanyan.keyring</span><br><span class="line">[client.yanyan]</span><br><span class="line">key = AQCxpdhfjQt1OxAAGe0mqTMveNu2ZMEem3tb0g==</span><br><span class="line">caps mds = <span class="string">&quot;allow rw&quot;</span></span><br><span class="line">caps mon = <span class="string">&quot;allow r&quot;</span></span><br><span class="line">caps osd = <span class="string">&quot;allow rwx pool=cephfs-data&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#同步客户端认证文件 ：</span></span><br><span class="line"></span><br><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ scp ceph.conf ceph.client.yanyan.keyring</span><br><span class="line">yanyan.key root@172.31.6.109:/etc/ceph/</span><br><span class="line"></span><br><span class="line"><span class="comment">#客户端验证权限</span></span><br><span class="line"></span><br><span class="line">root@ceph-node4:~<span class="comment"># ceph --user yanyan -s</span></span><br><span class="line">  cluster:</span><br><span class="line">    <span class="built_in">id</span>:     7c088d6f-06b0-4584-b23f-c0f150af51d4</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph-mon1,ceph-mon2,ceph-mon3 (age 24m)</span><br><span class="line">    mgr: ceph-mgr1(active, since 65m)</span><br><span class="line">    mds: 1/1 daemons up</span><br><span class="line">    osd: 16 osds: 16 up (since 24m), 16 <span class="keyword">in</span> (since 13d)</span><br><span class="line">    rgw: 1 daemon active (1 hosts, 1 zones)</span><br><span class="line">  data:</span><br><span class="line">    volumes: 1/1 healthy</span><br><span class="line">    pools:   10 pools, 329 pgs</span><br><span class="line">    objects: 296 objects, 218 MiB</span><br><span class="line">    usage:   948 MiB used, 60 TiB / 60 TiB avail</span><br><span class="line">    pgs:     329 active+clean</span><br><span class="line"></span><br><span class="line"><span class="comment">#客户端通过 key 文件挂载:</span></span><br><span class="line"></span><br><span class="line"> root@ceph-node4:~<span class="comment">#mkdir /data</span></span><br><span class="line"> root@ceph-node4:/etc/ceph<span class="comment"># mount -t ceph 172.31.6.101:6789,172.31.6.102:6789,172.31.6.103:6789:/ /data -o name=yanyan,secretfile=/etc/ceph/yanyan.key</span></span><br><span class="line">root@ceph-node4:/etc/ceph<span class="comment"># df -h</span></span><br><span class="line">Filesystem                                               Size  Used Avail Use% Mounted on</span><br><span class="line">udev                                                     955M     0  955M   0% /dev</span><br><span class="line">172.31.6.101:6789,172.31.6.102:6789,172.31.6.103:6789:/   19T     0   19T   0% /data</span><br><span class="line"></span><br><span class="line"><span class="comment">#客户端通过key挂载</span></span><br><span class="line"></span><br><span class="line">root@ceph-node3:~<span class="comment"># mkdir /data</span></span><br><span class="line">root@ceph-node3:~<span class="comment"># mount -t ceph 172.31.6.101:6789,172.31.6.102:6789,172.31.6.103:6789:/ /data -o name=yanyan,secret=AQAfebVjaIPgABAAzkW4ChX2Qm2Sha/5twdxPA==</span></span><br><span class="line">root@ceph-node3:~<span class="comment"># df -h</span></span><br><span class="line">Filesystem                                               Size  Used Avail Use% Mounted on</span><br><span class="line">udev                                                     955M     0  955M   0% /dev</span><br><span class="line">172.31.6.101:6789,172.31.6.102:6789,172.31.6.103:6789:/   19T     0   19T   0% /data</span><br><span class="line">root@ceph-node3:~<span class="comment"># cp /var/log/auth.log /data</span></span><br><span class="line">root@ceph-node3:~<span class="comment"># cd /data</span></span><br><span class="line">root@ceph-node3:/data<span class="comment"># ls</span></span><br><span class="line">auth.log</span><br><span class="line">root@ceph-node3:/data<span class="comment"># vim auth.log </span></span><br><span class="line">root@ceph-node3:/data<span class="comment"># echo &quot;12345678&quot; &gt;&gt; auth.log </span></span><br><span class="line">root@ceph-node3:/data<span class="comment"># echo &quot;testlog&quot; &gt;&gt; auth.log</span></span><br><span class="line"><span class="comment">#在node4客户端上查看cephfs挂载点/data 目录下内容，已经同步</span></span><br><span class="line"></span><br><span class="line">root@ceph-node4:/<span class="comment"># tail -f /data/auth.log </span></span><br><span class="line">Jan  4 22:25:01 ceph-node3 CRON[4365]: pam_unix(cron:session): session closed <span class="keyword">for</span> user root</span><br><span class="line">12345678</span><br><span class="line">testlog</span><br><span class="line"><span class="comment">#客户端内核加载 ceph.ko 模块挂载 cephfs 文件系统</span></span><br><span class="line"></span><br><span class="line">root@ceph-node4:/<span class="comment"># lsmod|grep ceph</span></span><br><span class="line">ceph                  380928  1</span><br><span class="line">libceph               315392  1 ceph</span><br><span class="line">fscache                65536  1 ceph</span><br><span class="line">libcrc32c              16384  5 nf_conntrack,nf_nat,xfs,raid456,libceph</span><br><span class="line">root@ceph-node4:/<span class="comment"># modinfo ceph</span></span><br><span class="line">filename:       /lib/modules/4.15.0-130-generic/kernel/fs/ceph/ceph.ko</span><br><span class="line">license:        GPL</span><br><span class="line">description:    Ceph filesystem <span class="keyword">for</span> Linux</span><br><span class="line">author:         Patience Warnick &lt;patience@newdream.net&gt;</span><br><span class="line">author:         Yehuda Sadeh &lt;yehuda@hq.newdream.net&gt;</span><br><span class="line">author:         Sage Weil &lt;sage@newdream.net&gt;</span><br><span class="line"><span class="built_in">alias</span>:          fs-ceph</span><br><span class="line">srcversion:     CB79D9E4790452C6A392A1C</span><br><span class="line">depends:        libceph,fscache</span><br><span class="line">retpoline:      Y</span><br><span class="line">intree:         Y</span><br><span class="line">name:           ceph</span><br><span class="line">vermagic:       4.15.0-130-generic SMP mod_unload </span><br><span class="line">signat:         PKCS<span class="comment">#7</span></span><br><span class="line">signer:         </span><br><span class="line">sig_key:        </span><br><span class="line">sig_hashalgo:   md4</span><br></pre></td></tr></table></figure>

<p>五.实现 MDS 服务的多主一备高可用架构</p>
<p>#当前mds服务器状态</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~$ ceph mds <span class="built_in">stat</span></span><br><span class="line">mycephfs:1 &#123;0=ceph-mgr1=up:active&#125;</span><br><span class="line"><span class="comment">#添加MDS服务器</span></span><br><span class="line">将 ceph-mgr2 和 ceph-mon2 和 ceph-mon3 作为 mds 服务角色添加至 ceph 集群，最后实两主两备的 mds 高可用和高性能结构。</span><br><span class="line"><span class="comment">#mds 服务器安装 ceph-mds 服务</span></span><br><span class="line"></span><br><span class="line">[root@ceph-mgr2 ~]<span class="comment"># apt install ceph-mds -y</span></span><br><span class="line">[root@ceph-mon2 ~]<span class="comment"># apt install ceph-mds -y</span></span><br><span class="line">[root@ceph-mon3 ~]<span class="comment"># apt install ceph-mds -y</span></span><br><span class="line"><span class="comment">#添加 mds 服务器</span></span><br><span class="line"></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph-deploy mds create ceph-mgr2</span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph-deploy mds create ceph-mon2</span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph-deploy mds create ceph-mon3</span><br></pre></td></tr></table></figure>
<p>#验证 mds 服务器当前状态：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[cephadmin@ceph-deploy ceph-cluster]$ ceph mds <span class="built_in">stat</span></span><br><span class="line">mycephfs-1/1/1 up &#123;0=ceph-mgr1=up:active&#125;, 3 up:standby</span><br></pre></td></tr></table></figure>
<p>#验证 ceph集群当前状态<br>当前处于激活状态的 mds 服务器有一台，处于备份状态的 mds 服务器有三台。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph fs status</span><br><span class="line">mycephfs - 0 clients</span><br><span class="line">RANK  STATE      MDS        ACTIVITY     DNS    INOS   DIRS   CAPS  </span><br><span class="line"> 0    active  ceph-mgr1  Reqs:    0 /s    16     14     12      0   </span><br><span class="line">      POOL         TYPE     USED  AVAIL  </span><br><span class="line">cephfs-metadata  metadata   282k  18.9T  </span><br><span class="line">  cephfs-data      data     564k  18.9T  </span><br><span class="line">STANDBY MDS  </span><br></pre></td></tr></table></figure>
<p>#当前的文件系统状态:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph fs get mycephfs</span><br><span class="line">Filesystem <span class="string">&#x27;mycephfs&#x27;</span> (1)</span><br><span class="line">fs_name        mycephfs</span><br><span class="line">epoch        28</span><br><span class="line">flags        12</span><br><span class="line">created        2023-01-01T19:09:29.258956+0800</span><br><span class="line">modified        2023-01-05T14:58:00.369468+0800</span><br><span class="line">tableserver        0</span><br><span class="line">root        0</span><br><span class="line">session_timeout        60</span><br><span class="line">session_autoclose        300</span><br><span class="line">max_file_size        1099511627776</span><br><span class="line">required_client_features        &#123;&#125;</span><br><span class="line">last_failure        0</span><br><span class="line">last_failure_osd_epoch        406</span><br><span class="line">compat        compat=&#123;&#125;,rocompat=&#123;&#125;,incompat=&#123;1=base v0.20,2=client writeable ranges,3=default file layouts on <span class="built_in">dirs</span>,4=<span class="built_in">dir</span> inode <span class="keyword">in</span> separate object,5=mds uses versioned encoding,6=dirfrag is stored <span class="keyword">in</span> omap,7=mds uses inline data,8=no anchor table,9=file layout v2,10=snaprealm v2&#125;</span><br><span class="line">max_mds        1</span><br><span class="line"><span class="keyword">in</span>        0</span><br><span class="line">up        &#123;0=144106&#125;</span><br><span class="line">failed        </span><br><span class="line">damaged        </span><br><span class="line">stopped        </span><br><span class="line">data_pools        [8]</span><br><span class="line">metadata_pool        7</span><br><span class="line">inline_data        disabled</span><br><span class="line">balancer        </span><br><span class="line">standby_count_wanted        1</span><br><span class="line">[mds.ceph-mgr1&#123;0:144106&#125; state up:active <span class="built_in">seq</span> 27 addr [v2:172.31.6.104:6800/428364709,v1:172.31.6.104:6801/428364709] compat &#123;c=[1],r=[1],i=[7ff]&#125;]</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置处于激活状态mds的数量</span></span><br><span class="line">目前有四个 mds 服务器，但是有一个主三个备，可以优化一下部署架构，设置为为两主两备。</span><br><span class="line"></span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph fs <span class="built_in">set</span> mycephfs max_mds 2</span><br><span class="line">cephadmin@ceph-deploy:~/ceph-cluster$ ceph fs status</span><br><span class="line">mycephfs - 0 clients</span><br><span class="line">RANK  STATE      MDS        ACTIVITY     DNS    INOS   DIRS   CAPS  </span><br><span class="line"> 0    active  ceph-mgr1  Reqs:    0 /s    16     14     12      0   </span><br><span class="line"> 1    active  ceph-mon2  Reqs:    0 /s    10     13     11      0   </span><br><span class="line">      POOL         TYPE     USED  AVAIL  </span><br><span class="line">cephfs-metadata  metadata   354k  18.9T  </span><br><span class="line">  cephfs-data      data     564k  18.9T  </span><br><span class="line">STANDBY MDS  </span><br><span class="line"> ceph-mgr2   </span><br><span class="line"> ceph-mon3   </span><br><span class="line">MDS version: ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)</span><br></pre></td></tr></table></figure>


<p>安装完成后ceph -s提示：“mon is allowing insecure global_id reclaim”。</p>
<p>解决方案：禁用不安全模式</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph config <span class="built_in">set</span> mon auth_allow_insecure_global_id_reclaim <span class="literal">false</span></span><br></pre></td></tr></table></figure>
<p>输入完成后需要等待5-10秒。</p>
<p>PG数量计算公式：</p>
<p>（总OSD数量*100）&#x2F; 副本数（复制分数，默认为3）&#x3D; PG数量</p>
<p>一般情况下结果取2的N次方，尽量先设置小点，后期可以增大。如果先设置的比较大的话后期减小风险较高，所以尽量取小的2的N次方结果。<br>建一个存储池，要想使用ceph的存储功能，必须先创建存储池</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd pool create rbd 128 128 </span><br></pre></td></tr></table></figure>

<p>初始化存储池</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd pool init -p rbd</span><br></pre></td></tr></table></figure>
<p>1.设置存储池副本数</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@controller-1 ~]<span class="comment"># ceph osd pool get rbd size</span></span><br><span class="line">size: 2</span><br><span class="line">[root@controller-1 ~]<span class="comment"># ceph osd pool set rbd size 1</span></span><br><span class="line"><span class="built_in">set</span> pool 1 size to 1</span><br></pre></td></tr></table></figure>
<p>升级client的虚拟机内核到5版本</p>
<p>修改client下文件权限</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">chmod</span> +r /etc/ceph/ceph.client.admin.keyring</span><br><span class="line">client节点创建设备镜像，单位是M</span><br><span class="line">rbd create --size 4096 --pool rbd img</span><br><span class="line">client节点映射镜像到主机：</span><br><span class="line">rbd map img --name client.admin</span><br><span class="line">client节点格式化块设备</span><br><span class="line">mkfs.ext4 -m 0 /dev/rbd/rbd/foo </span><br><span class="line">client节点挂载mount块设备</span><br><span class="line"><span class="built_in">mkdir</span> /mnt/ceph-block-device</span><br><span class="line">mount /dev/rbd/rbd/foo /mnt/ceph-block-device -o discard</span><br><span class="line">创建文件系统时报不支持EC数据池问题</span><br><span class="line">当拥有一个ceph_metadata元数据副本类型池和一个ceph_data数据EC类型池时，建立文件系统：</span><br><span class="line">ceph fs new cephfs cephfs_metadata cephfs_data</span><br><span class="line">N版可能会报：</span><br><span class="line">Error EINVAL: pool <span class="string">&#x27;cephfs_data&#x27;</span> (<span class="built_in">id</span> <span class="string">&#x27;11&#x27;</span>) is an erasure-coded pool. Use of an EC pool <span class="keyword">for</span> the default data pool is discouraged; see the online CephFS documentation <span class="keyword">for</span> more information. Use --force to override.</span><br><span class="line">加上—force后：</span><br><span class="line">ceph fs new cephfs cephfs_metadata cephfs_data --force</span><br><span class="line"></span><br><span class="line">也可能会报：</span><br><span class="line"></span><br><span class="line">Error EINVAL: pool <span class="string">&#x27;cephfs_data&#x27;</span> (<span class="built_in">id</span> <span class="string">&#x27;11&#x27;</span>) is an erasure-coded pool, with no overwrite support</span><br><span class="line"></span><br><span class="line">这是需要手动设置ceph_data池 allow_ec_overwrites=<span class="literal">true</span></span><br><span class="line">ceph osd pool <span class="built_in">set</span> cephfs_data allow_ec_overwrites <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">再执行</span><br><span class="line">ceph fs new cephfs cephfs_metadata cephfs_data –forc</span><br><span class="line">就可以创建文件系统成功。</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>群晖变身airdrop</title>
    <url>/2023/08/16/%E7%BE%A4%E6%99%96%E5%8F%98%E8%BA%ABairdrop/</url>
    <content><![CDATA[<p>把群晖 NAS 变成「时间返回舱」，轻松搞定 Time Machine 无线备份</p>
<p>2018年11月18日</p>
<p>相比 Windows 自带的系统还原功能，macOS 有着更加完善的备份还原机制：通过内置的 Time Machine，我们可以方便地进行整机备份，在关键时候成为系统以及重要资料一颗「后悔药」。</p>
<p>当然，如果需要使用 Time Machine 功能备份，苹果官方提供了两种方式，无论哪一种方式，你都需要外置的存储来解决：一种是准备一个较合适容量的移动硬盘，如果你正好有一个闲置不用的移动硬盘，拿出来做专门的 Mac 的 Time Machine 备份倒是不错，只不过就会比较麻烦——每一次备份都需要插入移动硬盘。</p>
<p>还有一种是使用网络存储器，比如说苹果官方的高存储容量 AirPort Time Capsule（时间返回舱），设置一次就可以轻松完成自动备份工作，但其售价高昂不说，现在也已经全面在苹果在线商店下架，显然无论是从经济角度还是便利性角度而言，以上两个方案似乎都并不合适。</p>
<p>其实，如果你恰好有一台群晖 NAS ，就可以将其轻松打造成一个稳定可靠的「时间返回舱」，而该功能从入门级的 J 系列就已经默认搭载，换句话说，你几乎没有什么额外成本就可以拥有和 AirPort Time Capsule 近乎一样的功能，那么如何使用 Time Machine 将 Mac 文件备份至群晖呢？</p>
<h2 id="如何让群晖-NAS-支持-Time-Machine-功能？"><a href="#如何让群晖-NAS-支持-Time-Machine-功能？" class="headerlink" title="如何让群晖 NAS 支持 Time Machine 功能？"></a>如何让群晖 NAS 支持 Time Machine 功能？</h2><p>要想让群晖支持 Time Machine ，还需要对群晖进行一番设置，首先我们使用管理员账号登录群晖的 DSM 系统1 ，首先来创建一个共享文件夹，专门用来存放 Time Machine 的备份数据：</p>
<p>登录到 DSM 之后，点击「控制面板 - 共享文件夹」，点击「创建」来添加共享文件夹。</p>
<p><img src="https://cdn.sspai.com/2018/11/18/ac135adbe134d0703a7acac6bbc4a04c.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1" alt="img"></p>
<p>然后输入共享文件夹的名字2 ，然后选择一个所在位置（单盘 NAS 选择默认）：</p>
<p><img src="https://cdn.sspai.com/2018/11/18/7992d79626fd2f571cae842bde49b2c1.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1" alt="img"></p>
<p>后面一步就是是否需要加密，因为我是本地家庭环境使用，为了减少一些不必要的麻烦直接跳过，如果你是在公司中来操作则建议设置加密，然后点击下一步。</p>
<p><img src="https://cdn.sspai.com/2018/11/18/940f3d290ab4cb4f91a7e6ee58d08d49.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1" alt="img"></p>
<p>后面是为共享文件夹配置高级功能，这里我就直接跳过，然后确认设置下面选择应用。</p>
<p><img src="https://cdn.sspai.com/2018/11/18/57993635211b319945fe96921a7ef000.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1" alt="img"></p>
<p>最后会弹出读取权限的设置，这里面默认应该只有管理员（当前你登录的账户）读写权限，这里默认不修改，点击确定生成分享文件夹。</p>
<p><img src="https://cdn.sspai.com/2018/11/18/a715c3349fbeca61d476e5050bb05bcf.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1" alt="img"></p>
<p>现在你应该可以看到名为 Time Machine Folder 新共享文件夹了，下面进入到下一步，为这个文件夹配可访问的账号。</p>
<h2 id="在-NAS-中为-Time-Machine-创建管理用户"><a href="#在-NAS-中为-Time-Machine-创建管理用户" class="headerlink" title="在 NAS 中为 Time Machine 创建管理用户"></a>在 NAS 中为 Time Machine 创建管理用户</h2><p>Time Machine Folder 这个文件夹显然应该只需要有一个专门操作的账户来操作，使用管理员账号操作并不安全，因此在这里我们需要给 Time Machine 创建管理用户并为其配备的对应配额限制。</p>
<p><img src="https://cdn.sspai.com/2018/11/18/86234695bbbb0758f2aa1d1567eca86b.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1" alt="img"></p>
<p>首先，还是通过群晖的管理员账户登录 DSM ，在「控制面板 - 用户」中，点击「创建用户」，输入用户名3 并设置一个复杂密码，然后点下一步，在群组中保持默认再点下一步。</p>
<p><img src="https://cdn.sspai.com/2018/11/18/9b3e81fc108b528526b47257a8c8e7a5.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1" alt="img"></p>
<p>在文件夹的权限分配这个设置页中，找到之前设置的共享文件夹「Time Machine Folder」，<strong>勾选中「读写」权限（一定要做），</strong>然后点击下一步，在用户配合设置页面中，选择存储配额。</p>
<p>这一步需要注意的是，为了避免 Time Machine 备份占用你全部的存储空间，建议设置在一个额定的空间范围内，比如我的 MacBook Pro 是 128GB 的存储，而我的群晖 NAS 的存储「空间 1」剩余 2.0 TB ，因此从使用权衡来看，选择 200GB 完全是绰绰有余，制定好之后选择下一步。</p>
<p><img src="https://cdn.sspai.com/2018/11/18/85d0d591291cf483391f7ad169493d12.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1" alt="img"></p>
<p>后续两个设置中都默认不选，直接点下一步，最后点击到应用后完成用户「Time Machine User」的新建，最后你在「控制面板 - 用户」中应该可以看到这个新建的用户了。</p>
<p><img src="https://cdn.sspai.com/2018/11/18/cf650e3ab4e93f9bb7ec52c84e61124b.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1" alt="img"></p>
<p>p.s.  如果你是在企业生产环境中，可以通过生成配对的「共享文件夹」+「对应的用户」来创建多个 Time Machine 备份空间，这样可以为多台 Mac 设备开启单独的备份，管理也更方便。</p>
<h2 id="在-NAS-中开启相关文件服务"><a href="#在-NAS-中开启相关文件服务" class="headerlink" title="在 NAS 中开启相关文件服务"></a>在 NAS 中开启相关文件服务</h2><p>前面共享文件夹和相关的访问用户已经设置完毕，那么接下来就是对相关的文件夹进行一系列的设置。首先在群晖的「控制面板 - 文件服务」设置中，找到 <strong>SMB&#x2F;AFP&#x2F;NFS</strong> 选项卡，然后点击勾选 启用 SMB 服务以及 AFP 服务。注意下方生成的访问地址，例如我这里生成的 SMB 访问地址：<code>smb://DS215J</code>。</p>
<p><img src="https://cdn.sspai.com/2018/11/18/9a0808946eb2a9d0c1e4cebe4919a8bb.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1" alt="img"></p>
<p>然后切换到高级设置中，在 Bonjour 中勾选「启用 Bonjour 服务发现」，以及下方的「启用通过 SMB 进行 Bonjour Time Machine 播送」和「启用通过 AFP 进行 Bonjour Time Machine 播送」，然后点击下方的设置 Time machine 文件夹，选择之前我们建立的 「Time Machine Folder」共享文件夹点击确定即可。</p>
<p><img src="https://cdn.sspai.com/2018/11/18/dbf8339cefcd9273213e57735e8c8d4d.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1" alt="img"></p>
<p>p.s. 如果你的环境中，Mac 设备的系统版本在 10.12 以上，那么只需要启用 SMB 相关服务即可，可以不用勾选 AFP 服务。</p>
<p>至此，在群晖上的相关设置就全部完成了，换言之，这时候的群晖已经脱胎换骨成为一台「时间返回舱」了。</p>
<h2 id="让-Mac-的-Time-Machine-备份至-NAS"><a href="#让-Mac-的-Time-Machine-备份至-NAS" class="headerlink" title="让 Mac 的 Time Machine 备份至 NAS"></a>让 Mac 的 Time Machine 备份至 NAS</h2><p>接下来的步骤就是在 Mac 上设置时间机器，然后将备份目的地改成刚才设置好的群晖，首先在我们需要可以让 Mac 访问群晖中的备份文件夹。</p>
<p><img src="https://cdn.sspai.com/2018/11/18/c67463c703df1874de7c6318052b2430.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1" alt="img"></p>
<p>打开访达，在菜单中点击「前往 - 连接服务器」，然后输入 NAS 的 smb 地址，例如我这里的 <code>smb://DS215J</code>，然后点击链接，在输入之前设置的账户名和密码之后，在弹出的文件夹中，选择时间机器的备份文件夹，之后点击好完成装载。</p>
<p><img src="https://cdn.sspai.com/2018/11/18/1f1c1dea15242ee5d673acbdbc7e7429.png?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1" alt="img"></p>
<p>接下来从 Dock 栏中打开「系统偏好设置」，在「时间机器」中选择「备份磁盘」，然后选择之前链接的群晖对应的文件夹，然后点击使用磁盘。</p>
<p>这时候还有可能会要求输入之前在群晖中创建的账户和密码，然后点击链接，然后时间机器会自动绑定群晖设置到了备份目的磁盘，然后到这里你的时间机器就算是真正意义上的设置好了，如果不出意外的话就会自动开始进行备份工作。</p>
<p>通过一连串的操作之后，你的群晖可以轻松化身为一台「廉价版 AirPort Time Capsule」，为你的 Mac 设备保驾护航，而无论是性价比上还是便利性上都可以称作是 Mac 时间机器的最佳解决方案，如果你恰好有一台 Mac 和群晖设备，不妨试试这个方案来打造一个最廉价的时间机器备份方案。</p>
]]></content>
  </entry>
  <entry>
    <title>ceph安装以及测试</title>
    <url>/ceph%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E6%B5%8B%E8%AF%95/index.html</url>
    <content><![CDATA[]]></content>
  </entry>
</search>
