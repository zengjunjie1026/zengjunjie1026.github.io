<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>spark null 值处理</title>
    <url>/2023/08/16/spark-null-%E5%80%BC%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<p> scala spark na null处理 | 分子美食家的博客                  </p>
<h1 id="分子美食家的博客"><a href="#分子美食家的博客" class="headerlink" title="分子美食家的博客"></a><a href="/">分子美食家的博客</a></h1><h2 id="野生工程师的专栏"><a href="#野生工程师的专栏" class="headerlink" title="野生工程师的专栏"></a><a href="/">野生工程师的专栏</a></h2><p><a href="/">Home</a> <a href="/archives">Archives</a></p>
<p><a href="/atom.xml" title="RSS Feed"></a></p>
<p></p>
<p><a href="/2022/05/14/scala-spark-na-null%E5%A4%84%E7%90%86/">2022-05-14</a></p>
<h1 id="scala-spark-na-null处理"><a href="#scala-spark-na-null处理" class="headerlink" title="scala spark na null处理"></a>scala spark na null处理</h1><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parseCheck</span></span>(s: <span class="type">String</span>,min:<span class="type">Double</span>,max:<span class="type">Double</span>): <span class="type">Option</span>[<span class="type">Double</span>] = {</span><br><span class="line">      <span class="keyword">try</span> {</span><br><span class="line">        <span class="keyword">val</span> va = s.toDouble</span><br><span class="line">        <span class="keyword">if</span> (va &lt; min || va &gt; max) {</span><br><span class="line">          <span class="type">Some</span>(va)</span><br><span class="line">        }<span class="keyword">else</span> {</span><br><span class="line">          <span class="type">Some</span>(<span class="type">Double</span>.<span class="type">NaN</span>)</span><br><span class="line">        }</span><br><span class="line">      }</span><br><span class="line">      <span class="keyword">catch</span> { <span class="keyword">case</span> _ =&gt; <span class="type">Some</span>(<span class="type">Double</span>.<span class="type">NaN</span>)</span><br><span class="line">      }</span><br><span class="line">    }</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parseDouble</span></span>(s: <span class="type">String</span>): <span class="type">Option</span>[<span class="type">Double</span>] = <span class="keyword">try</span> { <span class="type">Some</span>(s.toDouble) } <span class="keyword">catch</span> { <span class="keyword">case</span> _ =&gt; <span class="type">Some</span>(<span class="type">Double</span>.<span class="type">NaN</span>) }</span><br></pre></td></tr></tbody></table>

<p>Share</p>
<p>[<strong>Older</strong></p>
<p>scala-spark-dataframe—groupby基本操作</p>
<p>](&#x2F;2022&#x2F;05&#x2F;07&#x2F;scala-spark-dataframe%E2%80%94groupby%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C&#x2F;)</p>
<h3 id="Archives"><a href="#Archives" class="headerlink" title="Archives"></a>Archives</h3><ul>
<li><a href="/archives/2022/05/">May 2022</a></li>
</ul>
<h3 id="Recent-Posts"><a href="#Recent-Posts" class="headerlink" title="Recent Posts"></a>Recent Posts</h3><ul>
<li><a href="/2022/05/14/scala-spark-na-null%E5%A4%84%E7%90%86/">scala spark na null处理</a></li>
<li><a href="/2022/05/07/scala-spark-dataframe%E2%80%94groupby%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/">scala-spark-dataframe—groupby基本操作</a></li>
<li><a href="/2022/05/04/fedora-k8s/">fedora-k8s</a></li>
<li><a href="/2022/05/04/openwrt%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/">openwrt安装配置</a></li>
<li><a href="/2022/05/04/ubuntu-supervisor%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/">ubuntu supervisor进程管理</a></li>
</ul>
<p>© 2022 andrew<br>Powered by <a href="https://hexo.io/">Hexo</a></p>
<p><a href="/">Home</a> <a href="/archives">Archives</a></p>
<p>%</p>
]]></content>
  </entry>
  <entry>
    <title>spark group by 操作</title>
    <url>/2023/08/16/spark-group-by-%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<p>scala-spark-dataframe—groupby基本操作<br>[andrew@hadoop102 bin]$ .&#x2F;spark-shell<br>2022-05-07 20:35:13,392 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable<br>Setting default log level to “WARN”.<br>To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).<br>Spark context Web UI available at <a href="http://hadoop102:4040/">http://hadoop102:4040</a><br>Spark context available as ‘sc’ (master &#x3D; local[*], app id &#x3D; local-1651926921962).<br>Spark session available as ‘spark’.<br>Welcome to<br>      ____              __<br>     &#x2F; <strong>&#x2F;</strong>  ___ _<em><em><strong>&#x2F; &#x2F;</strong><br>    <em>\ / _ / _ &#96;&#x2F; _<em>&#x2F;  ‘</em>&#x2F;<br>   &#x2F;_</em></em>&#x2F; .__&#x2F;_,</em>&#x2F;<em>&#x2F; &#x2F;</em>&#x2F;_\   version 3.0.0<br>      &#x2F;_&#x2F;</p>
<p>Using Scala version 2.12.10 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_212)<br>Type in expressions to have them evaluated.<br>Type :help for more information.</p>
<p>scala&gt; val df &#x3D; spark.createDataset(Seq(<br>     |   (“aaa”,1,2),(“bbb”,3,4),(“ccc”,3,5),(“bbb”,4, 6))   ).toDF(“key1”,”key2”,”key3”)<br>df: org.apache.spark.sql.DataFrame &#x3D; [key1: string, key2: int … 1 more field]</p>
<p>scala&gt; df.show()<br>+—-+—-+—-+<br>|key1|key2|key3|<br>+—-+—-+—-+<br>| aaa|   1|   2|<br>| bbb|   3|   4|<br>| ccc|   3|   5|<br>| bbb|   4|   6|<br>+—-+—-+—-+</p>
<p>scala&gt; df.printSchema()<br>root<br> |– key1: string (nullable &#x3D; true)<br> |– key2: integer (nullable &#x3D; false)<br> |– key3: integer (nullable &#x3D; false)</p>
<p>scala&gt; df.groupBy(“key1”).count.show<br>+—-+—–+<br>|key1|count|<br>+—-+—–+<br>| ccc|    1|<br>| aaa|    1|<br>| bbb|    2|<br>+—-+—–+</p>
<p>scala&gt; df.select(“key1”).distinct.show<br>+—-+<br>|key1|<br>+—-+<br>| ccc|<br>| aaa|<br>| bbb|<br>+—-+</p>
<p>scala&gt; df.select(“key1”).distinct.count<br>res4: Long &#x3D; 3</p>
<p>scala&gt; f.groupBy(“key1”).count.sort(“key1”).show<br><console>:24: error: not found: value f<br>       f.groupBy(“key1”).count.sort(“key1”).show<br>       ^</p>
<p>scala&gt; df.groupBy(“key1”).count.sort(“key1”).show<br>+—-+—–+<br>|key1|count|<br>+—-+—–+<br>| aaa|    1|<br>| bbb|    2|<br>| ccc|    1|<br>+—-+—–+</p>
<p>scala&gt; df.groupBy(“key1”).count.sort($”count”.desc).show<br>+—-+—–+<br>|key1|count|<br>+—-+—–+<br>| bbb|    2|<br>| ccc|    1|<br>| aaa|    1|<br>+—-+—–+</p>
<p>scala&gt; df.groupBy(“key1”).count.withColumnRenamed(“count”, “cnt”).sort($”cnt”.desc).show<br>+—-+—+<br>|key1|cnt|<br>+—-+—+<br>| bbb|  2|<br>| aaa|  1|<br>| ccc|  1|<br>+—-+—+</p>
<p>scala&gt; df.groupBy(“key1”).agg(count(“key1”).as(“cnt”)).show<br>+—-+—+<br>|key1|cnt|<br>+—-+—+<br>| ccc|  1|<br>| aaa|  1|<br>| bbb|  2|<br>+—-+—+</p>
<p>scala&gt;  df.groupBy(“key1”).agg(count(“key1”), max(“key2”), avg(“key3”)).show<br>+—-+———–+———+———+<br>|key1|count(key1)|max(key2)|avg(key3)|<br>+—-+———–+———+———+<br>| ccc|          1|        3|      5.0|<br>| aaa|          1|        1|      2.0|<br>| bbb|          2|        4|      5.0|<br>+—-+———–+———+———+</p>
<p>scala&gt; f.groupBy(“key1”)<br><console>:24: error: not found: value f<br>       f.groupBy(“key1”)<br>       ^</p>
<p>scala&gt;        df.groupBy(“key1”).agg(“key1”-&gt;”count”, “key2”-&gt;”max”, “key3”-&gt;”avg”).show<br>+—-+———–+———+———+<br>|key1|count(key1)|max(key2)|avg(key3)|<br>+—-+———–+———+———+<br>| ccc|          1|        3|      5.0|<br>| aaa|          1|        1|      2.0|<br>| bbb|          2|        4|      5.0|<br>+—-+———–+———+———+</p>
<p>scala&gt; df.groupBy(“key1”).agg(Map((“key1”,”count”), (“key2”,”max”), (“key3”,”avg”))).show<br>+—-+———–+———+———+<br>|key1|count(key1)|max(key2)|avg(key3)|<br>+—-+———–+———+———+<br>| ccc|          1|        3|      5.0|<br>| aaa|          1|        1|      2.0|<br>| bbb|          2|        4|      5.0|<br>+—-+———–+———+———+</p>
<p>scala&gt; df.groupBy(“key1”).agg(count(“key1”).as(“cnt”), max(“key2”).as(“max_key2”), avg(“key3”).as(“avg_key3”)).sort($”cnt”,$”max_key2”.desc).show<br>+—-+—+——–+——–+<br>|key1|cnt|max_key2|avg_key3|<br>+—-+—+——–+——–+<br>| ccc|  1|       3|     5.0|<br>| aaa|  1|       1|     2.0|<br>| bbb|  2|       4|     5.0|<br>+—-+—+——–+——–+</p>
<p>package groupby</p>
<p>import org.apache.spark.SparkConf<br>import org.apache.spark.sql.SparkSession</p>
<p>object demos {</p>
<p>  def main(args: Array[String]): Unit &#x3D; {<br>    val conf &#x3D; new SparkConf().setAppName(“LzSparkDatasetExamples”).setMaster(“local[*]”)<br>    val sparkSession &#x3D; SparkSession.builder().enableHiveSupport().config(conf).getOrCreate()</p>
<p>&#x2F;&#x2F;    &#x2F;&#x2F;LOGGER.info(“——– this is info ——–”)<br>    import sparkSession.implicits._<br>    val df &#x3D; sparkSession.createDataset(Seq(<br>      (“aaa”, 1, 2),<br>      (“bbb”, 3, 4),<br>      (“ccc”, 3, 5),<br>      (“bbb”, 4, 6)<br>    )).toDF(“key1”, “key2”, “key3”)</p>
<pre><code>import org.apache.spark.sql.functions._
//LOGGER.info(&quot;--------df.groupBy(\&quot;key1\&quot;).count().show()-----------&quot;)
df.groupBy(&quot;key1&quot;).count().show()
//LOGGER.info(&quot;--------df.select(\&quot;key1\&quot;).distinct().show()-----------&quot;)
df.select(&quot;key1&quot;).distinct().show()
val key1Count = df.select(&quot;key1&quot;).distinct().count()
//LOGGER.info(&quot;--------df.select(\&quot;key1\&quot;).distinct().count()-----------&quot; +key1Count)
//LOGGER.info(&quot;--------df.groupBy(\&quot;key1\&quot;).count().sort(\&quot;key1\&quot;).show()-----------&quot;)
df.groupBy(&quot;key1&quot;).count().sort(&quot;key1&quot;).show()
//LOGGER.info(&quot;--------df.groupBy(\&quot;key1\&quot;).count().sort($\&quot;key1\&quot;.desc).show()-----------&quot;)
df.groupBy(&quot;key1&quot;).count().sort($&quot;key1&quot;.desc).show()
//LOGGER.info(&quot;--------df.groupBy(\&quot;key1\&quot;).count.withColumnRenamed(\&quot;count\&quot;, \&quot;cnt\&quot;).sort($\&quot;cnt\&quot;.desc).show-----------&quot;)
df.groupBy(&quot;key1&quot;).count
  .withColumnRenamed(&quot;count&quot;, &quot;cnt&quot;).sort($&quot;cnt&quot;.desc).show()
//LOGGER.info(&quot;--------df.groupBy(\&quot;key1\&quot;).agg(count(\&quot;key1\&quot;).as(\&quot;cnt\&quot;)).show-----------&quot;)
df.groupBy(&quot;key1&quot;).agg(count(&quot;key1&quot;).as(&quot;cnt&quot;)).show()

// 使用agg聚合函数
df.groupBy(&quot;key1&quot;).agg(count(&quot;key1&quot;), max(&quot;key2&quot;), avg(&quot;key3&quot;)).show
df.groupBy(&quot;key1&quot;).agg(&quot;key1&quot;-&gt;&quot;count&quot;, &quot;key2&quot;-&gt;&quot;max&quot;, &quot;key3&quot;-&gt;&quot;avg&quot;).show()
df.groupBy(&quot;key1&quot;).agg(Map((&quot;key1&quot;,&quot;count&quot;), (&quot;key2&quot;,&quot;max&quot;), (&quot;key3&quot;,&quot;avg&quot;))).show()
df.groupBy(&quot;key1&quot;).agg((&quot;key1&quot;,&quot;count&quot;), (&quot;key2&quot;,&quot;max&quot;), (&quot;key3&quot;,&quot;avg&quot;)).show
df.groupBy(&quot;key1&quot;)
  .agg(count(&quot;key1&quot;).as(&quot;cnt&quot;), max(&quot;key2&quot;).as(&quot;max_key2&quot;), avg(&quot;key3&quot;).as(&quot;avg_key3&quot;))
  .sort($&quot;cnt&quot;,$&quot;max_key2&quot;.desc).show
</code></pre>
<p>  }</p>
<p>}</p>
]]></content>
  </entry>
  <entry>
    <title>ceph安装以及测试</title>
    <url>/ceph%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E6%B5%8B%E8%AF%95/index.html</url>
    <content><![CDATA[]]></content>
  </entry>
</search>
